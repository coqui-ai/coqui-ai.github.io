{"version":3,"sources":["webpack:///./src/pages/blog/tts/two-methods-for-better-attention-in-tacotron.mdx","webpack:///./src/templates/BlogTemplate.tsx"],"names":["_frontmatter","layoutProps","pageQuery","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","isMDXComponent","BlogLayoutTemplate","children","data","pageContext","mdx","useEffect","consentedToGoogleAnalytics","title","frontmatter","description","excerpt","subtitle","name","picture","date","toc","tableOfContents","items"],"mappings":"6PAcaA,EAAe,GACtBC,EAAc,CAClBC,UAPuB,aAQvBF,gBAEIG,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,EACF,iBACD,OAAO,YAACJ,EAAD,iBAAeF,EAAiBM,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAI5E,4MAEiB,iBAAGC,WAAW,IAC3B,KAAQ,mCADK,aAFjB,0BAKQ,iBAAGA,WAAW,IAClB,KAAQ,oCADJ,yBALR,uBAQQ,iBAAGA,WAAW,IAClB,KAAQ,mCADJ,oBARR,6CAYA,kBACE,GAAM,wBACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,yBACR,aAAc,kCACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,yBAqBA,qBAAG,oBAAMA,WAAW,IAChB,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPf,WAUC,oBAAMA,WAAW,OACf,UAAa,qCACb,MAAS,CACP,cAAiB,QACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,ouBACnB,eAAkB,QAClB,QAAW,WAnBhB,OAsBH,mBAAKA,WAAW,OACV,UAAa,0BACb,IAAO,QACP,MAAS,QACT,IAAO,wHACP,OAAU,CAAC,6HAA8H,6HAA8H,8HAA+H,+HACtY,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,SAtCd,WAyCH,icAMA,qZAKA,uNAGA,qBAAG,oBAAMA,WAAW,IAChB,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,UAPf,WAUC,oBAAMA,WAAW,OACf,UAAa,qCACb,MAAS,CACP,cAAiB,MACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,opBACnB,eAAkB,QAClB,QAAW,WAnBhB,OAsBH,mBAAKA,WAAW,OACV,UAAa,0BACb,IAAO,QACP,MAAS,QACT,IAAO,qHACP,OAAU,CAAC,0HAA2H,0HAA2H,2HACjQ,MAAS,kCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,SAtCd,WAyCH,+fAOA,wcAMA,kBACE,GAAM,mBACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,oBACR,aAAc,6BACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,oBAqBA,uhBAOA,uNAGA,kgBAOA,qBAAG,oBAAMA,WAAW,IAChB,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,UAPf,WAUC,oBAAMA,WAAW,OACf,UAAa,qCACb,MAAS,CACP,cAAiB,sBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,4VACnB,eAAkB,QAClB,QAAW,WAnBhB,OAsBH,mBAAKA,WAAW,OACV,UAAa,0BACb,IAAO,QACP,MAAS,QACT,IAAO,oHACP,OAAU,CAAC,yHAA0H,yHAA0H,0HAC/P,MAAS,kCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,SAtCd,WAyCH,6KAED,4BAFC,mDAGA,0DAAyC,iBAAGA,WAAW,IACnD,KAAQ,oCAD6B,QAAzC,yTAOA,qBAAG,oBAAMA,WAAW,IAChB,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPf,WAUC,oBAAMA,WAAW,OACf,UAAa,qCACb,MAAS,CACP,cAAiB,MACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,o0BACnB,eAAkB,QAClB,QAAW,WAnBhB,OAsBH,mBAAKA,WAAW,OACV,UAAa,0BACb,IAAO,QACP,MAAS,QACT,IAAO,sHACP,OAAU,CAAC,2HAA4H,2HAA4H,4HAA6H,6HAChY,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,SAtCd,WAyCH,+QAIA,kLAEA,kBACE,GAAM,aACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,cACR,aAAc,uBACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,cAqBA,8HACmB,iBAAGA,WAAW,IAC7B,KAAQ,2CADO,YADnB,0EAImE,iBAAGA,WAAW,IAC7E,KAAQ,oCADuD,cAJnE,yCAQA,uOAGA,4UAIA,uFAAsE,iBAAGA,WAAW,IAChF,KAAQ,mCAD0D,cAAtE,uBASJJ,EAAWK,gBAAiB,G,kCC/V5B,6GAoDeC,IAlC0D,SAAC,GAIpE,IAHJC,EAGI,EAHJA,SACAC,EAEI,EAFJA,KACAC,EACI,EADJA,YAEQC,EAAQF,EAARE,IAMR,OAJAC,qBAAU,WACRC,iBAIA,kBAAC,IAAD,KACE,kBAAC,IAAD,CACEC,MAAUJ,EAAYK,YAAYD,MAA7B,UACLE,YAAaN,EAAYK,YAAYC,aAAeL,EAAIM,UAE1D,kBAAC,IAAD,KACE,kBAAC,IAAD,CACEH,MAAOJ,EAAYK,YAAYD,MAC/BI,SAAUR,EAAYK,YAAYC,YAClCG,KAAMT,EAAYK,YAAYI,KAC9BC,QAASV,EAAYK,YAAYK,QACjCC,KAAMX,EAAYK,YAAYM,KAC9BC,IAAKX,EAAIY,gBAAgBC,OAEzB,kBAAC,IAAD,KAAmBhB,KAGvB,kBAAC,IAAD","file":"component---src-pages-blog-tts-two-methods-for-better-attention-in-tacotron-mdx-6ca36d65aba009c44cc3.js","sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/coqui-ai.github.io/coqui-ai.github.io/src/templates/BlogTemplate.tsx\";\nimport { graphql } from 'gatsby';\nexport const pageQuery = graphql`\n  query($fileAbsolutePath: String) {\n    ...SidebarPageFragment\n  }\n`;\nexport const _frontmatter = {};\nconst layoutProps = {\n  pageQuery,\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n\n    <p>{`In this post, I‚Äôd like to introduce two methods that, in my experience, worked\nwell for better attention alignment in Tacotron models. If you like to try your\nown, you can visit `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/coqui-ai/TTS\"\n      }}>{`Coqui TTS`}</a>{`. The first\nmethod is `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1907.09006\"\n      }}>{`Bidirectional Decoder`}</a>{` and the\nsecond is `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1308.0850\"\n      }}>{`Graves Attention`}</a>{` (Gaussian\nAttention) with small tweaks.`}</p>\n    <h3 {...{\n      \"id\": \"bidirectional-decoder\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#bidirectional-decoder\",\n        \"aria-label\": \"bidirectional decoder permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`Bidirectional Decoder`}</h3>\n    <p><span parentName=\"p\" {...{\n        \"className\": \"gatsby-resp-image-wrapper\",\n        \"style\": {\n          \"position\": \"relative\",\n          \"display\": \"block\",\n          \"marginLeft\": \"auto\",\n          \"marginRight\": \"auto\",\n          \"maxWidth\": \"1000px\"\n        }\n      }}>{`\n      `}<span parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-background-image\",\n          \"style\": {\n            \"paddingBottom\": \"46.4%\",\n            \"position\": \"relative\",\n            \"bottom\": \"0\",\n            \"left\": \"0\",\n            \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABxElEQVQoz1VSyY7TQBDN/x858QdI3EBzQSAktkEsChKZiIyTjOPYcWLHjp12L+7Nj7KdBFHS62rV8rqqqyYgcb4j+AG9dARLNus8OrLl+wTheoV4u8E6WICzMzqr4Y26RNPZjXrSJ/nxPrgeXz/H5vOrwWYIWgnwmEj2IbRoUC9/gpcH8N0KKgthjIFz7kY6IT5wznEqS1giWH19i3T+HaeH9zhO72CsR0OEMvkDyys06ylkvkUdLSDSFfktlJLU3Ug6cUTScIEkPYCxBu3YNeosRpkswaWGIK2JxAkGsZlBH7dQu2DQHcWzikMJPRIK7VBJh7DgSMkR7Eu0ph3a31YdSt5CWYXWaRiCuNwlaeMNRN0iC2uwQo4tWyrR04cJY8FaSrJjhU/1HotTRDaDH9USC5agNhwfj3NEIsfsHGHJ0yFWcAmtzXUo3W1SvbwI7vFmN8OXwxTvok/g1MGMxQhkirNX+NWECGWGByIM2I6IqFopqSj3j3BYkcvKPJvf4WV4fxt7YzS+RXP8TgLEeYoPT1MEZYKgOSCRxTDh68oMLfdHv4f+sotXMTT+Hv1D+alEXhTISgJpJgQc5Rlv6c3/9/AvW26v6PezQ78AAAAASUVORK5CYII=')\",\n            \"backgroundSize\": \"cover\",\n            \"display\": \"block\"\n          }\n        }}></span>{`\n  `}<img parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-image\",\n          \"alt\": \"IMAGE\",\n          \"title\": \"IMAGE\",\n          \"src\": \"/static/e7f08e2cac087f13ae1c38673c41f046/da8b6/blog-tts-two-methods-for-better-attention-in-tacotron-architecture.png\",\n          \"srcSet\": [\"/static/e7f08e2cac087f13ae1c38673c41f046/43fa5/blog-tts-two-methods-for-better-attention-in-tacotron-architecture.png 250w\", \"/static/e7f08e2cac087f13ae1c38673c41f046/c6e3d/blog-tts-two-methods-for-better-attention-in-tacotron-architecture.png 500w\", \"/static/e7f08e2cac087f13ae1c38673c41f046/da8b6/blog-tts-two-methods-for-better-attention-in-tacotron-architecture.png 1000w\", \"/static/e7f08e2cac087f13ae1c38673c41f046/d2a27/blog-tts-two-methods-for-better-attention-in-tacotron-architecture.png 1419w\"],\n          \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n          \"style\": {\n            \"width\": \"100%\",\n            \"height\": \"100%\",\n            \"margin\": \"0\",\n            \"verticalAlign\": \"middle\",\n            \"position\": \"absolute\",\n            \"top\": \"0\",\n            \"left\": \"0\"\n          },\n          \"loading\": \"lazy\"\n        }}></img>{`\n    `}</span></p>\n    <p>{`Bidirectional decoding uses an extra decoder which takes the encoder outputs in\nthe reverse order and then, there is an extra loss function that compares the\noutput states of the forward decoder with the backward one. With this\nadditional loss, the forward decoder models what it needs to expect for the\nnext iterations. In this regard, the backward decoder punishes bad decisions of\nthe forward decoder and vice versa.`}</p>\n    <p>{`Intuitively, if the forward decoder fails to align the attention, that would\ncause a big loss and ultimately it would learn to go monotonically through the\nalignment process with a correction induced by the backward decoder. Therefore,\nthis method is able to prevent ‚Äúcatastrophic failure‚Äù where the attention falls\napart in the middle of a sentence and it never aligns again.`}</p>\n    <p>{`At the inference time, the paper suggests to only use the forward decoder and\ndemote the backward decoder. However, it is possible to think more elaborate\nways to combine these two models.`}</p>\n    <p><span parentName=\"p\" {...{\n        \"className\": \"gatsby-resp-image-wrapper\",\n        \"style\": {\n          \"position\": \"relative\",\n          \"display\": \"block\",\n          \"marginLeft\": \"auto\",\n          \"marginRight\": \"auto\",\n          \"maxWidth\": \"727px\"\n        }\n      }}>{`\n      `}<span parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-background-image\",\n          \"style\": {\n            \"paddingBottom\": \"30%\",\n            \"position\": \"relative\",\n            \"bottom\": \"0\",\n            \"left\": \"0\",\n            \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABh0lEQVQY00WQSy9DURSFT9GHUmLiXxgYVFT0oW5bWnr1treP0EETiURLNZHoa2Ri5g+YGogYGRRRDeJRIZQqEkkHfgRVatn3Ek6ys5OzvrP2XofF+pMwswk4VAHYlX7YW6ia/eDUItzdETy+HAP4QqlQAaclpjUg6xLnIJ5TeMHrIji/3iaqBjbTuwgDc8LKBFiYh2qCSoBJ4YFTG0KxWoR0bvbLMEma0vvHDVE3MjdcqknsXOyijnewWf0SgW6aKsLW5AOnCcDWFiRYAN8xhb3KPRpkWDooY5h5wSlFcO0hSuCHTeGTOY8ujI3DE7ziEyzaJxnycKgpii4ImyYgm1vpMd8VRv7ygaKQYaFM8Xw0VKS4NFwblHmJEzrD2Myd4a3RAJvuSUDPRiiiIK9vZONyNzAX/auIQv5OjnxNfYDuBn/1H44nzo1RWmZr/Qi1Om24Gl9D1JJGYiiLuDmD2GASC5Ys5o0pJMeWcXv6JBs+l6qIm9KkZf456nN0l/Kt4CR3hXrtA998YAbjGk7mYQAAAABJRU5ErkJggg==')\",\n            \"backgroundSize\": \"cover\",\n            \"display\": \"block\"\n          }\n        }}></span>{`\n  `}<img parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-image\",\n          \"alt\": \"IMAGE\",\n          \"title\": \"IMAGE\",\n          \"src\": \"/static/927f7566bf5e778d1a19f43f88a53bae/b5a20/blog-tts-two-methods-for-better-attention-in-tacotron-alignment.png\",\n          \"srcSet\": [\"/static/927f7566bf5e778d1a19f43f88a53bae/43fa5/blog-tts-two-methods-for-better-attention-in-tacotron-alignment.png 250w\", \"/static/927f7566bf5e778d1a19f43f88a53bae/c6e3d/blog-tts-two-methods-for-better-attention-in-tacotron-alignment.png 500w\", \"/static/927f7566bf5e778d1a19f43f88a53bae/b5a20/blog-tts-two-methods-for-better-attention-in-tacotron-alignment.png 727w\"],\n          \"sizes\": \"(max-width: 727px) 100vw, 727px\",\n          \"style\": {\n            \"width\": \"100%\",\n            \"height\": \"100%\",\n            \"margin\": \"0\",\n            \"verticalAlign\": \"middle\",\n            \"position\": \"absolute\",\n            \"top\": \"0\",\n            \"left\": \"0\"\n          },\n          \"loading\": \"lazy\"\n        }}></img>{`\n    `}</span></p>\n    <p>{`There are 2 main pitfalls of this method. The first, due to additional\nparameters of the backward decoder, it is slower to train this model (almost\n2x) and this makes a huge difference especially when the reduction rate is low\n(number of frames the model generates per iteration). The second, if the\nbackward decoder penalizes the forward one too harshly, that causes overall\nprosody degradation. Due to this the paper suggests activating the additional\nloss just for fine-tuning.`}</p>\n    <p>{`My experience is that Bidirectional training is quite robust against alignment\nproblems and it is especially useful if your dataset is hard. It also almost\naligns after the first epoch. Yes, at inference time, it sometimes causes\npronunciation problems but I solved this by doing the opposite of the paper‚Äôs\nsuggestion. Just for an epoch I finetuned the network without the additional\nloss and everything started to work well.`}</p>\n    <h3 {...{\n      \"id\": \"graves-attention\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#graves-attention\",\n        \"aria-label\": \"graves attention permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`Graves Attention`}</h3>\n    <p>{`Tacotron uses Bahdenau Attention which is a content-based attention method. It\ndoes not consider location information. Therefore, it needs to learn the\nmonotonicity of the alignment just looking into the content which is a hard\nproblem. Tacotron2 uses Location Sensitive Attention which takes into account\nthe previous attention weights. By doing so, it learns the monotonic\nconstraint. But it does not solve all of the problems and you can still\nexperience failures with long or out of domain sentences.`}</p>\n    <p>{`Graves Attention is an alternative that uses content information to decide how\nfar it needs to go on the alignment per iteration. It does this by using a\nmixture of Gaussian distributions.`}</p>\n    <p>{`Graves Attention takes the context vector of time t-1 and passes it through\ncouple of fully connected layers (`}{`[FC > ReLU > FC]`}{` in our model) and estimates\nstep-size, variance and distribution weights for time t. Then the estimated\nstep-size is used to update the mean of Gaussian modes. Analogously, mean is\nthe point of interest t the alignment path, variance is attention window over\nthis point of interest and distribution weight is the importance of each\ndistribution head.`}</p>\n    <p><span parentName=\"p\" {...{\n        \"className\": \"gatsby-resp-image-wrapper\",\n        \"style\": {\n          \"position\": \"relative\",\n          \"display\": \"block\",\n          \"marginLeft\": \"auto\",\n          \"marginRight\": \"auto\",\n          \"maxWidth\": \"772px\"\n        }\n      }}>{`\n      `}<span parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-background-image\",\n          \"style\": {\n            \"paddingBottom\": \"29.599999999999998%\",\n            \"position\": \"relative\",\n            \"bottom\": \"0\",\n            \"left\": \"0\",\n            \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAnklEQVQY042QjQqEIBCEff+XDAuVyLS0Hytyjl3ouOqCFoYFR7+dVeBSOWfu27YhhABjDKvv+5P/VOIJuK4rnHOQUkJrjZTS1//VdYi4wg6TAJSqLEtUVYW2bW93/iV+TDjPMwNjjBiGAfu+nx7TBq8SHp2A3nsURQFrLeq6Rtd1aJqGEyul+I9frUyiBMfa0zSx6GwcRwaTtyzLDfgBqTjW6fhwktgAAAAASUVORK5CYII=')\",\n            \"backgroundSize\": \"cover\",\n            \"display\": \"block\"\n          }\n        }}></span>{`\n  `}<img parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-image\",\n          \"alt\": \"IMAGE\",\n          \"title\": \"IMAGE\",\n          \"src\": \"/static/d1ceb96f942ada9348462e66567f5718/5a533/blog-tts-two-methods-for-better-attention-in-tacotron-equation.png\",\n          \"srcSet\": [\"/static/d1ceb96f942ada9348462e66567f5718/43fa5/blog-tts-two-methods-for-better-attention-in-tacotron-equation.png 250w\", \"/static/d1ceb96f942ada9348462e66567f5718/c6e3d/blog-tts-two-methods-for-better-attention-in-tacotron-equation.png 500w\", \"/static/d1ceb96f942ada9348462e66567f5718/5a533/blog-tts-two-methods-for-better-attention-in-tacotron-equation.png 772w\"],\n          \"sizes\": \"(max-width: 772px) 100vw, 772px\",\n          \"style\": {\n            \"width\": \"100%\",\n            \"height\": \"100%\",\n            \"margin\": \"0\",\n            \"verticalAlign\": \"middle\",\n            \"position\": \"absolute\",\n            \"top\": \"0\",\n            \"left\": \"0\"\n          },\n          \"loading\": \"lazy\"\n        }}></img>{`\n    `}</span></p>\n    <p>{`Formulated as I compute the alignment in my implementation. Here g, b, and k\nare intermediate values, ùõø is the step size, œÉ is the variance, and\nw`}<sub>{`k`}</sub>{` is the distribution weight for the GMM node k.`}</p>\n    <p>{`Some other versions are explained `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1910.10288\"\n      }}>{`here`}</a>{` but\nso far I found the above formulation works for me the best, without any NaNs in\ntraining. I also realized that with the best-claimed method in this paper, one\nof the distribution nodes overruns the others in the middle of the training and\nbasically, attention starts to run on a single Gaussian head.`}</p>\n    <p><span parentName=\"p\" {...{\n        \"className\": \"gatsby-resp-image-wrapper\",\n        \"style\": {\n          \"position\": \"relative\",\n          \"display\": \"block\",\n          \"marginLeft\": \"auto\",\n          \"marginRight\": \"auto\",\n          \"maxWidth\": \"1000px\"\n        }\n      }}>{`\n      `}<span parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-background-image\",\n          \"style\": {\n            \"paddingBottom\": \"40%\",\n            \"position\": \"relative\",\n            \"bottom\": \"0\",\n            \"left\": \"0\",\n            \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAACDElEQVQoz22S7U+SURiHn17c+iOaK821NCqtZTM/JOBWBjlS82WTIa2IF9MgNSFAV5qIzXyBGrVsi61af5QfWrlJKS8Cz/Mgz9URXJ/6cG3n/HbtPtv9O5Kj+SmGI73opT48ej8/5d+YVxNccc2hC88xtJ4gW0jj0PnQ1/TTcXwQt2GaFBm6Psa4sDTH2fkF7FNxFK2IZG3wcE26LbjFSIuPzcwuxjfrnLOHqAsF6Z5/x590BmvDKFclC63Cs7Y8YUvZoyO+TH1ohpNLYSzBGAVVQQp0LzBU52TolINQT5St7TS29x/o9EfQv4zgXPxEdidHoP81g3UuBmsfEBReKpvH/jnGzcgrbqxEeBTbQM6LgSW1hCqrqIpKSSlRLmsUiyqFgkJRIMsKmqZR9ZR/3kEmKwr5QhU5r1Yy6WBAhf1yJQCBVj1r++Jarmb/9w6hfIgYONsb5X7jGCNNXiKOFXZLKca+fWFgYh1zNMHzja8ocpqwZQl742NsTT4izmVypPEkk9jGo9yLxwh/T4q3RSnDZ9xi0WaBiZFmL5vFHQxv19CZpqidDWJaFKVkRCn1HlGKWdCF9bKXX2oe/bMVLrWNcfrhLJbpuChFRnK1B+g8MUDnsV5Grwf4kdrBsrpG690gFydm6JtPkE5lcbb5Mdb0YTzag1t425kcdyZXaRdf6Pz4C4Yn48h7Cn8BrCfGT+YhPpsAAAAASUVORK5CYII=')\",\n            \"backgroundSize\": \"cover\",\n            \"display\": \"block\"\n          }\n        }}></span>{`\n  `}<img parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-image\",\n          \"alt\": \"IMAGE\",\n          \"title\": \"IMAGE\",\n          \"src\": \"/static/982ff79cfecd1736d016d5821d5b7869/da8b6/blog-tts-two-methods-for-better-attention-in-tacotron-alignments.png\",\n          \"srcSet\": [\"/static/982ff79cfecd1736d016d5821d5b7869/43fa5/blog-tts-two-methods-for-better-attention-in-tacotron-alignments.png 250w\", \"/static/982ff79cfecd1736d016d5821d5b7869/c6e3d/blog-tts-two-methods-for-better-attention-in-tacotron-alignments.png 500w\", \"/static/982ff79cfecd1736d016d5821d5b7869/da8b6/blog-tts-two-methods-for-better-attention-in-tacotron-alignments.png 1000w\", \"/static/982ff79cfecd1736d016d5821d5b7869/16caf/blog-tts-two-methods-for-better-attention-in-tacotron-alignments.png 1465w\"],\n          \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n          \"style\": {\n            \"width\": \"100%\",\n            \"height\": \"100%\",\n            \"margin\": \"0\",\n            \"verticalAlign\": \"middle\",\n            \"position\": \"absolute\",\n            \"top\": \"0\",\n            \"left\": \"0\"\n          },\n          \"loading\": \"lazy\"\n        }}></img>{`\n    `}</span></p>\n    <p>{`The benefit of using GMM is to have more robust attention. It is also\ncomputationally light-weight compared to both bidirectional decoding and normal\nlocation attention. Therefore, you can increase your batch size and possibly\nconverge faster.`}</p>\n    <p>{`The downside is that, although my experiments are not complete, GMM‚Äôs not\nprovided slightly worse prosody and naturalness compared to the other methods.`}</p>\n    <h3 {...{\n      \"id\": \"comparison\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#comparison\",\n        \"aria-label\": \"comparison permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`Comparison`}</h3>\n    <p>{`Here I compare Graves Attention, Bidirectional Decoding, and Location Sensitive\nAttention trained on `}<a parentName=\"p\" {...{\n        \"href\": \"https://keithito.com/LJ-Speech-Dataset/\"\n      }}>{`LJSpeech`}</a>{`\ndataset. For the comparison, I used the set of sentences provided by `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1905.09263\"\n      }}>{`this\nwork`}</a>{`. There are in total of 50 sentences.`}</p>\n    <p>{`Out of these 50 sentences Bidirectional Decoding has 1 failure, Graves\nattention has 6 failures, Location Sensitive Attention has 18, and Location\nSensitive Attention with inference time windowing has 11.`}</p>\n    <p>{`In terms of prosodic quality, in my opinion, Location Sensitive Attention >\nBidirectional Decoding > Graves Attention > Location Sensitive Attention with\nWindowing. However, I should say the quality difference is hardly observable in\nLJSpeech dataset. I also need to point out that, it is a hard dataset.`}</p>\n    <p>{`If you like to try these methods, all these are implemented in `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/coqui-ai/TTS\"\n      }}>{`Coqui\nTTS`}</a>{`. Give them a try!`}</p>\n\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      ","/**\n * Copyright Coqui GmbH\n *\n * Use of this source code is governed under the Apache License, Version 2.0\n * found at http://www.apache.org/licenses/LICENSE-2.0.\n */\n\nimport React, { useEffect } from 'react';\nimport { PageProps } from 'gatsby';\nimport RootLayout from 'layouts/Root';\nimport { SidebarLayout } from 'layouts/Sidebar';\nimport TitledLayout from 'layouts/Titled';\nimport SEO from 'components/SEO';\nimport { MarkdownProvider } from 'components/MarkdownProvider';\nimport { IPageData, IPageContext } from './types';\nimport GogleAnalyticsCookieConsent from 'components/Cookies';\nimport { consentedToGoogleAnalytics } from 'utils/GoogleAnalytics';\n\nconst BlogLayoutTemplate: React.FC<PageProps<IPageData, IPageContext>> = ({\n  children,\n  data,\n  pageContext\n}) => {\n  const { mdx } = data;\n\n  useEffect(() => {\n    consentedToGoogleAnalytics();\n  });\n\n  return (\n    <RootLayout>\n      <SEO\n        title={`${pageContext.frontmatter.title} / Blog`}\n        description={pageContext.frontmatter.description || mdx.excerpt}\n      />\n      <SidebarLayout>\n        <TitledLayout\n          title={pageContext.frontmatter.title}\n          subtitle={pageContext.frontmatter.description}\n          name={pageContext.frontmatter.name}\n          picture={pageContext.frontmatter.picture}\n          date={pageContext.frontmatter.date}\n          toc={mdx.tableOfContents.items}\n        >\n          <MarkdownProvider>{children}</MarkdownProvider>\n        </TitledLayout>\n      </SidebarLayout>\n      <GogleAnalyticsCookieConsent />\n    </RootLayout>\n  );\n};\n\nexport default BlogLayoutTemplate;\n"],"sourceRoot":""}