(window.webpackJsonp=window.webpackJsonp||[]).push([[37],{KVSr:function(e,a,t){"use strict";t.r(a),t.d(a,"_frontmatter",(function(){return A})),t.d(a,"default",(function(){return c}));var n=t("zLVn"),o=(t("q1tI"),t("7ljp")),i=t("yDk1"),r=["components"],A={},s={pageQuery:"3484569904",_frontmatter:A},l=i.a;function c(e){var a=e.components,t=Object(n.a)(e,r);return Object(o.b)(l,Object.assign({},s,t,{components:a,mdxType:"MDXLayout"}),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"934px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"27.599999999999998%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA3XAAAN1wFCKJt4AAABCElEQVQY042RQUsCcRDF/VB9g+59gk6eOhTRpaBbh5CgYCuiTkYGah6CSIwiyiRbiiyVFg+5EoS6LO1/XS0oXffXriC1atCDOcyDeW/mTQAXjuMwhFHcH/g9H/CaPqG/W5T0V1ShUTbqjDIb7L/abR/vE8y8KEjyMfv5NHuFS87kO0LhCBvxBOuxBPHTc0K7EVJZGV2YhI+SZAvF3qzd7f4I9h287WpNgfho8WzWWdjaZnFzh+DSMtMra0zMzTO7KjE2GewJj0/NcOKKe+jY9vCGWsvkvlom55ZqamhvBqnrG26fFDIPeYTVRIoecHiR5ir3iKJWMBqW/+TBbDynz46by/9/4sv1G2shujMHTg19AAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png",srcSet:["/static/c9103bc33c8add5ab1fa4fa1c49c90ef/43fa5/logo-wordmark.png 250w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/c6e3d/logo-wordmark.png 500w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png 934w"],sizes:"(max-width: 934px) 100vw, 934px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("h3",{id:"work-at-coqui",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#work-at-coqui","aria-label":"work at coqui permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üë©‚ÄçüíªWork at Coqui"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(o.b)("p",null,"We‚Äôre still hiring!"),Object(o.b)("p",null,"An open source remote-friendly Berlin based startup founded by the creators of Mozilla‚Äôs\ntext-to-speech (TTS) and speech-to-text (STT) engines (over 725K downloads and 28K GitHub\nstars), with the backing of top-flight investors ",Object(o.b)("em",{parentName:"p"},"and")," we‚Äôre hiring!"),Object(o.b)("p",null,"What‚Äôs not to love?"),Object(o.b)("p",null,"We‚Äôre hiring across-the-board for a number of roles; so, there‚Äôs something for everyone:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/head-of-product"},"Head of Product")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-full-stack-engineer"},"Senior Full Stack Engineers")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-stt-deep-learning-engineer"},"Senior STT Deep Learning Engineers")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-tts-deep-learning-engineer"},"Senior TTS Deep Learning Engineers")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-developer-community-manager"},"Senior, Developer Community Managers"))),Object(o.b)("p",null,"The full list of open positions is available on our ",Object(o.b)("a",{parentName:"p",href:"/jobs"},"jobs page"),"."),Object(o.b)("p",null,"We‚Äôd love to hear from you; so, if any roles pique your interest, reach out to\n",Object(o.b)("a",{parentName:"p",href:"mailto:jobs@coqui.ai"},"jobs@coqui.ai"),". üê∏!"),Object(o.b)("h3",{id:"welcome-",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#welcome-","aria-label":"welcome  permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Welcome! üëãüê∏"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(o.b)("p",null,"Ready for a new month packed to the brim with Coqui goodness?"),Object(o.b)("p",null,"On November 10th Coqui will present at GTC! (Yes, that GTC, a conference that had over 50K\nregistrants last year.) We‚Äôll be talking about the long tail of languages, how the majority\nof the world speaks a ‚Äúminority‚Äù language, and what Coqui‚Äôs doing about it. If you‚Äôd like\nto hear more details,\n",Object(o.b)("a",{parentName:"p",href:"https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog?search=A31147&search=A31147"},"register"),"!\n(Also, at the end of our talk there‚Äôs a (secret) special surprise that you won‚Äôt want to\nmiss out on.)"),Object(o.b)("p",null,"üê∏üí¨TTS v0.4.0 is out the door! The rapid progress of Coqui‚Äôs TTS engine continues. This\nnewest version introduces:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Multi-speaker support for all the üê∏TTS models,"),Object(o.b)("li",{parentName:"ul"},"Updated Trainer API, and"),Object(o.b)("li",{parentName:"ul"},"Two new pre-trained models.")),Object(o.b)("p",null,"Along with much more TTS goodness."),Object(o.b)("p",null,"With üê∏ STT 1.0 hot off the presses, we aren‚Äôt resting on our laurels. This month we‚Äôve\nbeen working on an extension of our decoder package to greatly expand STT functionality,\neventually allowing for numerous types of scorers beyond what we currently support.\nThe ",Object(o.b)("a",{parentName:"p",href:"https://pypi.org/project/coqui-stt-ctcdecoder/1.1.0a1/"},"latest 1.1.0 preview release")," exposes\nAPIs that cover using acoustic models trained with CTC and ASG. So, if you‚Äôre looking to\nexplore, this is the place."),Object(o.b)("p",null,"Enjoy!"),Object(o.b)("h3",{id:"coqui-gtc-and-the-long-tail-of-languages",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#coqui-gtc-and-the-long-tail-of-languages","aria-label":"coqui gtc and the long tail of languages permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Coqui, GTC, and the Long Tail of Languages"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"70%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACfUlEQVQ4y3VT60tTYRjfP9O3bjAkslXQF8uJmOmKXGXXpaOCID+oSMSIok9REBJGmRXDIV7SlpikM6MbNYPlbSor1thql7Ozc9k526/3fes9Ozp64Dnbec7z/J7fc7MUi0VQpcL/m981vQCdaKGw3k5FkWVkMhlIkkS+F5jNshGsQNUUzIWBEjUnlAlQMplEPp83MCzmjBtF03WksxJUVTVs1JOzkRUF6XSaJNNLgByMMognRSxHfiO4EAM1h1aiqL5wD46OXrTdHUYoOA+opeA8SSQIQjnDn3EBUx/X8CKwhLHpJQy++kZY6fi69B3bHddg3XcG1dt2o3OXFXdaGhH7tcoAVEVGlgDSCjgxiyip8AcWCdAixmfDmHi7gtGpecK4SNjGscNWDXvlVhyvr0SDYw/sDRXouupmtSuqglQqBVEUS4CJZJYxm3y/xgCpPh6YRjQahf/5COrsNajbX4XGxiOod55CrbMFB892kt4J0DWN9TCXy5WmLOZkPBt6A5//MybfreGhL4CpwAfMjI/A09UO17GjcJ0+CXfTIXQ0O9F62A5Xcy00QihPSk6nU1DIcAxA+liNxPDI+xLXbz/ByOhrhL58wmBfD3rvd6PtykV43OdwovUS6qsqYd20Bf1eLwuWZJkxlMlv2ZRVNY+F8A/EI2HcuHkLno7LaHefx94aOw5UbIZtpw0OZxMmxoaMFVLVv2tTNmW6yFwUKYvAzCxGhwfQ/7QP3T0PMOjzYi44xy4Cpj3UCFA2Kxh7uo4hu5J/jv8T7sNjNDIUOmHaQ8rSYGi+FnMQP8WNl2TcMgFKJBJsypzlOkAzKHWgAWalNt4vzpDaKAFe3R8IOOnRc8gHkQAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/7049cfe0fa851646e1fba39a9398224d/da8b6/longtail_languages.png",srcSet:["/static/7049cfe0fa851646e1fba39a9398224d/43fa5/longtail_languages.png 250w","/static/7049cfe0fa851646e1fba39a9398224d/c6e3d/longtail_languages.png 500w","/static/7049cfe0fa851646e1fba39a9398224d/da8b6/longtail_languages.png 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"On November 10th Coqui will be afforded the honour of presenting at NVIDIA‚Äôs GTC conference\nin the ‚Äù",Object(o.b)("a",{parentName:"p",href:"https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog?search=A31147&search=A31147"},"From Speech to Text: How Does AI Understand the Spoken Word"),"?‚Äù\nsession."),Object(o.b)("p",null,"GTC is NVIDIA‚Äôs yearly conference which focuses largely on artificial intelligence and deep\nlearning, and it‚Äôs huge. Last year the conference had over 50K registrants!"),Object(o.b)("p",null,"Coqui‚Äôs presentation will focus on language‚Äôs long tail."),Object(o.b)("p",null,"Of the world‚Äôs roughly 7K languages there are a small number of languages that have over\n50 million native speakers, e.g. Mandarin, Spanish, English. However, this isn‚Äôt the\nentire story. There are a large number of languages with under 50 million native speakers.\nIn fact as of 2011 there were over ",Object(o.b)("a",{parentName:"p",href:"https://www.ethnologue.com/guides/ethnologue200"},"3.1 billion native speakers"),"\nof these ‚Äúlong tail languages‚Äù. As the world‚Äôs population was ",Object(o.b)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/World_population"},"7 billion"),"\nthen, when one includes non-native speakers of these long tail languages you reach the\nseemingly contradictory conclusion that the majority of the world speaks ‚Äúminority‚Äù\nlanguages?!"),Object(o.b)("p",null,"This leads to a massive problem: Most of the languages in the long tail have no\nspeech technology, but the majority of the world speaks these long tail languages."),Object(o.b)("p",null,"Intrigued? If you‚Äôd like to hear more details on Coqui‚Äôs approach to the long tail,\nI‚Äôd urge you to ",Object(o.b)("a",{parentName:"p",href:"https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog?search=A31147&search=A31147"},"register"),"\nto hear what Coqui has to say."),Object(o.b)("h3",{id:"-tts-v040-is-out-the-door",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#-tts-v040-is-out-the-door","aria-label":" tts v040 is out the door permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üê∏üí¨ TTS v0.4.0 is Out the Door"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"72%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQGAQP/xAAWAQEBAQAAAAAAAAAAAAAAAAAGBAX/2gAMAwEAAhADEAAAAX1ucmRTVxhLqf/EABkQAAMBAQEAAAAAAAAAAAAAAAMEBQIBIv/aAAgBAQABBQLPOMGVXC1o/glXpVk5Vgm7z9Pax//EABwRAAICAgMAAAAAAAAAAAAAAAECACESMSJR8P/aAAgBAwEBPwFODqrDHdDXqhJ7n//EAB4RAQABAwUBAAAAAAAAAAAAAAECAAMREiFBYbHh/9oACAECAQE/AWI2pysuQxvz37RqT61//8QAIhAAAgEEAgEFAAAAAAAAAAAAAQIDABESIQQxIiNBUWJx/9oACAEBAAY/ArxZABdtPpVH52aCxWSaLd3Hi9/kURLxuRl9DkKl5gl3Enpi3RrBjksyBDlWAQde1f/EAB4QAQEAAgICAwAAAAAAAAAAAAERACExUWFxkaHB/9oACAEBAAE/IdgmzaraPZ9GQJACHkHZe8kR+FPwn7vESWgGoyvq4dQxwLZb85JjBy1n/9oADAMBAAIAAwAAABC4/wD/xAAaEQEAAgMBAAAAAAAAAAAAAAABETEAUWGB/9oACAEDAQE/EGhqpBZIXvG3wTDn/8QAGxEBAAICAwAAAAAAAAAAAAAAAREhAIExcfD/2gAIAQIBAT8Qt8xKbAwvkjpW1ETxtz//xAAdEAEBAAMBAAMBAAAAAAAAAAABEQAhMUFhcYGR/9oACAEBAAE/EOPe1WRKjODEa3JPcYFeVAEOhBKMQQrykpBca06APcRkiFCLNGH5b9YR6aKQA4EP8+cWkFIu2xdl3L+5/9k=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/release.jpg",srcSet:["/static/1e77dc6fce06f5a577ab0e4d0674e779/0988f/release.jpg 250w","/static/1e77dc6fce06f5a577ab0e4d0674e779/d1f95/release.jpg 500w","/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/release.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(o.b)("p",null,"This version introduces:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Multi-speaker support for all the üê∏TTS models,"),Object(o.b)("li",{parentName:"ul"},"Updated Trainer API, and"),Object(o.b)("li",{parentName:"ul"},"Two new pre-trained models.")),Object(o.b)("p",null,"See the ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS/releases/tag/v0.4.0"},"release notes")," for the details."),Object(o.b)("p",null,"All the ",Object(o.b)("inlineCode",{parentName:"p"},"TTS.tts")," models now support multi-speaker training using speaker embedding layers.\nYou can find new recipes for training üê∏TTS models on the VCTK dataset and learn more about\nmulti-speaker training from our ",Object(o.b)("a",{parentName:"p",href:"https://tts.readthedocs.io/en/latest/training_a_model.html#multi-speaker-training"},"documentation"),"."),Object(o.b)("p",null,"The new API simplifies the Trainer by making it responsible for only the model training and\nremoving the use-case specific details. Now the üê∏TTS Trainer can be used as a generic\ntraining utility for deep learning models. We are actively working and improving the Trainer\nAPI. More updates are on the way!"),Object(o.b)("p",null,"We have also released two new ",Object(o.b)("inlineCode",{parentName:"p"},"TTS.tts")," models:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"FastPitch model trained on the VCTK dataset.")),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-bash"},'tts --model_name tts_models/en/vctk/fast_pitch --text "This is my sample text to voice." --speaker_idx VCTK_p229\n')),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Ukrainian GlowTTS model from üëë ",Object(o.b)("a",{parentName:"li",href:"https://github.com/robinhad/ukrainian-tts"},"https://github.com/robinhad/ukrainian-tts"))),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-bash"},'tts --model_name tts_models/uk/mai/glow-tts --text "–¶–µ –∑—Ä–∞–∑–æ–∫ —Ç–µ–∫—Å—Ç—É, —â–æ–± —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ –Ω–∞—à—É –º–æ–¥–µ–ª—å."\n')),Object(o.b)("h3",{id:"extending-our-beam-search-decoder-package",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#extending-our-beam-search-decoder-package","aria-label":"extending our beam search decoder package permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Extending our beam search decoder package"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMEBQf/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAcJWmBeQVAf/xAAbEAADAAIDAAAAAAAAAAAAAAABAgMABBESFP/aAAgBAQABBQKKv57Is5oEskiONmvaIYjP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAEDAQE/AYx//8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BV//EABwQAQADAAIDAAAAAAAAAAAAAAEAAhEQMiJBYf/aAAgBAQAGPwLotfqRsVKoQshs6V8faazMDj//xAAdEAEBAAIBBQAAAAAAAAAAAAABEQAxIUFRYXHR/9oACAEBAAE/IXRQNRdeAdYh1CbB9lc74LmNU5yRtz5+ZbiU0GGwc//aAAwDAQACAAMAAAAQc8//xAAXEQEBAQEAAAAAAAAAAAAAAAABEQAh/9oACAEDAQE/EIYZd47/xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPxBj/8QAGxABAAIDAQEAAAAAAAAAAAAAAREhADFBUWH/2gAIAQEAAT8QdXblyDJMdhYefcrZlCwdSbYkQaxx5iGsIvOVGCSeoxMqI5GI0YijR6yA35r5lFQ3rP/Z')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/32a4a2c6799dfaa882330f9e46c0cb78/dbdff/beam_search_decoder.jpg",srcSet:["/static/32a4a2c6799dfaa882330f9e46c0cb78/0988f/beam_search_decoder.jpg 250w","/static/32a4a2c6799dfaa882330f9e46c0cb78/d1f95/beam_search_decoder.jpg 500w","/static/32a4a2c6799dfaa882330f9e46c0cb78/dbdff/beam_search_decoder.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/reuben"},"Reuben Morais")),Object(o.b)("p",null,"Coqui STT uses two main subsystems for transcribing speech: an acoustic model which processes the\naudio and produces a sequence of character probabilities over time, and a beam search decoder\nwhich transforms these probabilities into actual transcripts, possibly with the help of an\nexternal scorer trained on text data."),Object(o.b)("p",null,"Enabling usage of scorers built entirely from text data lets users customize the transcription\nprocess in a faster and cheaper way than collecting audio for specialized vocabularies or\ndomains. The beam search decoder also enables features such as ",Object(o.b)("a",{parentName:"p",href:"https://stt.readthedocs.io/en/latest/HotWordBoosting-Examples.html"},"hot-word boosting"),",\nextracting timing metadata with the outputs, constraining the output to a fixed lexicon,\nas well as offering several optimization knobs to trade accuracy for speed."),Object(o.b)("p",null,"As we start to experiment with new ",Object(o.b)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"},"transformer-based architectures"),"\nfor speech recognition, it‚Äôs important to have flexibility in the beam search decoder in\norder to quickly experiment with different combinations of acoustic models and textual\nscorers. One can, for example, use a scorer trained on word n-grams, or character n-grams,\ntrading accuracy for model size and transcription speed. For acoustic model architectures\nthat blend the line between audio and text modelling, themselves embedding long term\ntextual knowledge, the decoder features allow for re-using a single high quality acoustic\nmodel for many different tasks by constraining its recognition space to certain jargon or\na specific list of phrases and words."),Object(o.b)("p",null,"In order to efficiently experiment with these new architectures, we‚Äôre integrating the\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/flashlight/flashlight/"},"flashlight library")," in our decoder\npackage. This means new APIs for decoding with word-based external scorers, character-based\nexternal scorers, and acoustic models trained with CTC, ASG, or Seq2seq loss criteria."),Object(o.b)("p",null,"On the ",Object(o.b)("a",{parentName:"p",href:"https://pypi.org/project/coqui-stt-ctcdecoder/1.1.0a1/"},"latest 1.1.0 preview release"),", the\nexposed APIs cover using acoustic models trained with CTC and ASG. We‚Äôre excited about\nthe possibilities here, and will soon be adding support for these models in the STT\ndeployment packages for use with our ",Object(o.b)("a",{parentName:"p",href:"https://coqui.ai/models/"},"pre-trained models"),".\nJoin ",Object(o.b)("a",{parentName:"p",href:"https://gitter.im/coqui-ai/STT"},"our üê∏STT chat room on Gitter")," and get involved!"))}c.isMDXComponent=!0},yDk1:function(e,a,t){"use strict";var n=t("q1tI"),o=t.n(n),i=t("O9mE"),r=t("v+Ly"),A=t("mrST"),s=t("1Yd/"),l=t("ozyN"),c=t("7cfw"),b=t("t4Fg");a.a=function(e){var a=e.children,t=e.data,p=e.pageContext,g=t.mdx;return Object(n.useEffect)((function(){Object(b.a)()})),o.a.createElement(i.a,null,o.a.createElement(s.a,{title:p.frontmatter.title+" / Newsletter",description:p.frontmatter.description||g.excerpt}),o.a.createElement(r.a,null,o.a.createElement(A.a,{title:p.frontmatter.title,subtitle:p.frontmatter.description,name:p.frontmatter.name,picture:p.frontmatter.picture,date:p.frontmatter.date,toc:g.tableOfContents.items},o.a.createElement(l.a,null,a))),o.a.createElement(c.a,null))}}}]);
//# sourceMappingURL=component---src-pages-newsletter-01-11-2021-mdx-95411a71dd3ad5d2928f.js.map