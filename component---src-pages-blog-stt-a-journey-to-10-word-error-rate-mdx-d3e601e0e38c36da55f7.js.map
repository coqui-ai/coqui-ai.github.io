{"version":3,"sources":["webpack:///./src/templates/BlogTemplate.tsx","webpack:///./src/pages/blog/stt/a-journey-to-10-word-error-rate.mdx"],"names":["BlogLayoutTemplate","children","data","pageContext","mdx","useEffect","consentedToGoogleAnalytics","title","frontmatter","description","excerpt","subtitle","name","picture","date","toc","tableOfContents","items","_frontmatter","layoutProps","pageQuery","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","align","isMDXComponent"],"mappings":"4FAAA,6GAoDeA,IAlC0D,SAAC,GAIpE,IAHJC,EAGI,EAHJA,SACAC,EAEI,EAFJA,KACAC,EACI,EADJA,YAEQC,EAAQF,EAARE,IAMR,OAJAC,qBAAU,WACRC,iBAIA,kBAAC,IAAD,KACE,kBAAC,IAAD,CACEC,MAAUJ,EAAYK,YAAYD,MAA7B,UACLE,YAAaN,EAAYK,YAAYC,aAAeL,EAAIM,UAE1D,kBAAC,IAAD,KACE,kBAAC,IAAD,CACEH,MAAOJ,EAAYK,YAAYD,MAC/BI,SAAUR,EAAYK,YAAYC,YAClCG,KAAMT,EAAYK,YAAYI,KAC9BC,QAASV,EAAYK,YAAYK,QACjCC,KAAMX,EAAYK,YAAYM,KAC9BC,IAAKX,EAAIY,gBAAgBC,OAEzB,kBAAC,IAAD,KAAmBhB,KAGvB,kBAAC,IAAD,S,mMCjCOiB,EAAe,GACtBC,EAAc,CAClBC,UAPuB,aAQvBF,gBAEIG,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,EACF,iBACD,OAAO,YAACJ,EAAD,iBAAeF,EAAiBM,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAI5E,8LAC6C,iBAAGC,WAAW,IACvD,KAAQ,uDADiC,yCAD7C,8DAKA,gKACa,iBAAGA,WAAW,IACvB,KAAQ,mCADC,KADb,KAGoB,iBAAGA,WAAW,IAC9B,KAAQ,oCADQ,KAHpB,6TASA,qKAEA,kBACE,GAAM,mBACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,oBACR,aAAc,6BACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,oBAqBA,+GACD,iBAAGA,WAAW,IACT,KAAQ,0DADb,OADC,wWAOA,qBAAG,mBAAKA,WAAW,IACf,IAAO,qGACP,IAAO,WAEX,2LAEA,iOAEF,iBAAGA,WAAW,IACR,KAAQ,iEADd,QAFE,qEAMA,qUAEgE,iBAAGA,WAAW,IAC1E,KAAQ,mDADoD,sBAFhE,iPAO4D,iBAAGA,WAAW,IACtE,KAAQ,mCADgD,kBAP5D,KAUA,kBACE,GAAM,WACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,YACR,aAAc,qBACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,YAqBA,4RAGA,wFACF,iBAAGA,WAAW,IACR,KAAQ,6DADd,YADE,QAG8B,iBAAGA,WAAW,IACxC,KAAQ,8BADkB,eAH9B,6CAMkC,iBAAGA,WAAW,IAC5C,KAAQ,4CADsB,UANlC,SASF,iBAAGA,WAAW,IACR,KAAQ,0CADd,eATE,gTAcA,6nBAMA,sdAIA,8LAC8C,iBAAGA,WAAW,IACxD,KAAQ,sCADkC,wBAD9C,8JAKkB,iBAAGA,WAAW,IAC5B,KAAQ,4CADM,iBALlB,wCAOmE,iBAAGA,WAAW,IAC7E,KAAQ,0IADuD,qCAPnE,KAYA,kBACE,GAAM,eACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,gBACR,aAAc,yBACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,gBAqBA,ucAIA,geAIA,gYAGF,iBAAGA,WAAW,IACR,KAAQ,iCADd,eAHE,gDAMF,iBAAGA,WAAW,IACR,KAAQ,8BADd,SANE,gEASA,kBACE,GAAM,0BACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,2BACR,aAAc,oCACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,2BAqBA,osBAK2F,iBAAGA,WAAW,IACrG,KAAQ,wDAD+E,2BAL3F,2WAYA,koBAMA,mBAAKC,MAAM,UACT,qBAAG,oBAAMD,WAAW,IAChB,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,UAPf,WAUD,oBAAMA,WAAW,OACb,UAAa,qCACb,MAAS,CACP,cAAiB,sBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,otBACnB,eAAkB,QAClB,QAAW,WAnBhB,OAsBL,mBAAKA,WAAW,OACR,UAAa,0BACb,IAAO,QACP,MAAS,QACT,IAAO,kGACP,OAAU,CAAC,uGAAwG,wGACnH,MAAS,kCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,SAtCd,YA0CL,gxBAOA,gTAE0C,iBAAGA,WAAW,IACpD,KAAQ,wDAD8B,WAF1C,KAKA,+hBAKA,kBACE,GAAM,qCACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,sCACR,aAAc,+CACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,sCAqBA,wjBAIgE,iBAAGA,WAAW,IAC1E,KAAQ,mCADoD,cAJhE,uBAQA,waAIF,iBAAGA,WAAW,IACR,KAAQ,iEADd,sBAJE,KAOA,2UAGA,sBACE,kBAAIA,WAAW,MAAf,qCAA0D,iBAAGA,WAAW,KACpE,KAAQ,oCAD8C,SAA1D,kCAGA,kBAAIA,WAAW,MAAK,iBAAGA,WAAW,KAC9B,KAAQ,sCADQ,QAApB,2EAGA,kBAAIA,WAAW,MAAf,kDAEF,imBAKA,4NAEA,+QAGA,kBACE,GAAM,UACN,MAAS,CACP,SAAY,aAEb,iBAAGA,WAAW,KACb,KAAQ,WACR,aAAc,oBACd,UAAa,iBACZ,mBAAKA,WAAW,IACf,MAAS,6BACT,MAAS,KACT,OAAU,KACV,UAAa,QACb,QAAW,aALZ,OAOL,oBAAMA,WAAW,MACT,KAAQ,eACR,EAAK,ujBAlBb,WAqBA,qBAAG,iBAAGA,WAAW,IACb,KAAQ,mDADT,yDAAH,2BAYJJ,EAAWM,gBAAiB","file":"component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx-d3e601e0e38c36da55f7.js","sourcesContent":["/**\n * Copyright Coqui GmbH\n *\n * Use of this source code is governed under the Apache License, Version 2.0\n * found at http://www.apache.org/licenses/LICENSE-2.0.\n */\n\nimport React, { useEffect } from 'react';\nimport { PageProps } from 'gatsby';\nimport RootLayout from 'layouts/Root';\nimport { SidebarLayout } from 'layouts/Sidebar';\nimport TitledLayout from 'layouts/Titled';\nimport SEO from 'components/SEO';\nimport { MarkdownProvider } from 'components/MarkdownProvider';\nimport { IPageData, IPageContext } from './types';\nimport GogleAnalyticsCookieConsent from 'components/Cookies';\nimport { consentedToGoogleAnalytics } from 'utils/GoogleAnalytics';\n\nconst BlogLayoutTemplate: React.FC<PageProps<IPageData, IPageContext>> = ({\n  children,\n  data,\n  pageContext\n}) => {\n  const { mdx } = data;\n\n  useEffect(() => {\n    consentedToGoogleAnalytics();\n  });\n\n  return (\n    <RootLayout>\n      <SEO\n        title={`${pageContext.frontmatter.title} / Blog`}\n        description={pageContext.frontmatter.description || mdx.excerpt}\n      />\n      <SidebarLayout>\n        <TitledLayout\n          title={pageContext.frontmatter.title}\n          subtitle={pageContext.frontmatter.description}\n          name={pageContext.frontmatter.name}\n          picture={pageContext.frontmatter.picture}\n          date={pageContext.frontmatter.date}\n          toc={mdx.tableOfContents.items}\n        >\n          <MarkdownProvider>{children}</MarkdownProvider>\n        </TitledLayout>\n      </SidebarLayout>\n      <GogleAnalyticsCookieConsent />\n    </RootLayout>\n  );\n};\n\nexport default BlogLayoutTemplate;\n","import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/coqui-ai.github.io/coqui-ai.github.io/src/templates/BlogTemplate.tsx\";\nimport { graphql } from 'gatsby';\nexport const pageQuery = graphql`\n  query($fileAbsolutePath: String) {\n    ...SidebarPageFragment\n  }\n`;\nexport const _frontmatter = {};\nconst layoutProps = {\n  pageQuery,\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n\n    <p>{`We believe speech interfaces will be a big part of how people interact with their devices in the future. Today we are\nexcited to announce the initial release of our `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/coqui-ai/STT/releases/tag/v0.1.0\"\n      }}>{`open source speech recognition\nmodel`}</a>{` so that anyone can develop compelling speech experiences.`}</p>\n    <p>{`The Machine Learning team has been working on an open source Automatic Speech Recognition engine modeled after the Deep\nSpeech papers (`}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1412.5567\"\n      }}>{`1`}</a>{`, `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1512.02595\"\n      }}>{`2`}</a>{`) published by Baidu. One of\nthe major goals from the beginning was to achieve a Word Error Rate in the transcriptions of under 10%. We have made\ngreat progress: Our word error rate on LibriSpeech’s test-clean set is 6.5%, which not only achieves our initial goal,\nbut gets us close to human level performance.`}</p>\n    <p>{`This post is an overview of the team’s efforts and ends with a more detailed explanation of the final piece of the\npuzzle: the CTC decoder.`}</p>\n    <h3 {...{\n      \"id\": \"the-architecture\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#the-architecture\",\n        \"aria-label\": \"the architecture permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`The Architecture`}</h3>\n    <p>{`Coqui STT is an end-to-end trainable, character-level, deep recurrent neural network\n(`}<a parentName=\"p\" {...{\n        \"href\": \"https://en.wikipedia.org/wiki/Recurrent_neural_network\"\n      }}>{`RNN`}</a>{`). In less buzzwordy terms: it’s a deep neural network\nwith recurrent layers that gets audio features as input and outputs characters directly — the transcription of the\naudio. It can be trained using supervised learning from scratch, without any external “sources of intelligence”, like a\ngrapheme to phoneme converter or forced alignment on the input.`}</p>\n    <p><img parentName=\"p\" {...{\n        \"src\": \"/static/blog-stt-a-journey-to-10-word-error-rate-architecture-1dd53cecf9b0f6023f3d4ed8eac5fa25.gif\",\n        \"alt\": \"IMAGE\"\n      }}></img></p>\n    <p>{`This animation shows how the data flows through the network. In practice, instead of processing slices of the audio\ninput individually, we do all slices at once.`}</p>\n    <p>{`The network has five layers: the input is fed into three fully connected layers, followed by a bidirectional RNN layer,\nand finally a fully connected layer. The hidden fully connected layers use the\n`}<a parentName=\"p\" {...{\n        \"href\": \"https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29\"\n      }}>{`ReLU`}</a>{` activation. The RNN layer uses LSTM cells with\ntanh activation.`}</p>\n    <p>{`The output of the network is a matrix of character probabilities over time. In other words, for each time step the\nnetwork outputs one probability for each character in the alphabet, which represents the likelihood of that character\ncorresponding to what’s being said in the audio at that time. The `}<a parentName=\"p\" {...{\n        \"href\": \"http://www.cs.toronto.edu/~graves/icml_2006.pdf\"\n      }}>{`CTC loss\nfunction`}</a>{` (PDF link) considers all alignments of the audio to the\ntranscription at the same time, allowing us to maximize the probability of the correct transcription being predicted\nwithout worrying about alignment. Finally, we train using the `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1412.6980\"\n      }}>{`Adam optimizer`}</a>{`.`}</p>\n    <h3 {...{\n      \"id\": \"the-data\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#the-data\",\n        \"aria-label\": \"the data permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`The Data`}</h3>\n    <p>{`Supervised learning requires data, lots and lots of it. Training a model like Coqui STT requires thousands of hours of\nlabeled audio, and obtaining and preparing this data can be as much work, if not more, as implementing the network and\nthe training logic.`}</p>\n    <p>{`We started by downloading freely available speech corpora like\n`}<a parentName=\"p\" {...{\n        \"href\": \"http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus\"\n      }}>{`TED-LIUM`}</a>{` and `}<a parentName=\"p\" {...{\n        \"href\": \"http://www.openslr.org/12/\"\n      }}>{`LibriSpeech`}</a>{`, as\nwell as acquiring paid corpora like `}<a parentName=\"p\" {...{\n        \"href\": \"https://catalog.ldc.upenn.edu/LDC2004S13\"\n      }}>{`Fisher`}</a>{` and\n`}<a parentName=\"p\" {...{\n        \"href\": \"https://catalog.ldc.upenn.edu/ldc97s62\"\n      }}>{`Switchboard`}</a>{`. We wrote importers in Python for the different data sets that\nconvert the audio files to WAV, split the audio and cleaned up the transcription of unneeded characters like punctuation\nand accents. Finally we stored the preprocessed data in CSV files that can be used to feed data into the network.`}</p>\n    <p>{`Using existing speech corpora allowed us to quickly start working on the model. But in order to achieve excellent\nresults, we needed a lot more data. We had to be creative. We thought that maybe this type of speech data would already\nexist out there, sitting in people’s archives, so we reached out to public TV and radio stations, language study\ndepartments in universities, and basically anyone who might have labeled speech data to share. Through this effort, we\nwere able to more than double the amount of training data we had to work with, which is now enough for training a\nhigh-quality English model.`}</p>\n    <p>{`Having a high-quality voice corpus publicly available not only helps advance our own speech recognition engine. It will\neventually allow for broad innovation because developers, startups and researchers around can train and experiment with\ndifferent architectures and models for different languages. It could help democratize access to deep learning for those\nwho can’t afford to pay for thousands of hours of training data (almost everyone).`}</p>\n    <p>{`To build a speech corpus that’s free, open source, and big enough to create meaningful products with, we worked with\nMozilla’s Open Innovation team and launched the `}<a parentName=\"p\" {...{\n        \"href\": \"https://commonvoice.mozilla.org/en\"\n      }}>{`Common Voice project`}</a>{` to collect\nand validate speech contributions from volunteers all over the world. Today, the team is releasing a large collection of\nvoice data into the `}<a parentName=\"p\" {...{\n        \"href\": \"https://creativecommons.org/choose/zero/\"\n      }}>{`public domain`}</a>{`. Find out more about the release on `}<a parentName=\"p\" {...{\n        \"href\": \"https://medium.com/mozilla-open-innovation/sharing-our-common-voice-mozilla-releases-second-largest-public-voice-data-set-e88f7d6b7666\"\n      }}>{`the\nOpen Innovation Medium\nblog`}</a>{`.`}</p>\n    <h3 {...{\n      \"id\": \"the-hardware\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#the-hardware\",\n        \"aria-label\": \"the hardware permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`The Hardware`}</h3>\n    <p>{`Coqui STT has over 120 million parameters, and training a model this large is a very computationally expensive task: you\nneed lots of GPUs if you don’t want to wait forever for results. We looked into training on the cloud, but it doesn’t\nwork financially: dedicated hardware pays for itself quite quickly if you do a lot of training. The cloud is a good way\nto do fast hyperparameter explorations though, so keep that in mind.`}</p>\n    <p>{`We started with a single machine running four Titan X Pascal GPUs, and then bought another two servers with 8 Titan XPs\neach. We run the two 8 GPU machines as a cluster, and the older 4 GPU machine is left independent to run smaller\nexperiments and test code changes that require more compute power than our development machines have. This setup is\nfairly efficient, and for our larger training runs we can go from zero to a good model in about a week.`}</p>\n    <p>{`Setting up distributed training with TensorFlow was an arduous process. Although it has the most mature distributed\ntraining tools of the available deep learning frameworks, getting things to actually work without bugs and to take full\nadvantage of the extra compute power is tricky. Our current setup works thanks to the incredible efforts of my colleague\n`}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/tilmankamp\"\n      }}>{`Tilman Kamp`}</a>{`, who endured long battles with TensorFlow,\n`}<a parentName=\"p\" {...{\n        \"href\": \"https://slurm.schedmd.com/\"\n      }}>{`Slurm`}</a>{`, and even the Linux kernel until we had everything working.`}</p>\n    <h3 {...{\n      \"id\": \"putting-it-all-together\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#putting-it-all-together\",\n        \"aria-label\": \"putting it all together permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`Putting it All Together`}</h3>\n    <p>{`At this point, we have two papers to guide us, a model implemented based on those papers, the resulting data, and the\nhardware required for the training process. It turns out that replicating the results of a paper isn’t that\nstraightforward. The vast majority of papers don’t specify all the hyperparameters they use, if they specify any at all.\nThis means you have to spend a whole lot of time and energy doing hyperparameter searches to find a good set of values.\nOur initial tests with values chosen through a mix of randomness and intuition weren’t even close to the ones reported\nby the paper, probably due to small differences in the architecture — for one, we used LSTM (`}<a parentName=\"p\" {...{\n        \"href\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\"\n      }}>{`Long short-term\nmemory`}</a>{`) cells instead of GRU (gated recurrent unit) cells. We\nspent a lot of time doing a binary search on dropout ratios, we reduced the learning rate, changed the way the weights\nwere initialized, and experimented with the size of the hidden layers as well. All of those changes got us pretty close\nto our desired target of <10% Word Error Rate, but not there.`}</p>\n    <p>{`One piece missing from our code was an important optimization: integrating our language model into the decoder. The CTC\n(Connectionist Temporal Classification) decoder works by taking the probability matrix that is output by the model and\nwalking over it looking for the most likely text sequence according to the probability matrix. If at time step 0 the\nletter “C” is the most likely, and at time step 1 the letter “A” is the most likely, and at time step 2 the letter “T”\nis the most likely, then the transcription given by the simplest possible decoder will be “CAT”. This strategy is called\ngreedy decoding.`}</p>\n    <div align=\"center\">\n      <p><span parentName=\"p\" {...{\n          \"className\": \"gatsby-resp-image-wrapper\",\n          \"style\": {\n            \"position\": \"relative\",\n            \"display\": \"block\",\n            \"marginLeft\": \"auto\",\n            \"marginRight\": \"auto\",\n            \"maxWidth\": \"500px\"\n          }\n        }}>{`\n      `}<span parentName=\"span\" {...{\n            \"className\": \"gatsby-resp-image-background-image\",\n            \"style\": {\n              \"paddingBottom\": \"57.599999999999994%\",\n              \"position\": \"relative\",\n              \"bottom\": \"0\",\n              \"left\": \"0\",\n              \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABuElEQVQoz2VSPa8BURC93m5EZKOjpRChkvADxH+gIERCJyQK29hCdDQ+KoVoSYhEQuIniEJoRINOfDQ0Nr7fO8+8tzbvTTGZO3POnLlzL0skEj6fTxTF4/H4fD6v1yv85z+73+8o3W43xOPxOJVKAcZarRbV4Il2Op0mk8lgMECp0Wj0er3VaoX84/GAr9VqOp3OZDKNRiMWCATK5TIQOMzn83q9brfb2a9pNBp4vV7v9/uLxWIkEuF5nkpms5lNp9N0Ou10OtFPzYF9vIz4akMvl8vl9XoZjYqxd7tdpVLhOA4EAnEvIz7/Mq1Wi3w2mwVFlmVGm6A7bzYbo9FIykoLtdFEWBjpMVom9C+XC4JoNEqaAMXjcUmSwuGwxWJBkm4bi8WI+b1t9WPAZzIZEvF4PDhiKPh8Pk9JzLVer5XNv5XhIY7NEa7b7dKzwZdKJavV6na7HQ4HgQn/Q6ZOi8XCYDC02+1gMIhXgSzlk8kkltRsNgVBOBwOihj785NCodBsNgMnl8t1Oh3MgvUUCgVw0Mtms+HbKOA3mUSGw2G1WkVwPp+XyyWC/X6/3W4J0+/30ZTyEP8CwP6ncwMl13MAAAAASUVORK5CYII=')\",\n              \"backgroundSize\": \"cover\",\n              \"display\": \"block\"\n            }\n          }}></span>{`\n  `}<img parentName=\"span\" {...{\n            \"className\": \"gatsby-resp-image-image\",\n            \"alt\": \"IMAGE\",\n            \"title\": \"IMAGE\",\n            \"src\": \"/static/951c29bfac70e155e5037020410b8cfe/c6e3d/blog-stt-a-journey-to-10-word-error-rate-cat.png\",\n            \"srcSet\": [\"/static/951c29bfac70e155e5037020410b8cfe/43fa5/blog-stt-a-journey-to-10-word-error-rate-cat.png 250w\", \"/static/951c29bfac70e155e5037020410b8cfe/c6e3d/blog-stt-a-journey-to-10-word-error-rate-cat.png 500w\"],\n            \"sizes\": \"(max-width: 500px) 100vw, 500px\",\n            \"style\": {\n              \"width\": \"100%\",\n              \"height\": \"100%\",\n              \"margin\": \"0\",\n              \"verticalAlign\": \"middle\",\n              \"position\": \"absolute\",\n              \"top\": \"0\",\n              \"left\": \"0\"\n            },\n            \"loading\": \"lazy\"\n          }}></img>{`\n    `}</span></p>\n    </div>\n    <p>{`This is a pretty good way of decoding the probabilities output by the model into a sequence of characters, but it has\none major flaw: it only takes into account the output of the network, which means it only takes into account the\ninformation from audio. When the same audio has two equally likely transcriptions (think “new” vs “knew”, “pause” vs\n“paws”), the model can only guess at which one is correct. This is far from optimal: if the first four words in a\nsentence are “the cat has tiny”, we can be pretty sure that the fifth word will be “paws” rather than “pause”. Answering\nthose types of questions is the job of a language model, and if we could integrate a language model into the decoding\nphase of our model, we could get way better results.`}</p>\n    <p>{`When we first tried to tackle this issue, we ran into a couple of blockers in TensorFlow: first, it doesn’t expose its\nbeam scoring functionality in the Python API (probably for performance reasons); and second, the log probabilities\noutput by the CTC loss function were (are?) `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/tensorflow/tensorflow/issues/6034\"\n      }}>{`invalid`}</a>{`.`}</p>\n    <p>{`We decided to work around the problem by building something like a spell checker instead: go through the transcription\nand see if there are any small modifications we can make that increase the likelihood of that transcription being valid\nEnglish, according to the language model. This did a pretty good job of correcting small spelling mistakes in the\noutput, but as we got closer and closer to our target error rate, we realized that it wasn’t going to be enough. We’d\nhave to bite the bullet and write some C++.`}</p>\n    <h3 {...{\n      \"id\": \"beam-scoring-with-a-language-model\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#beam-scoring-with-a-language-model\",\n        \"aria-label\": \"beam scoring with a language model permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`Beam Scoring with a Language Model`}</h3>\n    <p>{`Integrating the language model into the decoder involves querying the language model every time we evaluate an addition\nto the transcription. Going back to the previous example, when looking into whether we want to choose “paws” or “pause”\nfor the next word after “the cat has tiny”, we query the language model and use that score as a weight to sort the\ncandidate transcriptions. Now we get to use information not just from audio but also from our language model to decide\nwhich transcription is more likely. The algorithm is described in `}<a parentName=\"p\" {...{\n        \"href\": \"https://arxiv.org/abs/1408.2873\"\n      }}>{`this paper`}</a>{` by\nHannun et. al.`}</p>\n    <p>{`Luckily, TensorFlow does have an extension point on its CTC beam search decoder that allows the user to supply their own\nbeam scorer. This means all you have to do is write the beam scorer that queries the language model and plug that in.\nFor our case, we wanted that functionality to be exposed to our Python code, so we also exposed it as a custom\nTensorFlow operation that can be loaded using\n`}<a parentName=\"p\" {...{\n        \"href\": \"https://www.tensorflow.org/api_docs/python/tf/load_op_library\"\n      }}>{`tf.load_op_library`}</a>{`.`}</p>\n    <p>{`Getting all of this to work with our setup required quite a bit of effort, from fighting with the Bazel build system for\nhours, to making sure all the code was able to handle Unicode input in a consistent way, and debugging the beam scorer\nitself. The system requires quite a few pieces to work together:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`The language model itself (we use `}<a parentName=\"li\" {...{\n          \"href\": \"http://kheafield.com/code/kenlm/\"\n        }}>{`KenLM`}</a>{` for building and querying). A`}</li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://en.wikipedia.org/wiki/Trie\"\n        }}>{`trie`}</a>{` of all the words in our vocabulary. An alphabet file that maps integer`}</li>\n      <li parentName=\"ul\">{`labels output by the network into characters.`}</li>\n    </ul>\n    <p>{`Although adding this many moving parts does make our code harder to modify and apply to different use cases (like other\nlanguages), it brings great benefits: Our word error rate on LibriSpeech’s test-clean set went from 16% to 6.5%, which\nnot only achieves our initial goal, but gets us close to human level performance (5.83% according to the Deep Speech 2\npaper). On a MacBook Pro, using the GPU, the model can do inference at a real-time factor of around 0.3x, and around\n1.4x on the CPU alone. (A real-time factor of 1x means you can transcribe 1 second of audio in 1 second.)`}</p>\n    <p>{`It has been an incredible journey to get to this place: the initial release of our model! In the future we want to\nrelease a model that’s fast enough to run on a mobile device or a Raspberry Pi.`}</p>\n    <p>{`If this type of work sounds interesting or useful to you, come check out our repository on GitHub and our Discourse\nchannel. We have a growing community of contributors and we’re excited to help you create and publish a model for your\nlanguage.`}</p>\n    <h3 {...{\n      \"id\": \"license\",\n      \"style\": {\n        \"position\": \"relative\"\n      }\n    }}><a parentName=\"h3\" {...{\n        \"href\": \"#license\",\n        \"aria-label\": \"license permalink\",\n        \"className\": \"anchor before\"\n      }}><svg parentName=\"a\" {...{\n          \"xmlns\": \"http://www.w3.org/2000/svg\",\n          \"width\": \"16\",\n          \"height\": \"16\",\n          \"focusable\": \"false\",\n          \"viewBox\": \"0 0 16 16\"\n        }}>{`\n  `}<path parentName=\"svg\" {...{\n            \"fill\": \"currentColor\",\n            \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n          }}></path>\n        </svg></a>{`License`}</h3>\n    <p><a parentName=\"p\" {...{\n        \"href\": \"https://creativecommons.org/licenses/by-sa/3.0/\"\n      }}>{`Creative Commons Attribution Share-Alike License v3.0`}</a>{` or any later\nversion`}</p>\n    {\n      /* markdownlint-enable line-length */\n    }\n\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}