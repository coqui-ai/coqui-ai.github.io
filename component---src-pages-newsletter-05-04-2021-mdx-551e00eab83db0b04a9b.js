(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{Berg:function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return s})),a.d(t,"default",(function(){return p}));var n=a("zLVn"),o=(a("q1tI"),a("7ljp")),i=a("yDk1"),r=["components"],s={},l={pageQuery:"3484569904",_frontmatter:s},c=i.a;function p(e){var t=e.components,a=Object(n.a)(e,r);return Object(o.b)(c,Object.assign({},l,a,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"934px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"27.599999999999998%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA3XAAAN1wFCKJt4AAABCElEQVQY042RQUsCcRDF/VB9g+59gk6eOhTRpaBbh5CgYCuiTkYGah6CSIwiyiRbiiyVFg+5EoS6LO1/XS0oXffXriC1atCDOcyDeW/mTQAXjuMwhFHcH/g9H/CaPqG/W5T0V1ShUTbqjDIb7L/abR/vE8y8KEjyMfv5NHuFS87kO0LhCBvxBOuxBPHTc0K7EVJZGV2YhI+SZAvF3qzd7f4I9h287WpNgfho8WzWWdjaZnFzh+DSMtMra0zMzTO7KjE2GewJj0/NcOKKe+jY9vCGWsvkvlom55ZqamhvBqnrG26fFDIPeYTVRIoecHiR5ir3iKJWMBqW/+TBbDynz46by/9/4sv1G2shujMHTg19AAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png",srcSet:["/static/c9103bc33c8add5ab1fa4fa1c49c90ef/43fa5/logo-wordmark.png 250w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/c6e3d/logo-wordmark.png 500w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png 934w"],sizes:"(max-width: 934px) 100vw, 934px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("h3",{id:"welcome",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#welcome","aria-label":"welcome permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Welcome"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(o.b)("p",null,"Years ago now, I remember starting work on the speech recognition engine that would become the core of\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/STT"},"Coqui üê∏ STT"),". Then it was only a dream, creating open source speech\ntechnology that brought research into the hands of the enterprise and regular developers. But here we\nare, years later, and Coqui is a reality."),Object(o.b)("p",null,"We‚Äôve grown a lot since then. ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/STT"},"Coqui üê∏ STT")," has gone from aspiration\nto a actuality, powering the enterprise and providing speech technology to numerous low-resource languages.\nAlso, ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS"},"Coqui üê∏ TTS"),", through that same dream, was born, bringing open,\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS#-tts-performance"},"human-quality")," speech synthesis to regular developers\nand the enterprise."),Object(o.b)("p",null,"I am grateful that you are joining us on this journey, and I want to personally thank everyone who has ever subscribed\nto our newsletter, used our software, downloaded a model, filed a bug report, joined our\n",Object(o.b)("a",{parentName:"p",href:"https://gitter.im/coqui-ai"},"discussion forms"),", or simply given us a ‚≠ê star on GitHub. Thank you!"),Object(o.b)("p",null,"Stay tuned; there‚Äôs much more to come! üê∏"),Object(o.b)("p",null,"On to our first, monthly newsletter where you‚Äôll hear what we are up to and general reflections on speech tech!"),Object(o.b)("h3",{id:"coqui-stt-playbook",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#coqui-stt-playbook","aria-label":"coqui stt playbook permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Coqui STT Playbook"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/reuben"},"Reuben Morais")),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"61.6%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACt0lEQVQozw3SSVPaYACA4fygHnpsp1VLW6oioLIoWyAYSNgCX8AEvrCqkcgmYGQHCagglK2ijradTg/9YfX6Ht95EK3qk1Yt06tlWr0qmj6NpiAVZ0IpmBVLJ8KJcHFekq6EJJ0Ti8l8LhSmcQ9h2V2zqt9plR+RvW2Zzaj0WLcJ536hWr6sFqu9dutGKjcr9kPSEweNQVeajMtpNpsKFk5hLOwDhMGDKnGTEnGg2wHSGHDowyzVHQ2kQb/RFGvtWu4yH+SZEM9k6oXvj8t2NZMV4nySiUWDEQqjCUPIqUOcFjXP+Y9YFweB1BFnD4v58+NoMribDwez4fhhMb6fjn5MbxdTsXcl9RoXlWLYb48E7IA0Ib4DXcRvqwgQ0LgjiFV71UKrxOWSvCjEiqmq1Fw8L7uzu+7k7rLbvJmOa1I7cUgkGRdwWxDaZY4H8WLCTwcdvpTPHTvAoeOoJJCsg4B4RGDrt52LXmO4nBZq5c7wui51jhkiDjDGqUE4jyFMaE9DdobxNvotQcw0Rv2T8vG+d9cZsXN52Ok3pEF3/vJY79SG9/OG1C5AIh3CWHIPifnRgH2Ho6zRKB3OHqGUhggbUbCLApXWtQGOwOz5fvn75de/v9eT0fhp2epfHR/icfrg9TGCW9Q2gwIGsAjAYwnayWBmn0JDrO845ArbZxdHPP35eXM/ZcWzeCY6fny4aFUjfixGWXxWFfKKxKD5mmDdfkAGAFbp1/qLG3gCCIjtEAq9Rx8vJH0plyVkPKucT16ez8SSj7QAL3Zg0yMW3bpJK2fdJpNx643srVy/wvEwW8xDHprc+190coVZricUXs4TTfMwzVOHlNuqCnnRWMSPJAAWpdA8JBhyb3dzxapbd6Mqt1WN7m/Y9jZe8akUq+qtNaVCtvntw9a39xrl6mv0kmaX0/wfDtFUUQbKgn0AAAAASUVORK5CYII=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/13ebb059798b7e17243411a28f2fb044/da8b6/coqui-stt-playbook.png",srcSet:["/static/13ebb059798b7e17243411a28f2fb044/43fa5/coqui-stt-playbook.png 250w","/static/13ebb059798b7e17243411a28f2fb044/c6e3d/coqui-stt-playbook.png 500w","/static/13ebb059798b7e17243411a28f2fb044/da8b6/coqui-stt-playbook.png 1000w","/static/13ebb059798b7e17243411a28f2fb044/2e9ed/coqui-stt-playbook.png 1500w","/static/13ebb059798b7e17243411a28f2fb044/9fabd/coqui-stt-playbook.png 2000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"Getting started with speech-to-text can be somewhat intimidating. There are new terms to learn, tools and\npipelines to get acquainted with, and a whole lot of excitement about the speech experiences you can build.\nAt Coqui, we want developers to be able to focus entirely on that last part: let their creativity run wild\nso they can build applications, systems, and experiences using speech that will redefine how users interact\nwith technology in the future."),Object(o.b)("p",null,"With that in mind, Coqui co-founder ",Object(o.b)("a",{parentName:"p",href:"https://mobile.twitter.com/_josh_meyer_"},"Josh Meyer")," started a ‚Äúplaybook‚Äù:\na complement to our technical documentation that told a cohesive story from start to end, covering the entire\nprocess of getting familiar with Coqui üê∏ STT (and STT in general), defining all the pieces of the puzzle and\nhow they fit together, and then walking you through the process of collecting and refining your data,\ntraining models and getting confident in how they work, and finally deploying your model on your platform\nand in your programming language of choice."),Object(o.b)("p",null,"This idea resonated with the community, and things really kicked into high gear when ",Object(o.b)("a",{parentName:"p",href:"https://twitter.com/KathyReid"},"Kathy Reid"),"\njoined forces in the creation of the playbook. The result is an opinionated guide that is meant as a smooth\nonboarding process for getting familiar with STT, getting developers from zero to a working speech-to-text\nsystem, and giving them the knowledge and confidence to start tweaking with their processes and finding the\nbest tailored way to make speech work for them."),Object(o.b)("p",null,"At Coqui, we want to take this idea to the next level: we want developers to be able to build speech experiences\neffortlessly, as well as to easily share their work with the speech community - be it tools, models, new\narchitectures, or datasets. As a first step, we have released the updated ",Object(o.b)("a",{parentName:"p",href:"https://stt.readthedocs.io/en/latest/playbook/README.html"},"STT playbook"),",\nreadable now on our main documentation site. We will continue to refine our documentation, tools, and models\nworking together with our brilliant community to bring speech research into reality."),Object(o.b)("p",null,"Don‚Äôt forget to check out the updated ",Object(o.b)("a",{parentName:"p",href:"https://stt.readthedocs.io/en/latest/playbook/README.html"},"STT playbook"),"\nand ",Object(o.b)("a",{parentName:"p",href:"https://gitter.im/coqui-ai"},"join our growing speech community"),"."),Object(o.b)("h3",{id:"sc-glowtts-an-efficient-zero-shot-multi-speaker-tts-model",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#sc-glowtts-an-efficient-zero-shot-multi-speaker-tts-model","aria-label":"sc glowtts an efficient zero shot multi speaker tts model permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker TTS Model"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"743px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"60.4%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABgUlEQVQoz5WSwW7TQBRF+xks+RE+gj2/wIIN/QV2LFBXVKoEG8QWCYRS1KobiFqqNlIINCq0ceoQO47t2jPjsWd8eIpYVakUz/rqzLvnvS3WvLZt2eSty23dF2qcJ44ixr9GBJMJVWXpfTvn9ft99t5+YhYGa6H3Ao0A8smAP997FLMxjfc8ebHLg8fbPHz0lIODk1XOyccbAa21RJenXA+OyIORjFzzrnfEs1c7PH+5w8XwapXz3m8+Yfz7jPHJPmnwkzpXZHnE5+OPHA6+oGvVrbKuHDc3Uy5GQ5I0l26ecHrFYb/P19MztDLdgFnZkOYJTZGQ3SpUqUmXAfN4RJZKXa1ouwBL06CNwVVGIAlFqWTqhnhZiA4nTswKyMZbto75ImUazim0pRZGlpfM5tFKQV10ctjKHbYyYU28WFKKL20sy6wgDP+iBOiU7rgUHYmzH1yHQz7033B+eSy15ZSiBaU4bRrXDehcja3El1UktxGlLrC1R5kKI/39/9xd4D+GIpv/qdfXDwAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/3a26d3b6eb8dcc0c81662bc544ecc10f/b217e/sc-glowtts.png",srcSet:["/static/3a26d3b6eb8dcc0c81662bc544ecc10f/43fa5/sc-glowtts.png 250w","/static/3a26d3b6eb8dcc0c81662bc544ecc10f/c6e3d/sc-glowtts.png 500w","/static/3a26d3b6eb8dcc0c81662bc544ecc10f/b217e/sc-glowtts.png 743w"],sizes:"(max-width: 743px) 100vw, 743px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"At Coqui, we‚Äôre motivated to provide speech technology for all languages and people. One of the problems we‚Äôve\nencountered along way is data-hungry machine learning models. For some languages the data simply isn‚Äôt there!\nFinding enough data is hard, and even if it‚Äôs available, training machine learning models is difficult too."),Object(o.b)("p",null,"To solve a part of this problem for text-to-speech tasks, we investigated different ",Object(o.b)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Zero-shot_learning"},"zero-shot learning"),"\napproaches using state-of-the-art text-to-speech models. (Zero-shot learning techniquies can greatly reduce\ndata requirements for some algorithms.) This investigation bore fruit! We came up with a new algorithm we\nchristened ‚ÄúSC-GlowTTS‚Äù, a catchy name I know. SC-GlowTTS can generalize to novel speakers after training\nwith only 11 speakers for the target language. This means we need less data!"),Object(o.b)("p",null,"Soon after this newsletter finds its way into your hands, we‚Äôll release SC-GlowTTS‚Äôs code, models, and an\nassociated article. Please stay tuned! But, if you can‚Äôt wait, check out the SC-GlowTTS\n",Object(o.b)("a",{parentName:"p",href:"https://edresson.github.io/SC-GlowTTS/"},"project page"),"."),Object(o.b)("p",null,"This is a work of all the Coqui üê∏ TTS community but special thanks to the main author\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/Edresson"},"Edresson Casanova")," who organized and did the brunt of the work and\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/mueller91"},"Nicolas Michael M√ºller")," for training the most expansive open\nsource speaker encoder, which we used in this work."),Object(o.b)("p",null,"It is great to see how an open community of great developers and researchers can innovate without borders.\nWe hope this is just a start for us and our great community to pave the way for open source TTS."),Object(o.b)("h3",{id:"few-shot-keyword-spotting-in-any-language",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#few-shot-keyword-spotting-in-any-language","aria-label":"few shot keyword spotting in any language permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Few-Shot Keyword Spotting in Any Language"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/JRMeyer"},"Josh Meyer")),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"980px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"34.800000000000004%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABkElEQVQozy2RW4/SQBiG+//vvTXGRBM1MXpldl3NaswKrBpZMbstZaGFaQeYAoWeT4/TwTeZZDLPfO93spqmIctKAvsPqx+f8b59Rf616dUz5/Ytdx/e4X2ZIG89EhEbVqQpL1+85+nlBR/dCT/VknV+xKqbmkjFLIZXhDeXzK/HyPvgHJQnjK+eY1/8Qo0jUpFSJ7VhR7XjybM3vLofMYlDRH4gbyusY5IRuC6bwSeCO0UkCqq8pWlByUec6xuSJf/VmdN2HUt3wevRdxSFIcJfkiYJVpzkhJPfrEY2h6ik1VlMu9pwI2Yshw+0uqiqKgiE0Lw1pjNtOJw9mL+1Hs3jYk6e51hZUeENBqj5Vps0TJ2pydRrL1cE44W5l0WBY9sUxbkiz1/hhefRtG2H4zgkpxNWmuXIqcvpcNTLyZBhyEmDXnEUsVspqrom0UlCzeL4vBSlFPvDgbIqydKMzXpt4q1af+6BCAS+7xPKULd3bruf1W6/x/M8XZHHdrul6zrD+uDeVOgxBEGAlNK8/QO3dQwIWWFougAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/2c3d0d168ad5f9315b408b2df295551f/2b72d/few-shot-keyword-spotting-in-any-language.png",srcSet:["/static/2c3d0d168ad5f9315b408b2df295551f/43fa5/few-shot-keyword-spotting-in-any-language.png 250w","/static/2c3d0d168ad5f9315b408b2df295551f/c6e3d/few-shot-keyword-spotting-in-any-language.png 500w","/static/2c3d0d168ad5f9315b408b2df295551f/2b72d/few-shot-keyword-spotting-in-any-language.png 980w"],sizes:"(max-width: 980px) 100vw, 980px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"We have traditionally focused on open vocabulary speech-to-text at Coqui, but now we are expanding to new\nhorizons."),Object(o.b)("p",null,"The flexibility of open vocabulary STT is great: you can say literally anything to a\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/STT"},"Coqui üê∏ STT")," model, and it will transcribe it. However, there are many\napplications where you don‚Äôt need to transcribe every word, you just need to spot a few keywords in the\nstream of audio. One common application of this is wake word detection, e.g. ‚Äù",Object(o.b)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=1ZXugicgn6U"},"Hey, Computer"),"‚Äù.\nFor the task of wake word detection, deploying a large, open vocabulary STT model is impractical on edge devices.\n(Edge devices have very small compute resources and assume limited power consumption.) As such, we‚Äôve begun\nresearch into robust, multilingual, practical model architectures for keyword spotting."),Object(o.b)("p",null,"Collaborating with researchers from ",Object(o.b)("a",{parentName:"p",href:"https://edge.seas.harvard.edu/"},"Harvard University")," and\n",Object(o.b)("a",{parentName:"p",href:"https://www.tensorflow.org/lite"},"Google"),", Coqui co-authored a publication on an\nextremely efficient keyword spotting technique that scales to any language. The training technique is\nintuitive and it works: we first train a base model to classify lots of keywords in lots of different\nlanguages. This is a classification model, not a transcription model. This base model does really well\nfor the keywords on which it was trained, but we really care about how well this model can perform on\nnew keywords that it‚Äôs never heard in new languages. The answer: it does great! We can fine-tune this\nbase model to any new keyword in any language with just five audio clips of the new word."),Object(o.b)("p",null,"We submitted this work to the ",Object(o.b)("a",{parentName:"p",href:"https://www.interspeech2021.org/"},"INTERSPEECH 2021")," conference. Wish us luck\non acceptance! Until INTERSPEECH 2021, let us whet your appetite with this teaser, the abstract for the\npaper:"),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"Abstract:"),"\nWe introduce a few-shot transfer learning method for key-word spotting in any language. Leveraging open\nspeech corpora in nine languages, we automate the extraction of a large multilingual keyword bank and\nuse it to train an embedding model. With just five training examples, we fine-tune the embedding model\nfor keyword spotting and achieve an average F1 score of 0.75 on keyword classification for 180 new\nkeywords unseen by the embedding model in these nine languages. This embedding model also generalizes\nto new languages. We achieve an average F1 score of 0.65 on 5-shot models for 260 keywords sampled\nacross 13 new languages unseen by the embedding model. We investigate streaming accuracy for our 5-shot\nmodels in two contexts: keyword spotting and keyword search. Across 440 keywords in 22 languages,\nwe achieve an average streaming keyword spotting accuracy of 85.2% with a false acceptance rate of\n1.2%, and observe promising initial results on keyword search."),Object(o.b)("h3",{id:"new-release--tts-v0011",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#new-release--tts-v0011","aria-label":"new release  tts v0011 permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"New Release: üê∏ TTS v0.0.11"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(o.b)("p",null,"Oh, one more thing!"),Object(o.b)("p",null,"We are happy to release two new German üê∏ TTS models trained and shared by the great ",Object(o.b)("a",{parentName:"p",href:"https://twitter.com/ThorstenVoice"},"ThorstenVoice"),"."),Object(o.b)("p",null,"You can see a list of all the released models and you can start using them with the simple command line calls:"),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-terminal"},'> pip install -U tts\n> tts --list_models\n> tts --text "Coqui TTS is great!" --out_path path/to/save/output.wav\n> tts --model_name tts_models/de/thorsten/tacotron2-DCA --text "Coqui TTS ist bereit, Deutsch zu sprechen." --out_path output.wav\n')))}p.isMDXComponent=!0},yDk1:function(e,t,a){"use strict";var n=a("q1tI"),o=a.n(n),i=a("O9mE"),r=a("v+Ly"),s=a("mrST"),l=a("1Yd/"),c=a("ozyN"),p=a("7cfw"),b=a("t4Fg");t.a=function(e){var t=e.children,a=e.data,h=e.pageContext,g=a.mdx;return Object(n.useEffect)((function(){Object(b.a)()})),o.a.createElement(i.a,null,o.a.createElement(l.a,{title:h.frontmatter.title+" / Newsletter",description:h.frontmatter.description||g.excerpt}),o.a.createElement(r.a,null,o.a.createElement(s.a,{title:h.frontmatter.title,subtitle:h.frontmatter.description,name:h.frontmatter.name,picture:h.frontmatter.picture,date:h.frontmatter.date,toc:g.tableOfContents.items},o.a.createElement(c.a,null,t))),o.a.createElement(p.a,null))}}}]);
//# sourceMappingURL=component---src-pages-newsletter-05-04-2021-mdx-551e00eab83db0b04a9b.js.map