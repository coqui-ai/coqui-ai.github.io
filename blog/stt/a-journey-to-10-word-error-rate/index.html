<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.2549702dff56d075d6ff.css" id="gatsby-global-css">/*!
 * Copyright Zendesk, Inc.
 *
 * Use of this source code is governed under the Apache License, Version 2.0
 * found at http://www.apache.org/licenses/LICENSE-2.0.
 */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}[hidden],template{display:none}html{background-color:#fff;min-height:100%;box-sizing:border-box;overflow-y:scroll;line-height:20px;color:#2f3941;font-feature-settings:"kern","kern";-webkit-font-kerning:normal;font-kerning:normal;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,Arial,sans-serif;font-size:14px}*{font-weight:inherit}*,:after,:before{box-sizing:inherit}a{color:#1f73b7}a,ins,u{text-decoration:none}a:focus,a:hover{text-decoration:underline;color:#1f73b7}a:focus{outline:none}b{font-weight:600}button{cursor:pointer;padding:0}button,input,optgroup,select,textarea{line-height:inherit;font-family:inherit}code{font-size:.95em}code,kbd,pre,samp{font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace}em{font-style:inherit}fieldset,iframe{border:0}h1,h2,h3,h4,h5,h6{font-size:inherit}blockquote,dd,dl,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,ol,p,pre,ul{margin:0;padding:0}hr{border:none;border-top:1px solid}ol,ul{list-style:none}img{max-width:100%}strong{font-weight:inherit}svg{max-height:100%}[tabindex="-1"]:focus{outline:none!important}</style><meta name="generator" content="Gatsby 2.27.5"/><title data-react-helmet="true">A Journey to &lt;10% Word Error Rate / Blog / Coqui</title><link data-react-helmet="true" rel="mask-icon" href="/mask-icon.svg" color="#03363d"/><link data-react-helmet="true" rel="apple-touch-icon" href="/apple-touch-icon.png"/><link data-react-helmet="true" rel="shortcut icon" href="/favicon.ico"/><meta data-react-helmet="true" name="application-name" content="Coqui"/><meta data-react-helmet="true" name="description" content="We believe speech interfaces will be a big part of how people interact with their devices in the future. Today we are
excited to announce…"/><meta data-react-helmet="true" name="msapplication-config" content="/browserconfig.xml"/><meta data-react-helmet="true" property="og:title" content="Coqui"/><meta data-react-helmet="true" property="og:description" content="Coqui, Freeing Speech."/><meta data-react-helmet="true" property="og:image" content="https://coqui.ai/og-image.png"/><meta data-react-helmet="true" property="og:image:alt" content="Coqui"/><meta data-react-helmet="true" property="og:image:width" content="1280"/><meta data-react-helmet="true" property="og:image:height" content="640"/><meta data-react-helmet="true" property="twitter:card" content="summary_large_image"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><style data-styled="boRYJl frPItv  hJGfqI gbWTPN esrYoz iTpVGQ iqDliS Fwlym eGBoZs kpuXOb ifalUs jIHHbm ZurFO dmeNDE dEzVDV fqXuiB hJmzvI bJsLke tqxgC gKyVMe fOLerP cEEVdI jJwXiO iARHbT cxJGyA hKRDwX jnzUYZ hUigtZ cdcKhH bSMrqM cgPVmz gBLfIq dYOvul ZLMZX fWmrzw EQyIi jeeFUs bINUcJ gGFgqj fbGZzl gjZoLh kKEUTC hmdGyC bUpFSy jDgxFn gaUkbp ePmrNi jCZXmA hCsyko dcLymE kdJXOp dIPvJZ jOzwVI dSuWUw kPitly bRvpwy kNCRIy iRhBUd cyVuQe dAyBhk jUlaGX jJClhf gXlMSe mZaVr gnMBjg iBbHyF dJVuoI cVciVx dJejfB" data-styled-version="4.4.1">
/* sc-component-id: gatsby-theme___StyledDiv-x2kfdx-0 */
.boRYJl{width:100%;height:100%;}
/* sc-component-id: sc-htoDjs */
.gbWTPN{-webkit-transition:opacity 0.2s ease-out,clip 0s linear 0.2s;transition:opacity 0.2s ease-out,clip 0s linear 0.2s;opacity:0;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;position:absolute;left:50%;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);z-index:2;border:1px solid;border-radius:4px;-webkit-text-decoration:underline;text-decoration:underline;white-space:nowrap;top:26px;padding:20px;padding-left:16px;line-height:1.4285714285714286;font-size:14px;border-color:#d8dcde;box-shadow:0 20px 28px 0 rgba(23,73,77,0.15);background-color:#fff;color:#1f73b7;} .gbWTPN:focus{-webkit-transition:opacity 0.2s ease-in-out;transition:opacity 0.2s ease-in-out;-webkit-animation:0.2s cubic-bezier(0.15,0.85,0.35,1.2) jtXLs;animation:0.2s cubic-bezier(0.15,0.85,0.35,1.2) jtXLs;opacity:1;-webkit-clip:auto;clip:auto;} .gbWTPN:focus{outline:none;} .gbWTPN:hover,.gbWTPN:focus{color:#1f73b7;}
/* sc-component-id: sc-dnqmqq */
.esrYoz{color:#68737d;margin-right:8px;width:16px;height:16px;}
/* sc-component-id: sc-hSdWYo */
.ZurFO{-webkit-transition:-webkit-transform 0.25s ease-in-out;-webkit-transition:transform 0.25s ease-in-out;transition:transform 0.25s ease-in-out;}
/* sc-component-id: sc-cvbbAY */
.bINUcJ{display:inline;-webkit-transition: border-color 0.25s ease-in-out, box-shadow 0.1s ease-in-out, background-color 0.25s ease-in-out, color 0.25s ease-in-out;transition: border-color 0.25s ease-in-out, box-shadow 0.1s ease-in-out, background-color 0.25s ease-in-out, color 0.25s ease-in-out;margin:0;border:none;border-radius:0;cursor:pointer;overflow:hidden;-webkit-text-decoration:none;text-decoration:none;text-overflow:ellipsis;font-family:inherit;font-weight:inherit;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;-webkit-touch-callout:none;padding:0;font-size:inherit;background-color:transparent;color:#1f73b7;} .bINUcJ::-moz-focus-inner{border:0;padding:0;} .bINUcJ:focus{outline:none;-webkit-text-decoration:none;text-decoration:none;} .bINUcJ:hover{-webkit-text-decoration:underline;text-decoration:underline;} .bINUcJ[data-garden-focus-visible]{-webkit-text-decoration:underline;text-decoration:underline;} .bINUcJ:active,.bINUcJ[aria-pressed='true'],.bINUcJ[aria-pressed='mixed']{-webkit-transition: border-color 0.1s ease-in-out, background-color 0.1s ease-in-out, color 0.1s ease-in-out;transition: border-color 0.1s ease-in-out, background-color 0.1s ease-in-out, color 0.1s ease-in-out;-webkit-text-decoration:underline;text-decoration:underline;} .bINUcJ:hover,.bINUcJ:focus,.bINUcJ[data-garden-focus-visible]{color:#144a75;} .bINUcJ:active,.bINUcJ[aria-pressed='true'],.bINUcJ[aria-pressed='mixed']{color:#0f3554;} .bINUcJ:disabled{color:#5293c7;} .bINUcJ:disabled{cursor:default;-webkit-text-decoration:none;text-decoration:none;} .bINUcJ .sc-hSdWYo{width:16px;min-width:16px;height:16px;vertical-align:middle;} .sc-iAyFgw .bINUcJ{position:relative;margin-left:-1px;} .sc-iAyFgw .bINUcJ:hover,.sc-iAyFgw .bINUcJ:active{z-index:1;} .sc-iAyFgw .bINUcJ:disabled{z-index:-1;border-top-width:0;border-bottom-width:0;border-right-color:#fff;border-left-color:#fff;} .sc-iAyFgw .bINUcJ:first-of-type:not(:last-of-type){margin-left:0;border-top-right-radius:0;border-bottom-right-radius:0;} .sc-iAyFgw .bINUcJ:last-of-type:not(:first-of-type){border-top-left-radius:0;border-bottom-left-radius:0;} .sc-iAyFgw .bINUcJ:not(:first-of-type):not(:last-of-type){border-radius:0;}
/* sc-component-id: sc-jWBwVP */
.kKEUTC{margin-bottom:-0.085em;padding-left:0.25em;box-sizing:content-box;width:0.85em;height:0.85em;}
/* sc-component-id: sc-brqgnP */
.jIHHbm{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-transition: border-color 0.25s ease-in-out, box-shadow 0.1s ease-in-out, background-color 0.25s ease-in-out, color 0.25s ease-in-out;transition: border-color 0.25s ease-in-out, box-shadow 0.1s ease-in-out, background-color 0.25s ease-in-out, color 0.25s ease-in-out;margin:0;border:1px solid transparent;border-radius:4px;cursor:pointer;overflow:hidden;-webkit-text-decoration:none;text-decoration:none;text-overflow:ellipsis;white-space:nowrap;font-family:inherit;font-weight:400;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-touch-callout:none;padding:0 1.3571428571428572em;height:48px;line-height:46px;font-size:14px;background-color:transparent;color:#1f73b7;border:none;padding:0;width:48px;min-width:48px;color:#68737d;} .jIHHbm::-moz-focus-inner{border:0;padding:0;} .jIHHbm:focus{outline:none;} .jIHHbm:hover{-webkit-text-decoration:none;text-decoration:none;} .jIHHbm[data-garden-focus-visible]{-webkit-text-decoration:none;text-decoration:none;} .jIHHbm:active,.jIHHbm[aria-pressed='true'],.jIHHbm[aria-pressed='mixed']{-webkit-transition: border-color 0.1s ease-in-out, background-color 0.1s ease-in-out, color 0.1s ease-in-out;transition: border-color 0.1s ease-in-out, background-color 0.1s ease-in-out, color 0.1s ease-in-out;-webkit-text-decoration:none;text-decoration:none;} .jIHHbm:hover{background-color:rgba(31,115,183,0.08);color:#144a75;} .jIHHbm[data-garden-focus-visible]{box-shadow:  0 0 0 3px rgba(31,115,183,0.35);} .jIHHbm:active,.jIHHbm[aria-pressed='true'],.jIHHbm[aria-pressed='mixed']{background-color:rgba(31,115,183,0.2);color:#0f3554;} .jIHHbm:disabled{border-color:transparent;background-color:#cee2f2;color:#5293c7;} .jIHHbm:disabled{cursor:default;} .jIHHbm .sc-hSdWYo{width:16px;min-width:16px;height:16px;} .sc-iAyFgw .jIHHbm{position:relative;margin-left:-1px;} .sc-iAyFgw .jIHHbm:hover,.sc-iAyFgw .jIHHbm:active{z-index:1;} .sc-iAyFgw .jIHHbm:disabled{z-index:-1;border-top-width:0;border-bottom-width:0;border-right-color:#fff;border-left-color:#fff;} .sc-iAyFgw .jIHHbm:first-of-type:not(:last-of-type){margin-left:0;border-top-right-radius:0;border-bottom-right-radius:0;} .sc-iAyFgw .jIHHbm:last-of-type:not(:first-of-type){border-top-left-radius:0;border-bottom-left-radius:0;} .sc-iAyFgw .jIHHbm:not(:first-of-type):not(:last-of-type){border-radius:0;} .jIHHbm:hover{color:#49545c;} .jIHHbm:active,.jIHHbm[aria-pressed='true'],.jIHHbm[aria-pressed='mixed']{color:#2f3941;} .jIHHbm .sc-hSdWYo{width:16px;height:16px;} .jIHHbm .sc-hSdWYo > svg{-webkit-transition:opacity 0.15s ease-in-out;transition:opacity 0.15s ease-in-out;}
/* sc-component-id: MaxWidth__MaxWidthLayout-sc-1rbvyso-0 */
.tqxgC{margin-right:auto;margin-left:auto;max-width:2880px;}
/* sc-component-id: Footer__StyledFooterItem-owx86q-0 */
.dJVuoI{margin-right:20px;color:#fff;} @media (max-width:767.98px){.dJVuoI{margin-left:20px;}} .dJVuoI:hover,.dJVuoI:focus{color:inherit;}
/* sc-component-id: Footer___StyledFooter-owx86q-1 */
.jOzwVI{background-color:#03363d;padding:20px;line-height:20px;color:#fff;font-size:14px;}
/* sc-component-id: Footer___StyledDiv-owx86q-2 */
.dSuWUw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-bottom:20px;} @media (max-width:767.98px){.dSuWUw{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding-right:36px;padding-left:36px;text-align:center;}}
/* sc-component-id: Footer___StyledDiv2-owx86q-3 */
.kPitly{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledGitHubIcon-owx86q-4 */
.bRvpwy{margin-right:32px;width:26px;height:26px;color:#fff;}
/* sc-component-id: Footer___StyledDiv3-owx86q-5 */
.kNCRIy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledTwitterIcon-owx86q-6 */
.iRhBUd{margin-right:32px;width:26px;height:26px;color:#fff;}
/* sc-component-id: Footer___StyledDiv4-owx86q-7 */
.cyVuQe{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledFacebookIcon-owx86q-8 */
.dAyBhk{margin-right:32px;width:26px;height:26px;color:#fff;}
/* sc-component-id: Footer___StyledDiv5-owx86q-9 */
.jUlaGX{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledLinkedInIcon-owx86q-10 */
.jJClhf{margin-right:32px;width:26px;height:26px;color:#fff;}
/* sc-component-id: Footer___StyledDiv6-owx86q-11 */
.gXlMSe{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledGitterIcon-owx86q-12 */
.mZaVr{width:26px;height:26px;color:#fff;}
/* sc-component-id: Footer___StyledDiv7-owx86q-13 */
.gnMBjg{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-top:1px solid #467b7c;padding-top:12px;} @media (max-width:767.98px){.gnMBjg{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;text-align:center;}}
/* sc-component-id: Footer___StyledDiv8-owx86q-14 */
.iBbHyF{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-basis:36px;-ms-flex-preferred-size:36px;flex-basis:36px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledDiv9-owx86q-15 */
.cVciVx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-basis:36px;-ms-flex-preferred-size:36px;flex-basis:36px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Footer___StyledDiv10-owx86q-16 */
@media (max-width:767.98px){.dJejfB{margin-top:12px;width:100%;text-align:center;}}
/* sc-component-id: Header__StyledDesktopNavItem-sc-1bc1xqp-0 */
.dEzVDV{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:12px;}
/* sc-component-id: Header__StyledDesktopNavLink-sc-1bc1xqp-1 */
.fqXuiB{border-radius:4px;padding:6px 8px;color:#2f3941;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;} .fqXuiB:focus,.fqXuiB:hover,.fqXuiB:active{-webkit-text-decoration:none;text-decoration:none;color:inherit;} .fqXuiB.is-current{background-color:rgba(47,57,65,0.1);} .fqXuiB[data-garden-focus-visible]{box-shadow:0 0 0 3px rgba(82,147,199,0.35);} .fqXuiB:hover{background-color:rgba(47,57,65,0.05);} .fqXuiB:active{background-color:rgba(47,57,65,0.2);}
/* sc-component-id: Header__StyledHeader-sc-1bc1xqp-2 */
.iTpVGQ{z-index:1;box-shadow:0 16px 24px 0 rgba(47,57,65,0.05);padding:0 16px;height:80px;} .iTpVGQ[data-show-navigation='true']{border-bottom-color:#fff;} @media (max-width:767.98px){.iTpVGQ{padding:0;height:60px;}}
/* sc-component-id: Header___StyledDiv-sc-1bc1xqp-3 */
.Fwlym{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:left;-webkit-justify-content:left;-ms-flex-pack:left;justify-content:left;} @media (max-width:767.98px){.Fwlym{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-left:20px;}}
/* sc-component-id: Header___StyledDiv2-sc-1bc1xqp-4 */
.eGBoZs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
/* sc-component-id: Header___StyledImg-sc-1bc1xqp-5 */
@media (max-width:575.98px){.kpuXOb{height:26pxpx;}}
/* sc-component-id: Header___StyledDiv3-sc-1bc1xqp-6 */
.ifalUs{padding:6px;} @media (min-width:768px){.ifalUs{display:none;}}
/* sc-component-id: Header___StyledNav-sc-1bc1xqp-9 */
.dmeNDE{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;} @media (max-width:767.98px){.dmeNDE{display:none;}}
/* sc-component-id: Header___StyledMaxWidthLayout-sc-1bc1xqp-10 */
.iqDliS{margin-right:auto;margin-left:auto;max-width:2880px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:100%;min-height:100%;}
/* sc-component-id: sc-global-2373854496 */
*{-ms-overflow-style:-ms-autohiding-scrollbar;}
/* sc-component-id: Root___StyledDiv-k2xmz5-0 */
.frPItv{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;min-height:100vh;}
/* sc-component-id: Root___StyledSkipNav-k2xmz5-1 */
.hJGfqI{top:40px;box-shadow:0 16px 24px 0 rgba(47,57,65,0.05);}
/* sc-component-id: Root___StyledMain-k2xmz5-2 */
.hJmzvI{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;}
/* sc-component-id: sc-cMljjf */
.jnzUYZ{box-sizing:border-box;width:100%;padding-right:10px;padding-left:10px;} @media (min-width:0px){} @media (min-width:576px){} @media (min-width:768px){} @media (min-width:992px){.jnzUYZ{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;max-width:100%;}} @media (min-width:1200px){.jnzUYZ{-webkit-flex-basis:75%;-ms-flex-preferred-size:75%;flex-basis:75%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;max-width:75%;}}.hUigtZ{box-sizing:border-box;width:100%;padding-right:10px;padding-left:10px;} @media (min-width:0px){} @media (min-width:576px){} @media (min-width:768px){} @media (min-width:992px){.hUigtZ{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;max-width:100%;}} @media (min-width:1200px){.hUigtZ{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;max-width:25%;}}
/* sc-component-id: sc-jAaTju */
.cxJGyA{margin-right:auto;margin-left:auto;width:100%;box-sizing:border-box;padding-right:10px;padding-left:10px;}
/* sc-component-id: sc-jDwBTQ */
.hKRDwX{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-10px;margin-left:-10px;} @media (min-width:0px){} @media (min-width:576px){} @media (min-width:768px){} @media (min-width:992px){} @media (min-width:1200px){}
/* sc-component-id: sc-iRbamj */
.bSMrqM{line-height:44px;font-size:36px;font-weight:400;direction:ltr;}.cgPVmz{line-height:24px;font-size:18px;font-weight:400;direction:ltr;}.gBLfIq{line-height:20px;font-size:14px;font-weight:400;direction:ltr;}.dYOvul{line-height:32px;font-size:26px;font-weight:400;direction:ltr;}
/* sc-component-id: sc-fBuWsC */
.jDgxFn{direction:ltr;margin:0;margin-left:24px;padding:0;list-style-position:outside;list-style-type:disc;}
/* sc-component-id: sc-fAjcbJ */
.ePmrNi{direction:ltr;padding:2px 0;}
/* sc-component-id: sc-caSCKo */
.fbGZzl{margin:0;padding:0;direction:ltr;} blockquote + .fbGZzl,.fbGZzl + .fbGZzl{margin-top:20px;}
/* sc-component-id: Anchor__StyledAnchor-sc-1q3ov98-0 */
.gjZoLh.anchor.before{position:relative;margin-left:-32px;border-radius:4px;padding:0 16px;color:transparent;} @media (max-width:991.98px){.gjZoLh.anchor.before{margin-left:-24px;padding:0 12px;}} .gjZoLh.anchor.before[data-garden-focus-visible]{box-shadow:inset 0 0 0 2px rgba(31,115,183,0.35);color:inherit;} .gjZoLh.anchor.before > svg{position:absolute;top:50%;-webkit-transform:translate(-50%,-50%);-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%);}
/* sc-component-id: Typography__StyledH2-xo896r-1 */
.cdcKhH{margin-bottom:12px;color:#03363d;font-weight:600;margin-top:20px;margin-bottom:20px;} .cdcKhH:hover .Anchor__StyledAnchor-sc-1q3ov98-0{color:inherit;}
/* sc-component-id: Typography__StyledH3-xo896r-2 */
.hmdGyC{margin-bottom:12px;color:#03363d;font-weight:600;} .hmdGyC:hover .Anchor__StyledAnchor-sc-1q3ov98-0{color:inherit;}
/* sc-component-id: Typography__StyledHr-xo896r-7 */
.fWmrzw{margin:20px 0;border-top:1px solid #e9ebed;}
/* sc-component-id: Typography__StyledParagraph-xo896r-8 */
.gGFgqj{margin-bottom:20px;}
/* sc-component-id: DesktopSidebar___StyledNav-sc-1ezn4du-0 */
.fOLerP{background-color:#fff;padding:32px 20px 32px 0;min-width:220px;} @media (max-width:1440px){.fOLerP{padding:32px 20px;}} @media (max-width:1199.98px){.fOLerP{display:none;}}
/* sc-component-id: DesktopSidebar___StyledDiv-sc-1ezn4du-1 */
.cEEVdI{position:absolute;top:0;right:50%;bottom:0;left:0;z-index:-1;background-color:#fff;} @media (max-width:1199.98px){.cEEVdI{display:none;}}
/* sc-component-id: Sidebar___StyledDiv-q5rdfu-1 */
.bJsLke{position:relative;}
/* sc-component-id: Sidebar___StyledDiv2-q5rdfu-2 */
.gKyVMe{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:calc(100vh - 204px);}
/* sc-component-id: Sidebar___StyledDiv3-q5rdfu-3 */
.jJwXiO{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;background-color:#fff;padding:32px 20px;max-width:100vw;} @media (max-width:991.98px){.jJwXiO{padding:32px 12px;}}
/* sc-component-id: Sidebar___StyledDiv4-q5rdfu-4 */
.iARHbT{margin-right:auto;margin-left:auto;max-width:992px;}
/* sc-component-id: sc-kjoXOD */
.ZLMZX{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;position:relative;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-transition:box-shadow 0.25s ease-in-out,color 0.1s ease-in-out;transition:box-shadow 0.25s ease-in-out,color 0.1s ease-in-out;margin:0;vertical-align:middle;box-sizing:border-box;border-radius:50%;width:40px !important;height:40px !important;box-shadow:0 0 0 2px transparent;background-color:transparent;color:transparent;} .ZLMZX::before{box-shadow:inset 0 0 0 2px;} .ZLMZX > svg{font-size:16px;} .ZLMZX .sc-gisBJw{line-height:40px;font-size:18px;} .ZLMZX > svg,.ZLMZX .sc-gisBJw{color:#fff;} .ZLMZX::after{background-color:transparent;-webkit-text-fill-color:#fff;} .ZLMZX _:-ms-input-placeholder,.ZLMZX::after{color:#fff;} .ZLMZX::before{position:absolute;top:0;left:0;-webkit-transition:box-shadow 0.25s ease-in-out;transition:box-shadow 0.25s ease-in-out;content:'';} .ZLMZX::before,.ZLMZX.ZLMZX > img{border-radius:inherit;width:100%;height:100%;} .ZLMZX.ZLMZX > img{box-sizing:inherit;vertical-align:bottom;object-fit:cover;} .ZLMZX.ZLMZX > svg{width:1em;height:1em;} .ZLMZX::after{display:inline-block;position:absolute;right:2px;bottom:2px;-webkit-transition:all 0.25s ease-in-out,color 0s;transition:all 0.25s ease-in-out,color 0s;opacity:0;border:2px solid;border-radius:100px;padding:0 0;min-width:0;max-width:2em;height:0;box-sizing:content-box !important;overflow:hidden;text-align:center;text-overflow:ellipsis;line-height:1px;white-space:nowrap;font-size:0;font-weight:600;content:'';}
/* sc-component-id: SectionCallout__StyledSectionHeader-sc-6sz3ai-0 */
.jeeFUs{text-transform:uppercase;line-height:16px;-webkit-letter-spacing:0.5px;-moz-letter-spacing:0.5px;-ms-letter-spacing:0.5px;letter-spacing:0.5px;color:#68737d;font-size:10px;font-weight:600;}
/* sc-component-id: TOC___StyledStyledAnchor-sc-1hj7fxm-3 */
.dIPvJZ{display:block;margin:4px 0;text-align:left;padding-left:20px;}
/* sc-component-id: TOC___StyledDiv-sc-1hj7fxm-4 */
.hCsyko{position:-webkit-sticky;position:sticky;top:32px;margin-left:60px;padding-right:12px;max-height:calc(100vh - 64px);overflow-y:auto;}
/* sc-component-id: TOC___StyledStyledSectionHeader-sc-1hj7fxm-5 */
.dcLymE{text-transform:uppercase;line-height:16px;-webkit-letter-spacing:0.5px;-moz-letter-spacing:0.5px;-ms-letter-spacing:0.5px;letter-spacing:0.5px;color:#68737d;font-size:10px;font-weight:600;margin-bottom:8px;margin-left:20px;}
/* sc-component-id: TOC___StyledUl2-sc-1hj7fxm-6 */
.kdJXOp{border-left:1px solid #e9ebed;}
/* sc-component-id: Titled___StyledTOCBlock-sc-16c5y7v-0 */
@media (min-width:1200px){.EQyIi{display:none;}}
/* sc-component-id: Titled___StyledCol-sc-16c5y7v-1 */
@media (max-width:1199.98px){.jCZXmA{display:none;}}
/* sc-component-id: Lists___StyledUnorderedList-sc-1kwxf12-0 */
.bUpFSy{margin:12px 0 12px 32px;}</style><style data-styled="jtXLs" data-styled-version="4.4.1">
/* sc-component-id: sc-keyframes-jtXLs */
@-webkit-keyframes jtXLs{0%{-webkit-transform:translate(-50%,-50%);-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%);}} @keyframes jtXLs{0%{-webkit-transform:translate(-50%,-50%);-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%);}}</style><link as="script" rel="preload" href="/webpack-runtime-cd81a07c9f762d69379e.js"/><link as="script" rel="preload" href="/framework-ee11d0eace73c41c3c83.js"/><link as="script" rel="preload" href="/app-af1c97ad83df906cf459.js"/><link as="script" rel="preload" href="/styles-e9d24b1846c7d6eb9685.js"/><link as="script" rel="preload" href="/commons-bba31756c6fcb7269917.js"/><link as="script" rel="preload" href="/c8009b1aa4aa7cab362953d62580c63ee788718e-4d06a6002dd8c86609bc.js"/><link as="script" rel="preload" href="/a07bd5dedc62d21c7939c15c99d93503b9d8933a-7371829ce647bee71163.js"/><link as="script" rel="preload" href="/be82f8d225c2fc115d9c6269d125ee0a43595c1d-93b47f283fdb325cd158.js"/><link as="script" rel="preload" href="/8670d74a43773b53ee6e572785bb3432945d03b3-b12debeb494dc41a2dda.js"/><link as="script" rel="preload" href="/1589c1a7b9012a6dafead636e4c0053ac9156c10-54f374974951db144fc7.js"/><link as="script" rel="preload" href="/component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx-5a3edaf15dba219bfdec.js"/><link as="fetch" rel="preload" href="/page-data/blog/stt/a-journey-to-10-word-error-rate/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/1942088059.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3709355695.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div><div class="gatsby-theme___StyledDiv-x2kfdx-0 boRYJl"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="Root___StyledDiv-k2xmz5-0 frPItv"><a href="#main-content" class="sc-htoDjs gbWTPN Root___StyledSkipNav-k2xmz5-1 hJGfqI" data-garden-id="chrome.skipnav" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" focusable="false" role="presentation" theme="[object Object]" data-garden-id="chrome.skipnav_icon" data-garden-version="8.31.0" class="sc-dnqmqq esrYoz"><path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg>Skip to main content</a><header role="banner" class="Header__StyledHeader-sc-1bc1xqp-2 iTpVGQ"><div class="MaxWidth__MaxWidthLayout-sc-1rbvyso-0 Header___StyledMaxWidthLayout-sc-1bc1xqp-10 iqDliS"><div class="Header___StyledDiv-sc-1bc1xqp-3 Fwlym"><a aria-label="Coqui" href="/"><div class="Header___StyledDiv2-sc-1bc1xqp-4 eGBoZs"><div class="Header___StyledImg-sc-1bc1xqp-5 kpuXOb gatsby-image-wrapper" style="position:relative;overflow:hidden;display:inline-block;width:95px;height:26px"><noscript><picture><source srcset="/static/38a06ec53309f617be3eb3b8b9367abf/598c3/logo-wordmark.png 1x,
/static/38a06ec53309f617be3eb3b8b9367abf/e3b64/logo-wordmark.png 1.5x,
/static/38a06ec53309f617be3eb3b8b9367abf/5aa2f/logo-wordmark.png 2x" /><img loading="lazy" width="95" height="26" srcset="/static/38a06ec53309f617be3eb3b8b9367abf/598c3/logo-wordmark.png 1x,
/static/38a06ec53309f617be3eb3b8b9367abf/e3b64/logo-wordmark.png 1.5x,
/static/38a06ec53309f617be3eb3b8b9367abf/5aa2f/logo-wordmark.png 2x" src="/static/38a06ec53309f617be3eb3b8b9367abf/598c3/logo-wordmark.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div></div></a></div><div class="Header___StyledDiv3-sc-1bc1xqp-6 ifalUs"><button aria-label="Global navigation" aria-expanded="false" data-garden-id="buttons.icon_button" data-garden-version="8.31.0" type="button" class="sc-eHgmQL sc-brqgnP jIHHbm"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" focusable="false" role="presentation" data-garden-id="buttons.icon" data-garden-version="8.31.0" class="sc-hSdWYo ZurFO"><path fill="currentColor" d="M9.5 2.5a1.5 1.5 0 11-3 0 1.5 1.5 0 013 0zm0 5.5a1.5 1.5 0 11-3 0 1.5 1.5 0 013 0zm0 5.5a1.5 1.5 0 11-3 0 1.5 1.5 0 013 0z"></path></svg></button></div><nav role="navigation" aria-label="Global" class="Header___StyledNav-sc-1bc1xqp-9 dmeNDE"><div class="Header__StyledDesktopNavItem-sc-1bc1xqp-0 dEzVDV"><a class="StyledNavigationLink-sc-1a9tae1-0 Header__StyledDesktopNavLink-sc-1bc1xqp-1 fqXuiB" href="/about">About</a></div><div class="Header__StyledDesktopNavItem-sc-1bc1xqp-0 dEzVDV"><a class="StyledNavigationLink-sc-1a9tae1-0 Header__StyledDesktopNavLink-sc-1bc1xqp-1 fqXuiB is-current" href="/blog">Blog</a></div><div class="Header__StyledDesktopNavItem-sc-1bc1xqp-0 dEzVDV"><a class="StyledNavigationLink-sc-1a9tae1-0 Header__StyledDesktopNavLink-sc-1bc1xqp-1 fqXuiB" href="/code">Code</a></div><div class="Header__StyledDesktopNavItem-sc-1bc1xqp-0 dEzVDV"><a class="StyledNavigationLink-sc-1a9tae1-0 Header__StyledDesktopNavLink-sc-1bc1xqp-1 fqXuiB" href="/models">Models</a></div></nav></div></header><main class="Root___StyledMain-k2xmz5-2 hJmzvI"><div class="Sidebar___StyledDiv-q5rdfu-1 bJsLke"><div class="MaxWidth__MaxWidthLayout-sc-1rbvyso-0 tqxgC"><div class="Sidebar___StyledDiv2-q5rdfu-2 gKyVMe"><nav aria-label="Primary" class="DesktopSidebar___StyledNav-sc-1ezn4du-0 fOLerP"><div class="DesktopSidebar___StyledDiv-sc-1ezn4du-1 cEEVdI"></div></nav><div class="Sidebar___StyledDiv3-q5rdfu-3 jJwXiO"><div id="main-content" class="Sidebar___StyledDiv4-q5rdfu-4 iARHbT"><div data-garden-id="grid.grid" data-garden-version="8.31.0" class="sc-jAaTju cxJGyA"><div data-garden-id="grid.row" data-garden-version="8.31.0" class="sc-jDwBTQ hKRDwX"><div data-garden-id="grid.col" data-garden-version="8.31.0" class="sc-cMljjf jnzUYZ"><h2 class="sc-iRbamj bSMrqM Typography__StyledH2-xo896r-1 cdcKhH" data-garden-id="typography.font" data-garden-version="8.31.0">A Journey to &lt;10% Word Error Rate</h2><div><figure aria-atomic="true" aria-live="polite" data-garden-id="avatars.avatar" data-garden-version="8.31.0" class="sc-kjoXOD ZLMZX"><img alt="avatar" src="https://secure.gravatar.com/avatar/a0806241b0bfd0b4339c8d987d98b6db?s=128&amp;d=mm&amp;r=g"/></figure><div data-garden-id="typography.font" data-garden-version="8.31.0" class="sc-iRbamj cgPVmz">Reuben Morais</div><div data-garden-id="typography.font" data-garden-version="8.31.0" class="sc-iRbamj gBLfIq">November 29, 2017</div></div><hr class="Typography__StyledHr-xo896r-7 fWmrzw"/><div class="Titled___StyledTOCBlock-sc-16c5y7v-0 EQyIi"><div class="SectionCallout__StyledSectionHeader-sc-6sz3ai-0 jeeFUs">Table of Contents</div><ul><li><a href="#the-architecture" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ">The Architecture</a></li><li><a href="#the-data" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ">The Data</a></li><li><a href="#the-hardware" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ">The Hardware</a></li><li><a href="#putting-it-all-together" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ">Putting it All Together</a></li><li><a href="#beam-scoring-with-a-language-model" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ">Beam Scoring with a Language Model</a></li><li><a href="#license" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ">License</a></li></ul><hr class="Typography__StyledHr-xo896r-7 fWmrzw"/></div><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">We believe speech interfaces will be a big part of how people interact with their devices in the future. Today we are
excited to announce the initial release of our <a target="_blank" rel="noopener noreferrer" href="https://github.com/coqui-ai/STT/releases/tag/v0.1.0" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">open source speech recognition
model<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> so that anyone can develop compelling speech experiences.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">The Machine Learning team has been working on an open source Automatic Speech Recognition engine modeled after the Deep
Speech papers (<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.5567" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">1<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>, <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1512.02595" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">2<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>) published by Baidu. One of
the major goals from the beginning was to achieve a Word Error Rate in the transcriptions of under 10%. We have made
great progress: Our word error rate on LibriSpeech’s test-clean set is 6.5%, which not only achieves our initial goal,
but gets us close to human level performance.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">This post is an overview of the team’s efforts and ends with a more detailed explanation of the final piece of the
puzzle: the CTC decoder.</p><h3 id="the-architecture" style="position:relative" class="sc-iRbamj dYOvul Typography__StyledH3-xo896r-2 hmdGyC" data-garden-id="typography.font" data-garden-version="8.31.0"><a href="#the-architecture" aria-label="the architecture permalink" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh anchor before" data-garden-id="buttons.anchor" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" focusable="false" viewBox="0 0 16 16">
  <path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg></a>The Architecture</h3><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Coqui STT is an end-to-end trainable, character-level, deep recurrent neural network
(<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">RNN<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>). In less buzzwordy terms: it’s a deep neural network
with recurrent layers that gets audio features as input and outputs characters directly — the transcription of the
audio. It can be trained using supervised learning from scratch, without any external “sources of intelligence”, like a
grapheme to phoneme converter or forced alignment on the input.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0"><img src="/static/blog-stt-a-journey-to-10-word-error-rate-architecture-1dd53cecf9b0f6023f3d4ed8eac5fa25.gif" alt="IMAGE"/></p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">This animation shows how the data flows through the network. In practice, instead of processing slices of the audio
input individually, we do all slices at once.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">The network has five layers: the input is fed into three fully connected layers, followed by a bidirectional RNN layer,
and finally a fully connected layer. The hidden fully connected layers use the
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">ReLU<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> activation. The RNN layer uses LSTM cells with
tanh activation.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">The output of the network is a matrix of character probabilities over time. In other words, for each time step the
network outputs one probability for each character in the alphabet, which represents the likelihood of that character
corresponding to what’s being said in the audio at that time. The <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/~graves/icml_2006.pdf" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">CTC loss
function<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> (PDF link) considers all alignments of the audio to the
transcription at the same time, allowing us to maximize the probability of the correct transcription being predicted
without worrying about alignment. Finally, we train using the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.6980" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Adam optimizer<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>.</p><h3 id="the-data" style="position:relative" class="sc-iRbamj dYOvul Typography__StyledH3-xo896r-2 hmdGyC" data-garden-id="typography.font" data-garden-version="8.31.0"><a href="#the-data" aria-label="the data permalink" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh anchor before" data-garden-id="buttons.anchor" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" focusable="false" viewBox="0 0 16 16">
  <path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg></a>The Data</h3><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Supervised learning requires data, lots and lots of it. Training a model like Coqui STT requires thousands of hours of
labeled audio, and obtaining and preparing this data can be as much work, if not more, as implementing the network and
the training logic.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">We started by downloading freely available speech corpora like
<a target="_blank" rel="noopener noreferrer" href="http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">TED-LIUM<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> and <a target="_blank" rel="noopener noreferrer" href="http://www.openslr.org/12/" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">LibriSpeech<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>, as
well as acquiring paid corpora like <a target="_blank" rel="noopener noreferrer" href="https://catalog.ldc.upenn.edu/LDC2004S13" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Fisher<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> and
<a target="_blank" rel="noopener noreferrer" href="https://catalog.ldc.upenn.edu/ldc97s62" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Switchboard<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>. We wrote importers in Python for the different data sets that
convert the audio files to WAV, split the audio and cleaned up the transcription of unneeded characters like punctuation
and accents. Finally we stored the preprocessed data in CSV files that can be used to feed data into the network.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Using existing speech corpora allowed us to quickly start working on the model. But in order to achieve excellent
results, we needed a lot more data. We had to be creative. We thought that maybe this type of speech data would already
exist out there, sitting in people’s archives, so we reached out to public TV and radio stations, language study
departments in universities, and basically anyone who might have labeled speech data to share. Through this effort, we
were able to more than double the amount of training data we had to work with, which is now enough for training a
high-quality English model.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Having a high-quality voice corpus publicly available not only helps advance our own speech recognition engine. It will
eventually allow for broad innovation because developers, startups and researchers around can train and experiment with
different architectures and models for different languages. It could help democratize access to deep learning for those
who can’t afford to pay for thousands of hours of training data (almost everyone).</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">To build a speech corpus that’s free, open source, and big enough to create meaningful products with, we worked with
Mozilla’s Open Innovation team and launched the <a target="_blank" rel="noopener noreferrer" href="https://commonvoice.mozilla.org/en" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Common Voice project<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> to collect
and validate speech contributions from volunteers all over the world. Today, the team is releasing a large collection of
voice data into the <a target="_blank" rel="noopener noreferrer" href="https://creativecommons.org/choose/zero/" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">public domain<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>. Find out more about the release on <a target="_blank" rel="noopener noreferrer" href="https://medium.com/mozilla-open-innovation/sharing-our-common-voice-mozilla-releases-second-largest-public-voice-data-set-e88f7d6b7666" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">the
Open Innovation Medium
blog<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>.</p><h3 id="the-hardware" style="position:relative" class="sc-iRbamj dYOvul Typography__StyledH3-xo896r-2 hmdGyC" data-garden-id="typography.font" data-garden-version="8.31.0"><a href="#the-hardware" aria-label="the hardware permalink" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh anchor before" data-garden-id="buttons.anchor" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" focusable="false" viewBox="0 0 16 16">
  <path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg></a>The Hardware</h3><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Coqui STT has over 120 million parameters, and training a model this large is a very computationally expensive task: you
need lots of GPUs if you don’t want to wait forever for results. We looked into training on the cloud, but it doesn’t
work financially: dedicated hardware pays for itself quite quickly if you do a lot of training. The cloud is a good way
to do fast hyperparameter explorations though, so keep that in mind.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">We started with a single machine running four Titan X Pascal GPUs, and then bought another two servers with 8 Titan XPs
each. We run the two 8 GPU machines as a cluster, and the older 4 GPU machine is left independent to run smaller
experiments and test code changes that require more compute power than our development machines have. This setup is
fairly efficient, and for our larger training runs we can go from zero to a good model in about a week.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Setting up distributed training with TensorFlow was an arduous process. Although it has the most mature distributed
training tools of the available deep learning frameworks, getting things to actually work without bugs and to take full
advantage of the extra compute power is tricky. Our current setup works thanks to the incredible efforts of my colleague
<a target="_blank" rel="noopener noreferrer" href="https://github.com/tilmankamp" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Tilman Kamp<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>, who endured long battles with TensorFlow,
<a target="_blank" rel="noopener noreferrer" href="https://slurm.schedmd.com/" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Slurm<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>, and even the Linux kernel until we had everything working.</p><h3 id="putting-it-all-together" style="position:relative" class="sc-iRbamj dYOvul Typography__StyledH3-xo896r-2 hmdGyC" data-garden-id="typography.font" data-garden-version="8.31.0"><a href="#putting-it-all-together" aria-label="putting it all together permalink" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh anchor before" data-garden-id="buttons.anchor" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" focusable="false" viewBox="0 0 16 16">
  <path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg></a>Putting it All Together</h3><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">At this point, we have two papers to guide us, a model implemented based on those papers, the resulting data, and the
hardware required for the training process. It turns out that replicating the results of a paper isn’t that
straightforward. The vast majority of papers don’t specify all the hyperparameters they use, if they specify any at all.
This means you have to spend a whole lot of time and energy doing hyperparameter searches to find a good set of values.
Our initial tests with values chosen through a mix of randomness and intuition weren’t even close to the ones reported
by the paper, probably due to small differences in the architecture — for one, we used LSTM (<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Long short-term
memory<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>) cells instead of GRU (gated recurrent unit) cells. We
spent a lot of time doing a binary search on dropout ratios, we reduced the learning rate, changed the way the weights
were initialized, and experimented with the size of the hidden layers as well. All of those changes got us pretty close
to our desired target of &lt;10% Word Error Rate, but not there.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">One piece missing from our code was an important optimization: integrating our language model into the decoder. The CTC
(Connectionist Temporal Classification) decoder works by taking the probability matrix that is output by the model and
walking over it looking for the most likely text sequence according to the probability matrix. If at time step 0 the
letter “C” is the most likely, and at time step 1 the letter “A” is the most likely, and at time step 2 the letter “T”
is the most likely, then the transcription given by the simplest possible decoder will be “CAT”. This strategy is called
greedy decoding.</p><div align="center"><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0"><span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:500px">
      <span class="gatsby-resp-image-background-image" style="padding-bottom:57.599999999999994%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABuElEQVQoz2VSPa8BURC93m5EZKOjpRChkvADxH+gIERCJyQK29hCdDQ+KoVoSYhEQuIniEJoRINOfDQ0Nr7fO8+8tzbvTTGZO3POnLlzL0skEj6fTxTF4/H4fD6v1yv85z+73+8o3W43xOPxOJVKAcZarRbV4Il2Op0mk8lgMECp0Wj0er3VaoX84/GAr9VqOp3OZDKNRiMWCATK5TIQOMzn83q9brfb2a9pNBp4vV7v9/uLxWIkEuF5nkpms5lNp9N0Ou10OtFPzYF9vIz4akMvl8vl9XoZjYqxd7tdpVLhOA4EAnEvIz7/Mq1Wi3w2mwVFlmVGm6A7bzYbo9FIykoLtdFEWBjpMVom9C+XC4JoNEqaAMXjcUmSwuGwxWJBkm4bi8WI+b1t9WPAZzIZEvF4PDhiKPh8Pk9JzLVer5XNv5XhIY7NEa7b7dKzwZdKJavV6na7HQ4HgQn/Q6ZOi8XCYDC02+1gMIhXgSzlk8kkltRsNgVBOBwOihj785NCodBsNgMnl8t1Oh3MgvUUCgVw0Mtms+HbKOA3mUSGw2G1WkVwPp+XyyWC/X6/3W4J0+/30ZTyEP8CwP6ncwMl13MAAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <img class="gatsby-resp-image-image" alt="IMAGE" title="IMAGE" src="/static/951c29bfac70e155e5037020410b8cfe/c6e3d/blog-stt-a-journey-to-10-word-error-rate-cat.png" srcSet="/static/951c29bfac70e155e5037020410b8cfe/43fa5/blog-stt-a-journey-to-10-word-error-rate-cat.png 250w,/static/951c29bfac70e155e5037020410b8cfe/c6e3d/blog-stt-a-journey-to-10-word-error-rate-cat.png 500w" sizes="(max-width: 500px) 100vw, 500px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0" loading="lazy"/>
    </span></p></div><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">This is a pretty good way of decoding the probabilities output by the model into a sequence of characters, but it has
one major flaw: it only takes into account the output of the network, which means it only takes into account the
information from audio. When the same audio has two equally likely transcriptions (think “new” vs “knew”, “pause” vs
“paws”), the model can only guess at which one is correct. This is far from optimal: if the first four words in a
sentence are “the cat has tiny”, we can be pretty sure that the fifth word will be “paws” rather than “pause”. Answering
those types of questions is the job of a language model, and if we could integrate a language model into the decoding
phase of our model, we could get way better results.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">When we first tried to tackle this issue, we ran into a couple of blockers in TensorFlow: first, it doesn’t expose its
beam scoring functionality in the Python API (probably for performance reasons); and second, the log probabilities
output by the CTC loss function were (are?) <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tensorflow/issues/6034" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">invalid<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">We decided to work around the problem by building something like a spell checker instead: go through the transcription
and see if there are any small modifications we can make that increase the likelihood of that transcription being valid
English, according to the language model. This did a pretty good job of correcting small spelling mistakes in the
output, but as we got closer and closer to our target error rate, we realized that it wasn’t going to be enough. We’d
have to bite the bullet and write some C++.</p><h3 id="beam-scoring-with-a-language-model" style="position:relative" class="sc-iRbamj dYOvul Typography__StyledH3-xo896r-2 hmdGyC" data-garden-id="typography.font" data-garden-version="8.31.0"><a href="#beam-scoring-with-a-language-model" aria-label="beam scoring with a language model permalink" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh anchor before" data-garden-id="buttons.anchor" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" focusable="false" viewBox="0 0 16 16">
  <path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg></a>Beam Scoring with a Language Model</h3><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Integrating the language model into the decoder involves querying the language model every time we evaluate an addition
to the transcription. Going back to the previous example, when looking into whether we want to choose “paws” or “pause”
for the next word after “the cat has tiny”, we query the language model and use that score as a weight to sort the
candidate transcriptions. Now we get to use information not just from audio but also from our language model to decide
which transcription is more likely. The algorithm is described in <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1408.2873" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">this paper<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> by
Hannun et. al.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Luckily, TensorFlow does have an extension point on its CTC beam search decoder that allows the user to supply their own
beam scorer. This means all you have to do is write the beam scorer that queries the language model and plug that in.
For our case, we wanted that functionality to be exposed to our Python code, so we also exposed it as a custom
TensorFlow operation that can be loaded using
<a target="_blank" rel="noopener noreferrer" href="https://www.tensorflow.org/api_docs/python/tf/load_op_library" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">tf.load_op_library<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a>.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Getting all of this to work with our setup required quite a bit of effort, from fighting with the Bazel build system for
hours, to making sure all the code was able to handle Unicode input in a consistent way, and debugging the beam scorer
itself. The system requires quite a few pieces to work together:</p><ul class="sc-fBuWsC jDgxFn Lists___StyledUnorderedList-sc-1kwxf12-0 bUpFSy" data-garden-id="typography.unordered_list" data-garden-version="8.31.0"><li data-garden-id="typography.unordered_list_item" data-garden-version="8.31.0" class="sc-eqIVtm gaUkbp"><div data-garden-id="typography.font" data-garden-version="8.31.0" class="sc-iRbamj sc-fAjcbJ ePmrNi">The language model itself (we use <a target="_blank" rel="noopener noreferrer" href="http://kheafield.com/code/kenlm/" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">KenLM<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> for building and querying). A</div></li><li data-garden-id="typography.unordered_list_item" data-garden-version="8.31.0" class="sc-eqIVtm gaUkbp"><div data-garden-id="typography.font" data-garden-version="8.31.0" class="sc-iRbamj sc-fAjcbJ ePmrNi"><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Trie" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">trie<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> of all the words in our vocabulary. An alphabet file that maps integer</div></li><li data-garden-id="typography.unordered_list_item" data-garden-version="8.31.0" class="sc-eqIVtm gaUkbp"><div data-garden-id="typography.font" data-garden-version="8.31.0" class="sc-iRbamj sc-fAjcbJ ePmrNi">labels output by the network into characters.</div></li></ul><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">Although adding this many moving parts does make our code harder to modify and apply to different use cases (like other
languages), it brings great benefits: Our word error rate on LibriSpeech’s test-clean set went from 16% to 6.5%, which
not only achieves our initial goal, but gets us close to human level performance (5.83% according to the Deep Speech 2
paper). On a MacBook Pro, using the GPU, the model can do inference at a real-time factor of around 0.3x, and around
1.4x on the CPU alone. (A real-time factor of 1x means you can transcribe 1 second of audio in 1 second.)</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">It has been an incredible journey to get to this place: the initial release of our model! In the future we want to
release a model that’s fast enough to run on a mobile device or a Raspberry Pi.</p><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0">If this type of work sounds interesting or useful to you, come check out our repository on GitHub and our Discourse
channel. We have a growing community of contributors and we’re excited to help you create and publish a model for your
language.</p><h3 id="license" style="position:relative" class="sc-iRbamj dYOvul Typography__StyledH3-xo896r-2 hmdGyC" data-garden-id="typography.font" data-garden-version="8.31.0"><a href="#license" aria-label="license permalink" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh anchor before" data-garden-id="buttons.anchor" data-garden-version="8.31.0"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" focusable="false" viewBox="0 0 16 16">
  <path fill="currentColor" d="M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"></path></svg></a>License</h3><p class="sc-caSCKo fbGZzl Typography__StyledParagraph-xo896r-8 gGFgqj" data-garden-id="typography.paragraph" data-garden-version="8.31.0"><a target="_blank" rel="noopener noreferrer" href="https://creativecommons.org/licenses/by-sa/3.0/" class="sc-eHgmQL sc-cvbbAY bINUcJ Anchor__StyledAnchor-sc-1q3ov98-0 gjZoLh" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Creative Commons Attribution Share-Alike License v3.0<svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" theme="[object Object]" data-garden-id="buttons.external_icon" data-garden-version="8.31.0" class="sc-jWBwVP kKEUTC"><path fill="none" stroke="currentColor" stroke-linecap="round" d="M10.5 8.5V10c0 .3-.2.5-.5.5H2c-.3 0-.5-.2-.5-.5V2c0-.3.2-.5.5-.5h1.5M6 6l4-4m-3.5-.5H10c.3 0 .5.2.5.5v3.5"></path></svg></a> or any later
version</p></div><div class="sc-cMljjf hUigtZ Titled___StyledCol-sc-16c5y7v-1 jCZXmA" data-garden-id="grid.col" data-garden-version="8.31.0"><div class="TOC___StyledDiv-sc-1hj7fxm-4 hCsyko"><div class="SectionCallout__StyledSectionHeader-sc-6sz3ai-0 TOC___StyledStyledSectionHeader-sc-1hj7fxm-5 dcLymE">Table of Contents</div><ul class="TOC___StyledUl2-sc-1hj7fxm-6 kdJXOp"><li><a href="#the-architecture" class="sc-eHgmQL sc-cvbbAY bINUcJ TOC__StyledAnchor-sc-1hj7fxm-1 TOC___StyledStyledAnchor-sc-1hj7fxm-3 dIPvJZ" data-garden-id="buttons.anchor" data-garden-version="8.31.0">The Architecture</a></li><li><a href="#the-data" class="sc-eHgmQL sc-cvbbAY bINUcJ TOC__StyledAnchor-sc-1hj7fxm-1 TOC___StyledStyledAnchor-sc-1hj7fxm-3 dIPvJZ" data-garden-id="buttons.anchor" data-garden-version="8.31.0">The Data</a></li><li><a href="#the-hardware" class="sc-eHgmQL sc-cvbbAY bINUcJ TOC__StyledAnchor-sc-1hj7fxm-1 TOC___StyledStyledAnchor-sc-1hj7fxm-3 dIPvJZ" data-garden-id="buttons.anchor" data-garden-version="8.31.0">The Hardware</a></li><li><a href="#putting-it-all-together" class="sc-eHgmQL sc-cvbbAY bINUcJ TOC__StyledAnchor-sc-1hj7fxm-1 TOC___StyledStyledAnchor-sc-1hj7fxm-3 dIPvJZ" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Putting it All Together</a></li><li><a href="#beam-scoring-with-a-language-model" class="sc-eHgmQL sc-cvbbAY bINUcJ TOC__StyledAnchor-sc-1hj7fxm-1 TOC___StyledStyledAnchor-sc-1hj7fxm-3 dIPvJZ" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Beam Scoring with a Language Model</a></li><li><a href="#license" class="sc-eHgmQL sc-cvbbAY bINUcJ TOC__StyledAnchor-sc-1hj7fxm-1 TOC___StyledStyledAnchor-sc-1hj7fxm-3 dIPvJZ" data-garden-id="buttons.anchor" data-garden-version="8.31.0">License</a></li></ul></div></div></div></div></div></div></div></div></div></main><footer class="Footer___StyledFooter-owx86q-1 jOzwVI"><div class="MaxWidth__MaxWidthLayout-sc-1rbvyso-0 tqxgC"><div class="Footer___StyledDiv-owx86q-2 dSuWUw"><a href="https://github.com/coqui-ai" aria-label="Coqui" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ"><div class="Footer___StyledDiv2-owx86q-3 kPitly"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" class="Footer___StyledGitHubIcon-owx86q-4 bRvpwy"><path fill="currentColor" d="M6 0C2.687 0 0 2.754 0 6.152c0 2.718 1.719 5.024 4.103 5.837.3.057.41-.133.41-.296 0-.146-.005-.533-.008-1.046-1.669.371-2.021-.825-2.021-.825-.273-.711-.666-.9-.666-.9-.545-.382.04-.374.04-.374.603.044.92.634.92.634.535.94 1.404.668 1.746.511.055-.397.21-.669.381-.822-1.332-.155-2.733-.683-2.733-3.04 0-.672.234-1.221.618-1.651-.062-.156-.268-.781.058-1.629 0 0 .504-.165 1.65.631A5.614 5.614 0 016 2.975a5.58 5.58 0 011.502.207c1.146-.796 1.649-.63 1.649-.63.327.847.121 1.472.06 1.628.384.43.616.979.616 1.65 0 2.364-1.403 2.884-2.74 3.036.216.19.408.565.408 1.14 0 .821-.007 1.485-.007 1.687 0 .164.108.356.412.296 2.382-.816 4.1-3.12 4.1-5.837C12 2.754 9.313 0 6 0"></path></svg></div></a><a href="https://twitter.com/coqui_ai" aria-label="Coqui" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ"><div class="Footer___StyledDiv3-owx86q-5 kNCRIy"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" class="Footer___StyledTwitterIcon-owx86q-6 iRhBUd"><path fill="currentColor" d="M12 2.184a4.83 4.83 0 01-1.415.397 2.52 2.52 0 001.083-1.396 4.87 4.87 0 01-1.564.612A2.428 2.428 0 008.308 1c-1.36 0-2.463 1.13-2.463 2.524 0 .198.023.39.065.576C3.863 3.994 2.05 2.99.835 1.46a2.564 2.564 0 00-.332 1.27 2.54 2.54 0 001.094 2.102 2.413 2.413 0 01-1.115-.316v.032c0 1.224.849 2.243 1.974 2.476-.363.1-.743.115-1.112.042.314 1.002 1.223 1.734 2.3 1.754A4.857 4.857 0 010 9.866 6.83 6.83 0 003.774 11c4.528 0 7.005-3.847 7.005-7.182 0-.11-.003-.22-.007-.327.482-.358.898-.8 1.228-1.308z"></path></svg></div></a><a href="https://www.facebook.com/coquiai" aria-label="Coqui" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ"><div class="Footer___StyledDiv4-owx86q-7 cyVuQe"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" class="Footer___StyledFacebookIcon-owx86q-8 dAyBhk"><path fill="currentColor" d="M6 0a6 6 0 01.813 11.945V7.63h1.552l.244-1.585H6.812v-.867c0-.658.214-1.242.827-1.242h.985V2.55c-.173-.024-.538-.075-1.23-.075-1.444 0-2.29.767-2.29 2.513v1.055H3.618v1.585h1.484v4.304A6.001 6.001 0 016 0z"></path></svg></div></a><a href="https://www.linkedin.com/company/coqui-ai" aria-label="Coqui" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ"><div class="Footer___StyledDiv5-owx86q-9 jUlaGX"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12" focusable="false" role="presentation" class="Footer___StyledLinkedInIcon-owx86q-10 jJClhf"><path fill="currentColor" d="M10.8 0A1.2 1.2 0 0112 1.2v9.6a1.2 1.2 0 01-1.2 1.2H1.2A1.2 1.2 0 010 10.8V1.2A1.2 1.2 0 011.2 0h9.6zM8.09 4.356a1.87 1.87 0 00-1.598.792l-.085.133h-.024v-.783H4.676v5.727h1.778V7.392c0-.747.142-1.47 1.068-1.47.913 0 .925.854.925 1.518v2.785h1.778V7.084l-.005-.325c-.05-1.38-.456-2.403-2.13-2.403zm-4.531.142h-1.78v5.727h1.78V4.498zm-.89-2.846a1.032 1.032 0 100 2.064 1.032 1.032 0 000-2.064z"></path></svg></div></a><a href="https://gitter.im/coqui-ai/community" aria-label="Coqui" data-garden-id="buttons.anchor" data-garden-version="8.31.0" class="sc-eHgmQL sc-cvbbAY bINUcJ"><div class="Footer___StyledDiv6-owx86q-11 gXlMSe"><svg xmlns="http://www.w3.org/2000/svg" height="26" viewBox="0 0 26 26" focusable="false" role="presentation" class="Footer___StyledGitterIcon-owx86q-12 mZaVr"><path d="M5.2 1.04a.52.52 0 00-.52.52V15.6a.52.52 0 00.52.52h2.08a.52.52 0 00.52-.52V1.56a.52.52 0 00-.52-.52H5.2zm4.68 3.64a.52.52 0 00-.52.52v19.24a.52.52 0 00.52.52h2.08a.52.52 0 00.52-.52V5.2a.52.52 0 00-.52-.52H9.88zm4.68 0a.52.52 0 00-.52.52v19.24a.52.52 0 00.52.52h2.08a.52.52 0 00.52-.52V5.2a.52.52 0 00-.52-.52h-2.08zm4.68 0a.52.52 0 00-.52.52v10.4a.52.52 0 00.52.52h2.08a.52.52 0 00.52-.52V5.2a.52.52 0 00-.52-.52h-2.08z" fill="currentColor"></path></svg></div></a></div><div class="Footer___StyledDiv7-owx86q-13 gnMBjg"><div class="Footer___StyledDiv8-owx86q-14 iBbHyF"><a class="Footer__StyledFooterItem-owx86q-0 dJVuoI" href="/imprint">Imprint</a><a class="Footer__StyledFooterItem-owx86q-0 dJVuoI" href="/privacy">Privacy Policy</a></div><div class="Footer___StyledDiv9-owx86q-15 cVciVx"><a href="https://berlinlovesyou.com/" class="sc-eHgmQL sc-cvbbAY bINUcJ Footer__StyledFooterItem-owx86q-0 dJVuoI" data-garden-id="buttons.anchor" data-garden-version="8.31.0">Made with<!-- --> <span role="img" aria-label="heart">❤️</span> <!-- -->in Berlin!</a></div><div class="Footer___StyledDiv10-owx86q-16 dJejfB">© Coqui <!-- -->2021</div></div></div></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/blog/stt/a-journey-to-10-word-error-rate";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-b31bc1c35d739d561a85.js"],"app":["/app-af1c97ad83df906cf459.js"],"component---src-pages-404-tsx":["/component---src-pages-404-tsx-35314a09b9c2064446a2.js"],"component---src-pages-about-tsx":["/component---src-pages-about-tsx-8a1c88582ced44901292.js"],"component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx":["/component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx-5a3edaf15dba219bfdec.js"],"component---src-pages-blog-stt-deepspeech-0-6-speech-to-text-engine-mdx":["/component---src-pages-blog-stt-deepspeech-0-6-speech-to-text-engine-mdx-0718e7e6cdc961233c2f.js"],"component---src-pages-blog-stt-speech-recognition-deepspeech-mdx":["/component---src-pages-blog-stt-speech-recognition-deepspeech-mdx-23ff694c999834f998c7.js"],"component---src-pages-blog-tsx":["/component---src-pages-blog-tsx-39cf2309a1b3d69a5964.js"],"component---src-pages-blog-tts-gradual-training-with-tacotron-for-faster-convergence-mdx":["/component---src-pages-blog-tts-gradual-training-with-tacotron-for-faster-convergence-mdx-6205f79ebc74b2782096.js"],"component---src-pages-blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-mdx":["/component---src-pages-blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-mdx-589c130c7800f483143b.js"],"component---src-pages-blog-tts-two-methods-for-better-attention-in-tacotron-mdx":["/component---src-pages-blog-tts-two-methods-for-better-attention-in-tacotron-mdx-e4cfb17adb8e4a869479.js"],"component---src-pages-code-tsx":["/component---src-pages-code-tsx-967b62bfa4ccc098827f.js"],"component---src-pages-data-tsx":["/component---src-pages-data-tsx-7c9644040fcc906cd195.js"],"component---src-pages-imprint-tsx":["/component---src-pages-imprint-tsx-cd5d66da2a8d9330a709.js"],"component---src-pages-index-tsx":["/component---src-pages-index-tsx-f1857c50344e7926a966.js"],"component---src-pages-job-head-of-product-mdx":["/component---src-pages-job-head-of-product-mdx-6979fed548e2f722d4e7.js"],"component---src-pages-job-senior-developer-community-manager-mdx":["/component---src-pages-job-senior-developer-community-manager-mdx-5a6fc72a05b9935b2bca.js"],"component---src-pages-job-senior-full-stack-developer-mdx":["/component---src-pages-job-senior-full-stack-developer-mdx-9ade62ea91099d9e6254.js"],"component---src-pages-job-senior-mlops-deployment-engineer-mdx":["/component---src-pages-job-senior-mlops-deployment-engineer-mdx-628ee1182e7930787215.js"],"component---src-pages-job-senior-mlops-provisioning-engineer-mdx":["/component---src-pages-job-senior-mlops-provisioning-engineer-mdx-ed08150d279a0f2bdd75.js"],"component---src-pages-job-senior-mlops-training-pipeline-engineer-mdx":["/component---src-pages-job-senior-mlops-training-pipeline-engineer-mdx-97219eef1217a921792a.js"],"component---src-pages-job-senior-stt-deep-learning-developer-mdx":["/component---src-pages-job-senior-stt-deep-learning-developer-mdx-c73288a78885787c964b.js"],"component---src-pages-job-senior-tts-deep-learning-developer-mdx":["/component---src-pages-job-senior-tts-deep-learning-developer-mdx-65d5386f7d4901161af2.js"],"component---src-pages-models-tsx":["/component---src-pages-models-tsx-872ea8f1299f9afeca93.js"],"component---src-pages-newsletter-03-05-2021-mdx":["/component---src-pages-newsletter-03-05-2021-mdx-2f006c189656e71d21a6.js"],"component---src-pages-newsletter-05-04-2021-mdx":["/component---src-pages-newsletter-05-04-2021-mdx-f72b8e7d5d603fd3c51f.js"],"component---src-pages-newsletter-05-07-2021-mdx":["/component---src-pages-newsletter-05-07-2021-mdx-7334983ed527aa663b5e.js"],"component---src-pages-newsletter-06-06-2021-mdx":["/component---src-pages-newsletter-06-06-2021-mdx-ce9277a1147ff327537a.js"],"component---src-pages-privacy-tsx":["/component---src-pages-privacy-tsx-882ab751a9f016e2b4cf.js"],"component---src-templates-model-card-template-tsx":["/component---src-templates-model-card-template-tsx-dc097587faca4de7fb21.js"]};/*]]>*/</script><script src="/polyfill-b31bc1c35d739d561a85.js" nomodule=""></script><script src="/component---src-pages-blog-stt-a-journey-to-10-word-error-rate-mdx-5a3edaf15dba219bfdec.js" async=""></script><script src="/1589c1a7b9012a6dafead636e4c0053ac9156c10-54f374974951db144fc7.js" async=""></script><script src="/8670d74a43773b53ee6e572785bb3432945d03b3-b12debeb494dc41a2dda.js" async=""></script><script src="/be82f8d225c2fc115d9c6269d125ee0a43595c1d-93b47f283fdb325cd158.js" async=""></script><script src="/a07bd5dedc62d21c7939c15c99d93503b9d8933a-7371829ce647bee71163.js" async=""></script><script src="/c8009b1aa4aa7cab362953d62580c63ee788718e-4d06a6002dd8c86609bc.js" async=""></script><script src="/commons-bba31756c6fcb7269917.js" async=""></script><script src="/styles-e9d24b1846c7d6eb9685.js" async=""></script><script src="/app-af1c97ad83df906cf459.js" async=""></script><script src="/framework-ee11d0eace73c41c3c83.js" async=""></script><script src="/webpack-runtime-cd81a07c9f762d69379e.js" async=""></script></body></html>