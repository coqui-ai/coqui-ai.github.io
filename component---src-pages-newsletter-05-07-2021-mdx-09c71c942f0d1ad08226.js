(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{"4bMF":function(e,a,t){"use strict";t.r(a),t.d(a,"_frontmatter",(function(){return r})),t.d(a,"default",(function(){return c}));var n=t("zLVn"),A=(t("q1tI"),t("7ljp")),i=t("yDk1"),o=["components"],r={},l={pageQuery:"3484569904",_frontmatter:r},s=i.a;function c(e){var a=e.components,t=Object(n.a)(e,o);return Object(A.b)(s,Object.assign({},l,t,{components:a,mdxType:"MDXLayout"}),Object(A.b)("p",null,Object(A.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"934px"}},"\n      ",Object(A.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"27.599999999999998%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA3XAAAN1wFCKJt4AAABCElEQVQY042RQUsCcRDF/VB9g+59gk6eOhTRpaBbh5CgYCuiTkYGah6CSIwiyiRbiiyVFg+5EoS6LO1/XS0oXffXriC1atCDOcyDeW/mTQAXjuMwhFHcH/g9H/CaPqG/W5T0V1ShUTbqjDIb7L/abR/vE8y8KEjyMfv5NHuFS87kO0LhCBvxBOuxBPHTc0K7EVJZGV2YhI+SZAvF3qzd7f4I9h287WpNgfho8WzWWdjaZnFzh+DSMtMra0zMzTO7KjE2GewJj0/NcOKKe+jY9vCGWsvkvlom55ZqamhvBqnrG26fFDIPeYTVRIoecHiR5ir3iKJWMBqW/+TBbDynz46by/9/4sv1G2shujMHTg19AAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(A.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png",srcSet:["/static/c9103bc33c8add5ab1fa4fa1c49c90ef/43fa5/logo-wordmark.png 250w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/c6e3d/logo-wordmark.png 500w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png 934w"],sizes:"(max-width: 934px) 100vw, 934px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(A.b)("h3",{id:"welcome",style:{position:"relative"}},Object(A.b)("a",{parentName:"h3",href:"#welcome","aria-label":"welcome permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Welcome"),Object(A.b)("p",null,"By ",Object(A.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(A.b)("p",null,"üê∏TTS was on üî• this month bringing lots of improvements and updates your way. First, we‚Äôve\ncompletely re-written the ",Object(A.b)("a",{parentName:"p",href:"https://tts.readthedocs.io"},"TTS documentation"),". Now it‚Äôs easier\nthan ever to start your TTS journey with the trusted guide of our new üê∏TTS documentation\nby your side. Next, we introduced a new ",Object(A.b)("a",{parentName:"p",href:"https://tts.readthedocs.io/en/latest/main_classes/trainer_api.html"},"Trainer API"),"\nwhich is a lightweight, extensible, and feature-complete training framework for ",Object(A.b)("em",{parentName:"p"},"all")," the\nüê∏TTS models. If that wasn‚Äôt enough, we introduced a new char-to-phoneme model\n",Object(A.b)("a",{parentName:"p",href:"https://github.com/rhasspy/gruut"},"Gruut")," that makes üê∏TTS even more realistic."),Object(A.b)("p",null,"üê∏STT wasn‚Äôt sleeping either. Over the last month work began on a complete rewrite of\nüê∏STT‚Äôs ‚Äúnative client‚Äù, a native library written in C++ focused solely on inference.\nIn this rewrite we wanted to retain the performance, simplicity, ease-of-use, and\navailability of the current ‚Äúnative client‚Äù, but set a path towards a new, unified\n",Object(A.b)("a",{parentName:"p",href:"https://onnx.ai"},"ONNX")," based ‚Äúnative client‚Äù that works for both üê∏STT ",Object(A.b)("em",{parentName:"p"},"and")," üê∏TTS.\nThough it‚Äôs early days for the new ‚Äúnative client‚Äù, we‚Äôve released an\n",Object(A.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/inference-engine"},"initial version")," so you can kick the\ntires, drive it around the block, and give us some feedback."),Object(A.b)("p",null,"Last but not least, Coqui gave a talk at\n",Object(A.b)("a",{parentName:"p",href:"https://l3-ai.dev/"},"L3-AI: the conference for building next-level AI assistants"),".\nThe slides are available ",Object(A.b)("a",{parentName:"p",href:"https://docs.google.com/presentation/d/e/2PACX-1vQXtFe__a6P-r3lanv2CpZ0NzQzHDu_1E8uUhTaidnT-WtuPHPkKpiZsgc0gY4PmAZQ5d5CMw9fXAf9/pub?start=false&loop=false&delayms=3000"},"here"),";\nthe video + audio will be available soon."),Object(A.b)("p",null,"Enjoy the newsletter!"),Object(A.b)("h3",{id:"tts-v010-is-out",style:{position:"relative"}},Object(A.b)("a",{parentName:"h3",href:"#tts-v010-is-out","aria-label":"tts v010 is out permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üê∏TTS v0.1.0 is out"),Object(A.b)("p",null,Object(A.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(A.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"72%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQGAQP/xAAWAQEBAQAAAAAAAAAAAAAAAAAGBAX/2gAMAwEAAhADEAAAAX1ucmRTVxhLqf/EABkQAAMBAQEAAAAAAAAAAAAAAAMEBQIBIv/aAAgBAQABBQLPOMGVXC1o/glXpVk5Vgm7z9Pax//EABwRAAICAgMAAAAAAAAAAAAAAAECACESMSJR8P/aAAgBAwEBPwFODqrDHdDXqhJ7n//EAB4RAQABAwUBAAAAAAAAAAAAAAECAAMREiFBYbHh/9oACAECAQE/AWI2pysuQxvz37RqT61//8QAIhAAAgEEAgEFAAAAAAAAAAAAAQIDABESIQQxIiNBUWJx/9oACAEBAAY/ArxZABdtPpVH52aCxWSaLd3Hi9/kURLxuRl9DkKl5gl3Enpi3RrBjksyBDlWAQde1f/EAB4QAQEAAgICAwAAAAAAAAAAAAERACExUWFxkaHB/9oACAEBAAE/IdgmzaraPZ9GQJACHkHZe8kR+FPwn7vESWgGoyvq4dQxwLZb85JjBy1n/9oADAMBAAIAAwAAABC4/wD/xAAaEQEAAgMBAAAAAAAAAAAAAAABETEAUWGB/9oACAEDAQE/EGhqpBZIXvG3wTDn/8QAGxEBAAICAwAAAAAAAAAAAAAAAREhAIExcfD/2gAIAQIBAT8Qt8xKbAwvkjpW1ETxtz//xAAdEAEBAAMBAAMBAAAAAAAAAAABEQAhMUFhcYGR/9oACAEBAAE/EOPe1WRKjODEa3JPcYFeVAEOhBKMQQrykpBca06APcRkiFCLNGH5b9YR6aKQA4EP8+cWkFIu2xdl3L+5/9k=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(A.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/tts-v0.1.0-release.jpg",srcSet:["/static/1e77dc6fce06f5a577ab0e4d0674e779/0988f/tts-v0.1.0-release.jpg 250w","/static/1e77dc6fce06f5a577ab0e4d0674e779/d1f95/tts-v0.1.0-release.jpg 500w","/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/tts-v0.1.0-release.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(A.b)("p",null,"By ",Object(A.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(A.b)("p",null,"A ton of updates and improvements are in the new v0.1.0 version of üê∏TTS. Below we cover some of\nthe important ones, and you can find more info in our ",Object(A.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS/releases/tag/v0.1.0"},"release notes"),"."),Object(A.b)("h4",{id:"-tts-documentation",style:{position:"relative"}},Object(A.b)("a",{parentName:"h4",href:"#-tts-documentation","aria-label":" tts documentation permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üìù TTS documentation"),Object(A.b)("p",null,"We‚Äôve created shinny, new ",Object(A.b)("a",{parentName:"p",href:"https://tts.readthedocs.io"},"TTS documentation")," where you can find all\nthe information you need to train or test your models, implement new models, load new datasets,\nand much more."),Object(A.b)("p",null,"The new documentation also contains ",Object(A.b)("a",{parentName:"p",href:"https://tts.readthedocs.io/en/latest/tutorial_for_nervous_beginners.html"},"here"),"\na new, beginner-friendly intro to üê∏TTS. Getting started is easier than ever."),Object(A.b)("p",null,"If you see something is missing, let us know! We‚Äôre dying for feedback."),Object(A.b)("h4",{id:"-trainer-api",style:{position:"relative"}},Object(A.b)("a",{parentName:"h4",href:"#-trainer-api","aria-label":" trainer api permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üöÄ Trainer API"),Object(A.b)("p",null,"We‚Äôve also introduced a new ",Object(A.b)("a",{parentName:"p",href:"https://tts.readthedocs.io/en/latest/main_classes/trainer_api.html"},"Trainer API"),".\nIt provides a lightweight, extensible, and feature-complete training framework for all the üê∏TTS\nmodels. It supports mixed precision and multi GPU training right out-of-the-box and requires no\ncode changes in your model implementation to take advantage of these functionalities."),Object(A.b)("p",null,"With this new API, you can either keep your old way of training models on the terminal or use pure\nPython to initialize your model and call the trainer. Using only üêçPython allows you to run an\nexperiment on a Jupyter Notebook or customize as you like."),Object(A.b)("h4",{id:"Ô∏è-gruut-based-char-to-phoneme",style:{position:"relative"}},Object(A.b)("a",{parentName:"h4",href:"#%EF%B8%8F-gruut-based-char-to-phoneme","aria-label":"Ô∏è gruut based char to phoneme permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üó£Ô∏è Gruut based Char-to-Phoneme"),Object(A.b)("p",null,"v0.1.0 also comes with a new char-to-phoneme interface, based on ",Object(A.b)("a",{parentName:"p",href:"https://github.com/rhasspy/gruut"},"Gruut"),",\nthat covers most of the European languages and has a very flexible API."),Object(A.b)("p",null,"Gruut currently supports the following languages, and the list is growing constantly:"),Object(A.b)("ul",null,Object(A.b)("li",{parentName:"ul"},"Czech (cs or cs-cz)"),Object(A.b)("li",{parentName:"ul"},"German (de or de-de)"),Object(A.b)("li",{parentName:"ul"},"English (en or en-us)"),Object(A.b)("li",{parentName:"ul"},"Spanish (es or es-es)"),Object(A.b)("li",{parentName:"ul"},"Farsi/Persian (fa)"),Object(A.b)("li",{parentName:"ul"},"French (fr or fr-fr)"),Object(A.b)("li",{parentName:"ul"},"Italian (it or it-it)"),Object(A.b)("li",{parentName:"ul"},"Dutch (nl)"),Object(A.b)("li",{parentName:"ul"},"Russian (ru or ru-ru)"),Object(A.b)("li",{parentName:"ul"},"Swedish (sv or sv-se)")),Object(A.b)("p",null,"We also support Japanese and Chinese through ",Object(A.b)("inlineCode",{parentName:"p"},"pypinyin")," and ",Object(A.b)("inlineCode",{parentName:"p"},"MeCab"),"."),Object(A.b)("p",null,"If you need to target a language that is not listed above, let us know and we can work together to\nmake it available under üê∏TTS."),Object(A.b)("p",null,"üëè Big thanks to ",Object(A.b)("a",{parentName:"p",href:"https://github.com/synesthesiam"},"@synesthesiam")," for his library and efforts in\nbringing it to üê∏TTS."),Object(A.b)("h3",{id:"-monthly-tts-papers",style:{position:"relative"}},Object(A.b)("a",{parentName:"h3",href:"#-monthly-tts-papers","aria-label":" monthly tts papers permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üî¨ Monthly TTS Papers"),Object(A.b)("p",null,Object(A.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(A.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAUGA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAG8fTjqGJiV/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAECAxEhIv/aAAgBAQABBQKjsYYnqRTLzWzT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAABABETH/2gAIAQEABj8CKULD/8QAHRAAAgIBBQAAAAAAAAAAAAAAAREAITEQUXGhsf/aAAgBAQABPyHIa9jhO4KABXMVUMCFR30P/9oADAMBAAIAAwAAABCMz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB0QAQEAAgMAAwAAAAAAAAAAAAERACExQWFRgfD/2gAIAQEAAT8QE79Ycp9ayZ+Irb85V2jkm+5fmpmEui4RejKd/pkoKGl5c//Z')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(A.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/ee7ae83975f5a8e36a44fa9649c1ad37/dbdff/monthly-papers.jpg",srcSet:["/static/ee7ae83975f5a8e36a44fa9649c1ad37/0988f/monthly-papers.jpg 250w","/static/ee7ae83975f5a8e36a44fa9649c1ad37/d1f95/monthly-papers.jpg 500w","/static/ee7ae83975f5a8e36a44fa9649c1ad37/dbdff/monthly-papers.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(A.b)("p",null,"By ",Object(A.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(A.b)("p",null,"This month we‚Äôve also read some really interesting TTS papers. A few, which we found to be of\nparticular interest, are:"),Object(A.b)("ul",null,Object(A.b)("li",{parentName:"ul"},Object(A.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.15561"},"A Survey on Neural Speech Synthesis")," - Not only because\nthey cited üê∏TTS and our latest paper but they provide a very comprehensive survey of\nmodels. Especially useful for people just starting to work in the TTS field."),Object(A.b)("li",{parentName:"ul"},Object(A.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.07889"},"UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity\nWaveform Generation")," - A new GAN based vocoder. As we‚Äôre\nquick on the draw, it is already implemented in üê∏TTS!"),Object(A.b)("li",{parentName:"ul"},Object(A.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.09660"},"WaveGrad2")," - WaveGrad2 proposes an end-to-end TTS model\nbuilt on diffusion probabilistic models. You guessed it, we also have a WaveGrad vocoder\nimplementation in üê∏TTS.")),Object(A.b)("p",null,"üëÄ See our ",Object(A.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS-papers"},"TTS-papers")," list for even more TTS papers!"),Object(A.b)("h3",{id:"coqui-inference-engine-a-sneak-peek",style:{position:"relative"}},Object(A.b)("a",{parentName:"h3",href:"#coqui-inference-engine-a-sneak-peek","aria-label":"coqui inference engine a sneak peek permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Coqui Inference Engine: A Sneak Peek"),Object(A.b)("p",null,Object(A.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(A.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQDBQf/xAAWAQEBAQAAAAAAAAAAAAAAAAABBAX/2gAMAwEAAhADEAAAAclbmYzpaYfF/8QAGRAAAwEBAQAAAAAAAAAAAAAAAAECBAMR/9oACAEBAAEFAueGWPBKHkXvOx2Nn//EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAgBAwEBPwGsrP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAAQMQH/2gAIAQEABj8CcxQ//8QAGBABAQEBAQAAAAAAAAAAAAAAAQAxESH/2gAIAQEAAT8hkEjxwh+YXri//9oADAMBAAIAAwAAABCA7//EABURAQEAAAAAAAAAAAAAAAAAAABh/9oACAEDAQE/EKqP/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAgEBPxBXbt//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhkTFhgf/aAAgBAQABPxBk41MRrUeEApk6i0L+Q27ZP//Z')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(A.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/e14c1f8fc43a14aef0ffcea2ec716c96/dbdff/inference-engine-sneak-peek.jpg",srcSet:["/static/e14c1f8fc43a14aef0ffcea2ec716c96/0988f/inference-engine-sneak-peek.jpg 250w","/static/e14c1f8fc43a14aef0ffcea2ec716c96/d1f95/inference-engine-sneak-peek.jpg 500w","/static/e14c1f8fc43a14aef0ffcea2ec716c96/dbdff/inference-engine-sneak-peek.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(A.b)("p",null,"By ",Object(A.b)("a",{parentName:"p",href:"https://github.com/reuben"},"Reuben Morais")),Object(A.b)("p",null,"From the very inception of the üê∏STT project, one of our goals was to build a speech-to-text system\nthat is easy to integrate and deploy into any product, regardless of the platform or programming\nlanguage of choice. üê∏STT was naturally divided between two main components: the training\ninfrastructure and a lean deployment library meant to be integrated into speech-enabled products.\nThe training code is built largely in Python using TensorFlow and other ML libraries. The deployment\nlibrary, which in STT is called the ‚Äúnative client‚Äù, is a native library written in C++, focused\nsolely on inference. It exposes a C API which is then bound to various programming languages.\nThis architecture enabled us to make STT available universally, directly from your favorite package\nmanager."),Object(A.b)("p",null,"Since then, we‚Äôve learned a lot about how people use STT, and the technologies used to build machine\nlearning pipelines have also evolved significantly. One disadvantage of the current architecture\nis how fragile the connection between the training infrastructure and the deployment library is:\nwe‚Äôve managed to bend the TensorFlow tooling to our will in order to implement efficient model\nexports, sometimes leveraging obscure functionality which can be undocumented, unstable, and\nsometimes entirely deprecated. This meant that sometimes changing the model architecture would\nunearth obscure problems, which required deep knowledge of both STT and TensorFlow internals to fix."),Object(A.b)("p",null,"Given these learnings, we at Coqui have been thinking about the future direction of üê∏STT and its\ndeployment library. We want to keep the performance, simplicity, ease-of-use, and availability of\nour libraries, while finding solutions for the problems described above. We also want to draw a\npath towards a unified deployment tool that works for both üê∏STT and üê∏TTS, the latter being built\nwith PyTorch, not TensorFlow. This foundation could then be expanded to work with any speech model."),Object(A.b)("p",null,"With all this in mind, we‚Äôve started working on the next generation of our ‚Äúnative client‚Äù, the\nCoqui Inference Engine. Built as an independent project, rather than as an STT submodule, and based\non ",Object(A.b)("a",{parentName:"p",href:"https://onnx.ai"},"ONNX"),", a standard interchange format for machine learning models, the Coqui\nInference Engine will be a unified solution for running speech models efficiently. We‚Äôre starting\nwith üê∏STT and üê∏TTS, and in the future will support a variety of architectures, reducing the gap\nbetween speech model architecture exploration and efficient deployment."),Object(A.b)("p",null,Object(A.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(A.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"75.6%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAFxGAABcRgEUlENBAAACfUlEQVQ4y3VTTXPaMBTk7/faQ3vrTA9tD+k0ZRLSEiBAJkAgBBtj4y8Z+QNbkk2nH39gK8kJoSQ97FhjP+3b93bd4JxB8BKbLMTYv8IyNMAZx5yM0LNbsIklvwvkeY44TpBlGRhjGpxzjcNzo2AFSlFhTZf4Mn+HG68vCzhu3As0rQ9YBHeaMAgDzGZ3sB0HRVE8I3o8N3Q3qVKR7qofyLMcRVbgZ/kHPCvxq/oNtuXI0kwTKdQK+V7p4bnx1KGAEAIRjdCRow79S1xZbXStb7iy2/BjD4IJXavqdpVAVSqUKOVTQ75vPEpVKlUhoQRdp4W2dYa+10ZPEUpil6z1brfbLWicwlxv4IYJfEIRkBhkE+sd7xXuIS/txA4s5yhZCVEIxFEC07Tq3clJ7q0Ar94P8fZkjHenI7w5ucXrj9doD+dPhI/7UWcaU6RpqlWrCajsniaJHLPSCpXTG5qBSoRSYbhJ4PgbkIjWI6v5SRRpFw3DxFQ+b6cz2LYti0OsiYOVR+C4oW4kRAk7NTF2B5hZSwxuHXhhJBuWh6YwnTWlUl2q88YxWd3gbNLEp9YQrc4I7MFlZdKarhBQKtXJ/SVZHZvD/QlRO6guxHEslZeyiVxFXjw03OqRFdSuVdSUy1VVO7zP4XE4q7KSJFKFF8D3A4QhQUxjqY79Y+BxDp8pVFDdVUQ86sL2V1hYBlzflbGIdKSSNMGzZBygcaxOOdxeNvHd/YyO+xUdp4m+f47L1Sm65hk84u7X8uK/fPwhkwrnto/e1Mb1vYuxscZgbqM7XWJiuEiS9EWil0eWLxP5L08WPi76C3RGK7QGBk67Bs77JnoT58FN9t+R/wLtYzfPW0rAaAAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(A.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/5c1039aa2349c2efef02a024489ec724/da8b6/inference-engine-sneak-peek-diagram.png",srcSet:["/static/5c1039aa2349c2efef02a024489ec724/43fa5/inference-engine-sneak-peek-diagram.png 250w","/static/5c1039aa2349c2efef02a024489ec724/c6e3d/inference-engine-sneak-peek-diagram.png 500w","/static/5c1039aa2349c2efef02a024489ec724/da8b6/inference-engine-sneak-peek-diagram.png 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(A.b)("p",null,"In the diagram above you can see an overview of how the components connect and how models flow.\nFirst-party components (meaning usually maintained by Coqui) are colored green, third-party\ncomponents (community maintained) are colored in blue, and cases where both Coqui and the\ncommunity would maintain offerings are colored in both blue and green."),Object(A.b)("p",null,"These are the early days for the Coqui Inference Engine, and we want to invite all interested\ndevelopers to collaborate on its design and implementation. Check out\n",Object(A.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/inference-engine"},"the repository")," for more information and\n",Object(A.b)("a",{parentName:"p",href:"https://gitter.im/coqui-ai/inference-engine"},"join the discussion on Gitter"),"."),Object(A.b)("h3",{id:"coqui-appearance-at-l3-ai",style:{position:"relative"}},Object(A.b)("a",{parentName:"h3",href:"#coqui-appearance-at-l3-ai","aria-label":"coqui appearance at l3 ai permalink",className:"anchor before"},Object(A.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(A.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Coqui Appearance at L3-AI"),Object(A.b)("p",null,Object(A.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(A.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"50%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABx0lEQVQoz31STU8aURSdIB/DvDcfIAqIsqggMwyjMjMgrRqjsbZG1MQNYaebduuGuMO4aRpX3bjpz+l/6O85vfdNMFaJi5M7782559yPp1lOgOVKDKccwSIIu4O88GDI9rtIOD4MEYA1SsshTDuAVq31EcVD1Le/YoVgk2jecF8JeHNMPAjZIcEuams78IPPqjAtl3exkGkgm21Cz20QgYimT+T5VelkNkNiktylOZ+ixkQWMAgZEkxnG0hl1lWcJb0U4LZMZxPSCtR5VrkwE1PtpfuGe4h2+wvC7iV8/xTNtTEK9gCLS9vKdGdwicfHX3h4+Imnp9/49n2CrN76r4tnQW49jM4Qhhf49HGEuH8BSw4g5ZYaOAtGvSGm9z8wmUxxdzfF9c2tynsrKNpqYzxLRir9QUVdtObMLeEyWEwXDE/NnNvWVIJoImc05i7hNZ6FDQ856WLVjFGxI6SMFtKZJrRCMcJ6fYxS8RjSTobO74rfo4LFMTlLy6f/HQX+ZgPbpOXoLg6ORuj1z6EVSyEO/T842fyL6so+ytUuVusDOJUINsGi+Tm1HpxqjCItZ6kcEidCYXGLKqUNU5sL9CJ2965IcIh/F/QPtKqJincAAAAASUVORK5CYII=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(A.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/d76069cc5cdf496292251efa21b118ac/da8b6/l3-ai.png",srcSet:["/static/d76069cc5cdf496292251efa21b118ac/43fa5/l3-ai.png 250w","/static/d76069cc5cdf496292251efa21b118ac/c6e3d/l3-ai.png 500w","/static/d76069cc5cdf496292251efa21b118ac/da8b6/l3-ai.png 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(A.b)("p",null,"By ",Object(A.b)("a",{parentName:"p",href:"https://github.com/JRMeyer"},"Josh Meyer")),Object(A.b)("p",null,"This year Coqui appeared as a featured partner at ",Object(A.b)("a",{parentName:"p",href:"https://l3-ai.dev/"},"L3-AI: the conference for building next-level AI assistants"),".\nJosh Meyer delivered a talk about deploying scalable, neural voice technologies at the enterprise\nlevel. Soon the recordings will be made available to everyone who wasn‚Äôt able to attend the\nconference virtually, so you can watch the talk and lively Q&A session."),Object(A.b)("p",null,"You can find our presentation slides ",Object(A.b)("a",{parentName:"p",href:"https://docs.google.com/presentation/d/e/2PACX-1vQXtFe__a6P-r3lanv2CpZ0NzQzHDu_1E8uUhTaidnT-WtuPHPkKpiZsgc0gY4PmAZQ5d5CMw9fXAf9/pub?start=false&loop=false&delayms=3000"},"here"),",\nand listen to us talk about related themes with Rasa on their podcast\n",Object(A.b)("a",{parentName:"p",href:"https://podcasts.apple.com/us/podcast/open-source-speech-technology/id1533150162?i=1000518100336"},"here"),".\nWe will keep you posted on when the new L3AI recordings become available."),Object(A.b)("p",null,"In addition to our featured presentation, Coqui had the most active company booth at the\nconference! We want to thank all of you who showed up and participated in the lively\ndiscussions!"))}c.isMDXComponent=!0},yDk1:function(e,a,t){"use strict";var n=t("q1tI"),A=t.n(n),i=t("O9mE"),o=t("v+Ly"),r=t("mrST"),l=t("1Yd/"),s=t("ozyN"),c=t("7cfw"),p=t("t4Fg");a.a=function(e){var a=e.children,t=e.data,b=e.pageContext,h=t.mdx;return Object(n.useEffect)((function(){Object(p.a)()})),A.a.createElement(i.a,null,A.a.createElement(l.a,{title:b.frontmatter.title+" / Newsletter",description:b.frontmatter.description||h.excerpt}),A.a.createElement(o.a,null,A.a.createElement(r.a,{title:b.frontmatter.title,subtitle:b.frontmatter.description,name:b.frontmatter.name,picture:b.frontmatter.picture,date:b.frontmatter.date,toc:h.tableOfContents.items},A.a.createElement(s.a,null,a))),A.a.createElement(c.a,null))}}}]);
//# sourceMappingURL=component---src-pages-newsletter-05-07-2021-mdx-09c71c942f0d1ad08226.js.map