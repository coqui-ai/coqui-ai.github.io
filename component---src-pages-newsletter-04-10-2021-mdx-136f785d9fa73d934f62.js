(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{kLgQ:function(e,a,t){"use strict";t.r(a),t.d(a,"_frontmatter",(function(){return r})),t.d(a,"default",(function(){return c}));var A=t("zLVn"),o=(t("q1tI"),t("7ljp")),n=t("yDk1"),s=["components"],r={},i={pageQuery:"3484569904",_frontmatter:r},l=n.a;function c(e){var a=e.components,t=Object(A.a)(e,s);return Object(o.b)(l,Object.assign({},i,t,{components:a,mdxType:"MDXLayout"}),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"934px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"27.599999999999998%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA3XAAAN1wFCKJt4AAABCElEQVQY042RQUsCcRDF/VB9g+59gk6eOhTRpaBbh5CgYCuiTkYGah6CSIwiyiRbiiyVFg+5EoS6LO1/XS0oXffXriC1atCDOcyDeW/mTQAXjuMwhFHcH/g9H/CaPqG/W5T0V1ShUTbqjDIb7L/abR/vE8y8KEjyMfv5NHuFS87kO0LhCBvxBOuxBPHTc0K7EVJZGV2YhI+SZAvF3qzd7f4I9h287WpNgfho8WzWWdjaZnFzh+DSMtMra0zMzTO7KjE2GewJj0/NcOKKe+jY9vCGWsvkvlom55ZqamhvBqnrG26fFDIPeYTVRIoecHiR5ir3iKJWMBqW/+TBbDynz46by/9/4sv1G2shujMHTg19AAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png",srcSet:["/static/c9103bc33c8add5ab1fa4fa1c49c90ef/43fa5/logo-wordmark.png 250w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/c6e3d/logo-wordmark.png 500w","/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png 934w"],sizes:"(max-width: 934px) 100vw, 934px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("h3",{id:"work-at-coqui",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#work-at-coqui","aria-label":"work at coqui permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üë©‚ÄçüíªWork at Coqui"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(o.b)("p",null,"We‚Äôre still hiring!"),Object(o.b)("p",null,"An open source remote-friendly Berlin based startup founded by the creators of Mozilla‚Äôs\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/mozilla/tts"},"text-to-speech")," (TTS) and ",Object(o.b)("a",{parentName:"p",href:"https://github.com/mozilla/deepspeech"},"speech-to-text"),"\n(STT) engines (over ",Object(o.b)("a",{parentName:"p",href:"https://somsubhra.github.io/github-release-stats/?username=mozilla&repository=deepspeech&page=1&per_page=300"},"650K downloads"),"\nand 23K GitHub stars), with the backing of top-flight investors ",Object(o.b)("em",{parentName:"p"},"and")," we‚Äôre hiring!"),Object(o.b)("p",null,"What‚Äôs not to love?"),Object(o.b)("p",null,"We‚Äôre hiring across-the-board for a number of roles; so, there‚Äôs something for everyone:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/head-of-product"},"Head of Product")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-full-stack-engineer"},"Senior Full Stack Engineers")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-stt-deep-learning-engineer"},"Senior STT Deep Learning Engineers")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-tts-deep-learning-engineer"},"Senior TTS Deep Learning Engineers")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"/job/senior-developer-community-manager"},"Senior, Developer Community Managers"))),Object(o.b)("p",null,"The full list of open positions is available on our ",Object(o.b)("a",{parentName:"p",href:"/jobs"},"jobs page"),"."),Object(o.b)("p",null,"We‚Äôd love to hear from you; so, if any roles pique your interest, reach out to\n",Object(o.b)("a",{parentName:"p",href:"mailto:jobs@coqui.ai"},"jobs@coqui.ai"),". üê∏!"),Object(o.b)("h3",{id:"hello-",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#hello-","aria-label":"hello  permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Hello! üëãüê∏"),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/kdavis-coqui"},"Kelly Davis")),Object(o.b)("p",null,"We‚Äôve got quite a month for youüöÄ!"),Object(o.b)("p",null,"The ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/STT/releases/v1.0.0"},"1.0 release")," of Coqui üê∏STT is hereüéä!\nIt‚Äôs been brewing for quite some time, but we feel it‚Äôs finally ready! 1.0 brings with it\na ton of features:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Exclusive use of TensorFlow Lite"),Object(o.b)("li",{parentName:"ul"},"Experimental iOS support"),Object(o.b)("li",{parentName:"ul"},"Built-in support for transfer learning and fine tuning"),Object(o.b)("li",{parentName:"ul"},"Support for using our training and evaluation tools from Jupyter Notebooks,")),Object(o.b)("p",null,"and a lot more!"),Object(o.b)("p",null,"Alongside the Coqui üê∏STT 1.0 release, we‚Äôre also releasing our best ever\n",Object(o.b)("a",{parentName:"p",href:"https://coqui.ai/english/coqui/v1.0.0-huge-vocab"},"English STT model"),"!\nIt was trained on almost 47,000 hours of English and can outperform humansü§Ø! We‚Äôve come a\nlong way!"),Object(o.b)("p",null,"If that wasn‚Äôt enough, we‚Äôre also releasing a new version of our TTS engine\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS/releases/tag/v0.3.0"},"v0.3.0, üê∏TTS"),". The new version\nintroduces a New ",Object(o.b)("a",{parentName:"p",href:"https://tts.readthedocs.io/en/latest/models/forward_tts.html"},"ForwardTTS API"),",\nAn English ",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/2008.03802"},"SpeedySpeech")," model trained on LJSpeech\n(the most compact TTS model we‚Äôve released to the date), and A fine-tuned\n",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/2106.07889"},"UnivNet")," vocoder, and a lot more TTS goodness."),Object(o.b)("p",null,"We‚Äôve also doubled down on STT responsiveness. Latency matters a lot! For the Coqui üê∏STT 1.0\nrelease we spent a ton of time optimizing the inference library that powers all üê∏STT packages,\nand it shows. It‚Äôs lean, streamable, and loads from disk in the blink of an eye."),Object(o.b)("p",null,"Also, as always, over the last month we‚Äôve been spreading the ‚Äúword of Coqui‚Äù. We spoken at the\n",Object(o.b)("a",{parentName:"p",href:"https://voicelunch.com/"},"Voice Lunch"),", to the students of ",Object(o.b)("a",{parentName:"p",href:"https://www.rug.nl/masters/voice-technology/"},"University of Groningen‚Äôs new\nMSc in Voice Technology"),", and at Hyde Park‚Äôs\nSpeakers‚Äô Corner soapbox. (Well, not really the Hyde Park bit.)"),Object(o.b)("p",null,"Anyway, enjoy the newsletter!"),Object(o.b)("h3",{id:"coqui-stt-10-released",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#coqui-stt-10-released","aria-label":"coqui stt 10 released permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Coqui üê∏STT 1.0 Released"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"72%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQGAQP/xAAWAQEBAQAAAAAAAAAAAAAAAAAGBAX/2gAMAwEAAhADEAAAAX1ucmRTVxhLqf/EABkQAAMBAQEAAAAAAAAAAAAAAAMEBQIBIv/aAAgBAQABBQLPOMGVXC1o/glXpVk5Vgm7z9Pax//EABwRAAICAgMAAAAAAAAAAAAAAAECACESMSJR8P/aAAgBAwEBPwFODqrDHdDXqhJ7n//EAB4RAQABAwUBAAAAAAAAAAAAAAECAAMREiFBYbHh/9oACAECAQE/AWI2pysuQxvz37RqT61//8QAIhAAAgEEAgEFAAAAAAAAAAAAAQIDABESIQQxIiNBUWJx/9oACAEBAAY/ArxZABdtPpVH52aCxWSaLd3Hi9/kURLxuRl9DkKl5gl3Enpi3RrBjksyBDlWAQde1f/EAB4QAQEAAgICAwAAAAAAAAAAAAERACExUWFxkaHB/9oACAEBAAE/IdgmzaraPZ9GQJACHkHZe8kR+FPwn7vESWgGoyvq4dQxwLZb85JjBy1n/9oADAMBAAIAAwAAABC4/wD/xAAaEQEAAgMBAAAAAAAAAAAAAAABETEAUWGB/9oACAEDAQE/EGhqpBZIXvG3wTDn/8QAGxEBAAICAwAAAAAAAAAAAAAAAREhAIExcfD/2gAIAQIBAT8Qt8xKbAwvkjpW1ETxtz//xAAdEAEBAAMBAAMBAAAAAAAAAAABEQAhMUFhcYGR/9oACAEBAAE/EOPe1WRKjODEa3JPcYFeVAEOhBKMQQrykpBca06APcRkiFCLNGH5b9YR6aKQA4EP8+cWkFIu2xdl3L+5/9k=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/release.jpg",srcSet:["/static/1e77dc6fce06f5a577ab0e4d0674e779/0988f/release.jpg 250w","/static/1e77dc6fce06f5a577ab0e4d0674e779/d1f95/release.jpg 500w","/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/release.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/reuben"},"Reuben Morais")),Object(o.b)("p",null,"Today Coqui is proud to release ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/STT/releases/v1.0.0"},"Coqui STT 1.0"),",\nthe first stable release of our open source speech-to-text engine. Coqui STT offers high quality,\noffline speech-to-text technology that‚Äôs simple to integrate into your applications, thanks to\nour easy to use API which supports multiple architectures, platforms, and programming languages."),Object(o.b)("p",null,"We have continuously worked to make our speech-to-text engine faster, more capable and easier to\nuse. Developers are already relying on our technology to build voice powered applications around\nthe world (and maybe even on the moon!). Today, we are proud to announce our first stable release.\nWhen you integrate Coqui STT 1.0 into your application, you‚Äôll be able to easily upgrade to newer\n1.x versions and take advantage of performance improvements and bug fixes without having to change\nyour code."),Object(o.b)("p",null,"Coqui STT 1.0 is available for download from our ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/STT/releases/v1.0.0"},"release page"),".\nSince our last release, 0.9.3, we‚Äôve continued to work on performance, stability, flexibility and\naccuracy. We‚Äôve"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Moved exclusively to TensorFlow Lite to make sure our most important runtime variant receives full attention and care,"),Object(o.b)("li",{parentName:"ul"},"Released experimental iOS support,"),Object(o.b)("li",{parentName:"ul"},"Landed built-in support for transfer learning and fine tuning,"),Object(o.b)("li",{parentName:"ul"},"Added support for using our training and evaluation tools from Jupyter Notebooks,"),Object(o.b)("li",{parentName:"ul"},"Streamlined alphabet generation and dataset splitting,")),Object(o.b)("p",null,"and more!"),Object(o.b)("p",null,"Try Coqui STT today by following our ",Object(o.b)("a",{parentName:"p",href:"https://stt.readthedocs.io/en/latest/"},"usage guide")," for your\nfavorite programming language. Our ",Object(o.b)("a",{parentName:"p",href:"https://coqui.ai/english/coqui/v1.0.0-huge-vocab"},"1.0 English model")," has\nbeen trained on over 47 thousands of hours of speech data, including the latest release of the\nCommon Voice dataset with 2000 hours of speech. It is available for download in the\n",Object(o.b)("a",{parentName:"p",href:"https://coqui.ai/english/coqui/v1.0.0"},"Coqui Model Zoo"),", where we also document all of the training\nparameters that were used to create it. You can also find our latest release in your favorite package\nmanager: we offer packages for Python, Node.JS, Electron, Java on Android, and C/C++."),Object(o.b)("h3",{id:"-best-ever-stt-english-model-",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#-best-ever-stt-english-model-","aria-label":" best ever stt english model  permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üî• Best EVER üê∏STT English model üî•"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAUEBgj/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/aAAwDAQACEAMQAAABzU9T2ML0xBED/8QAGxAAAwACAwAAAAAAAAAAAAAAAgMEAAEFBhH/2gAIAQEAAQUC0O9ZJxdVwOkNDDP0I+wVQyuoNzf/xAAXEQEAAwAAAAAAAAAAAAAAAAACEBEx/9oACAEDAQE/AUKyP//EABYRAQEBAAAAAAAAAAAAAAAAAAIQUf/aAAgBAgEBPwErZ//EAB8QAAEDAwUAAAAAAAAAAAAAAAIAAQMREkEQEyExUf/aAAgBAQAGPwJXRREY+4TgbUJtNiJxYLrusopDKpFy7r//xAAaEAEAAgMBAAAAAAAAAAAAAAABABEhMUFR/9oACAEBAAE/ITr0RQDYSbeXO77FxqIkW+FYfA6x1n//2gAMAwEAAgADAAAAEJMf/8QAFxEBAAMAAAAAAAAAAAAAAAAAAQARMf/aAAgBAwEBPxA1TvNiAz//xAAXEQEBAQEAAAAAAAAAAAAAAAABACFx/9oACAECAQE/EFdzyFv/xAAcEAEBAAICAwAAAAAAAAAAAAABEQAhMVFBcZH/2gAIAQEAAT8QtpLz3jwL1IfBd8YbOXTZQeT3l6kADGgId0ABr1NY3rbm0auf/9k=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/55dce4b9588d7066049d2557093dabd2/dbdff/best-stt-model.jpg",srcSet:["/static/55dce4b9588d7066049d2557093dabd2/0988f/best-stt-model.jpg 250w","/static/55dce4b9588d7066049d2557093dabd2/d1f95/best-stt-model.jpg 500w","/static/55dce4b9588d7066049d2557093dabd2/dbdff/best-stt-model.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/JRMeyer"},"Josh Meyer")),Object(o.b)("p",null,"This is a very exciting day for us, because we are releasing our best ever Speech-to-Text model for\nEnglish!"),Object(o.b)("p",null,"We trained this model on almost 47,000 hours of English ü§Ø ‚Ä¶ And it shows"),Object(o.b)("h4",{id:"-accurate",style:{position:"relative"}},Object(o.b)("a",{parentName:"h4",href:"#-accurate","aria-label":" accurate permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üëâ Accurate"),Object(o.b)("p",null,"‚úîÔ∏è Huge improvement in accuracy over all past versions"),Object(o.b)("p",null,"With this new model, we get 4.5% word error rate on Librispeech clean (better than humans!). Our\nprevious v0.9.3 model has a 7.1% WER, so you should upgrade as soon as possible to feel the\ndifference:)"),Object(o.b)("p",null,"More diverse voices in the training data means better demographic representation, and better\nrepresentation means better Speech-to-Text for everyone!"),Object(o.b)("h4",{id:"-fast--local--small",style:{position:"relative"}},Object(o.b)("a",{parentName:"h4",href:"#-fast--local--small","aria-label":" fast  local  small permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üëâ Fast && Local && Small"),Object(o.b)("p",null,"‚úîÔ∏è It runs on your computer"),Object(o.b)("p",null,"‚úîÔ∏è It runs on your phone"),Object(o.b)("p",null,"‚úîÔ∏è It runs on your watch"),Object(o.b)("p",null,"We‚Äôve doubled down on making models go everywhere you go. For the first time, we‚Äôre releasing two\nversions of the model ready for Tensorflow Lite, in two sizes: small and extra-small:)"),Object(o.b)("p",null,"Don‚Äôt let the sizes trick you, both these models are ready for the big leagues. The extra-small\nmodel has optimally quantized weights, bringing this (open vocabulary!) speech recognizer down\nto just 46 Megabytes. Oftentimes your application will benefit from a language model, which\ncomes with a size‚ÜîÔ∏èaccuracy tradeoff. We are releasing a few language models so you can try out\nfor yourself a language model under a kilobyte versus one in the order of Gigs. Depending on your\napplication, you can choose the right model for you‚Ä¶ or you can build your own!"),Object(o.b)("h4",{id:"-easy-to-customize",style:{position:"relative"}},Object(o.b)("a",{parentName:"h4",href:"#-easy-to-customize","aria-label":" easy to customize permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üëâ Easy to Customize"),Object(o.b)("p",null,"STT is built so that you can easily improve performance for your application, no matter how\nspecific or how rare the vocabulary."),Object(o.b)("p",null,"If you‚Äôve got doctors talking about doctor things, no problem! Lawyers talking about lawyer\nthings? No sweat üòé No human knows all the jargon in all domains, but anyone can learn‚Ä¶\nAnd the same goes for Coqui STT!"),Object(o.b)("p",null,"This kind of customization happens at the language model, and you can easily (and quickly!)\ncreate custom language models for your custom applications."),Object(o.b)("h4",{id:"-download-it-yourself",style:{position:"relative"}},Object(o.b)("a",{parentName:"h4",href:"#-download-it-yourself","aria-label":" download it yourself permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üëâ Download it yourself"),Object(o.b)("p",null,"‚úîÔ∏è Newest member of Model Zoo"),Object(o.b)("p",null,"‚úîÔ∏è Apache 2.0 licensed"),Object(o.b)("p",null,"We love open source, and we‚Äôre releasing this new v1.0.0 model under the Apache 2.0 license üéâ"),Object(o.b)("p",null,"Download this new model from the Coqui model zoo, and check out all the other community models!"),Object(o.b)("h3",{id:"v030-tts-delivered-fresh",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#v030-tts-delivered-fresh","aria-label":"v030 tts delivered fresh permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"v0.3.0, üê∏TTS Delivered Fresh"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"72%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQGAQP/xAAWAQEBAQAAAAAAAAAAAAAAAAAGBAX/2gAMAwEAAhADEAAAAX1ucmRTVxhLqf/EABkQAAMBAQEAAAAAAAAAAAAAAAMEBQIBIv/aAAgBAQABBQLPOMGVXC1o/glXpVk5Vgm7z9Pax//EABwRAAICAgMAAAAAAAAAAAAAAAECACESMSJR8P/aAAgBAwEBPwFODqrDHdDXqhJ7n//EAB4RAQABAwUBAAAAAAAAAAAAAAECAAMREiFBYbHh/9oACAECAQE/AWI2pysuQxvz37RqT61//8QAIhAAAgEEAgEFAAAAAAAAAAAAAQIDABESIQQxIiNBUWJx/9oACAEBAAY/ArxZABdtPpVH52aCxWSaLd3Hi9/kURLxuRl9DkKl5gl3Enpi3RrBjksyBDlWAQde1f/EAB4QAQEAAgICAwAAAAAAAAAAAAERACExUWFxkaHB/9oACAEBAAE/IdgmzaraPZ9GQJACHkHZe8kR+FPwn7vESWgGoyvq4dQxwLZb85JjBy1n/9oADAMBAAIAAwAAABC4/wD/xAAaEQEAAgMBAAAAAAAAAAAAAAABETEAUWGB/9oACAEDAQE/EGhqpBZIXvG3wTDn/8QAGxEBAAICAwAAAAAAAAAAAAAAAREhAIExcfD/2gAIAQIBAT8Qt8xKbAwvkjpW1ETxtz//xAAdEAEBAAMBAAMBAAAAAAAAAAABEQAhMUFhcYGR/9oACAEBAAE/EOPe1WRKjODEa3JPcYFeVAEOhBKMQQrykpBca06APcRkiFCLNGH5b9YR6aKQA4EP8+cWkFIu2xdl3L+5/9k=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/release.jpg",srcSet:["/static/1e77dc6fce06f5a577ab0e4d0674e779/0988f/release.jpg 250w","/static/1e77dc6fce06f5a577ab0e4d0674e779/d1f95/release.jpg 500w","/static/1e77dc6fce06f5a577ab0e4d0674e779/dbdff/release.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(o.b)("p",null,"This new version introduces a new ",Object(o.b)("a",{parentName:"p",href:"https://tts.readthedocs.io/en/latest/models/forward_tts.html"},"ForwardTTS API"),"\nand 2 pre-trained models, an English ",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/2008.03802"},"SpeedySpeech")," model\ntrained on LJSpeech and a fine-tuned ",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/2106.07889"},"UnivNet")," vocoder."),Object(o.b)("p",null,"Forward TTS API provides a unified interface for all the current and future feed-forward TTS\nmodels. We currently support ",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/2008.03802"},"SpeedySpeech"),",\n",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/2006.06873"},"FastPitch"),", ",Object(o.b)("a",{parentName:"p",href:"https://arxiv.org/abs/1905.09263"},"FastSpeech"),"\narchitectures through this API. You can also create your own custom architectures by defining\nyour own Encoder, Decoder networks without any other fine-grained changes."),Object(o.b)("p",null,"We are also releasing the following pre-trained models:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"English ",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2008.03802"},"SpeedySpeech")," model trained on the LJSpeech dataset.\nThis is the most compact TTS model we‚Äôve released to the date.")),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-bash"},'tts --text "This is a sample text for my model to speak." --model_name tts_models/en/ljspeech/speedy-speech\n')),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.07889"},"UnivNet")," vocoder fine-tuned for the above model.")),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-bash"},'tts --text "This is how it is." --model_name tts_models/en/ljspeech/tacotron2-DDC_ph\n')),Object(o.b)("p",null,"Check the ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS/releases/tag/v0.3.0"},"release notes")," for all the details\nof the release and ",Object(o.b)("a",{parentName:"p",href:"https://github.com/coqui-ai/TTS/issues/378"},"dev plans")," to see what is next."),Object(o.b)("h3",{id:"tts-papers-picks-of-the-month",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#tts-papers-picks-of-the-month","aria-label":"tts papers picks of the month permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üìéTTS papers picks of the month"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAUGA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAG8fTjqGJiV/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAECAxEhIv/aAAgBAQABBQKjsYYnqRTLzWzT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAABABETH/2gAIAQEABj8CKULD/8QAHRAAAgIBBQAAAAAAAAAAAAAAAREAITEQUXGhsf/aAAgBAQABPyHIa9jhO4KABXMVUMCFR30P/9oADAMBAAIAAwAAABCMz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB0QAQEAAgMAAwAAAAAAAAAAAAERACExQWFRgfD/2gAIAQEAAT8QE79Ycp9ayZ+Irb85V2jkm+5fmpmEui4RejKd/pkoKGl5c//Z')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/ee7ae83975f5a8e36a44fa9649c1ad37/dbdff/monthly-papers.jpg",srcSet:["/static/ee7ae83975f5a8e36a44fa9649c1ad37/0988f/monthly-papers.jpg 250w","/static/ee7ae83975f5a8e36a44fa9649c1ad37/d1f95/monthly-papers.jpg 500w","/static/ee7ae83975f5a8e36a44fa9649c1ad37/dbdff/monthly-papers.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/erogol"},"Eren G√∂lge")),Object(o.b)("h4",{id:"towards-universal-text-to-speech-pdf",style:{position:"relative"}},Object(o.b)("a",{parentName:"h4",href:"#towards-universal-text-to-speech-pdf","aria-label":"towards universal text to speech pdf permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"Towards Universal Text-to-Speech (",Object(o.b)("a",{parentName:"h4",href:"http://www.interspeech2020.org/uploadfile/pdf/Wed-3-4-3.pdf"},"PDF"),")"),Object(o.b)("p",null,"This paper presents a multilingual TTS framework that can be trained on 1250 hours of professional recordings\nfrom multiple languages and then fine-tuned with new speakers and languages with only 5-6 minutes of recordings."),Object(o.b)("p",null,"The model proposed in the paper comprises an encoder-decoder Transformer model with a speaker and language\nconditioning networks and a WaveNet vocoder. The conditioning networks take one-hot vectors representing\nthe speaker or language ID and project to a hidden conditioning representation that is concatenated to\nthe encoder output."),Object(o.b)("p",null,"This show that this model can learn low-resource languages that cannot be learned by single speaker models\ndue to insufficient data. In their experiments, a single speaker model needs at least 3 hours of recordings\nwhereas the proposed model is able to learn with only 6 minutes."),Object(o.b)("h4",{id:"adaspeech-adaptive-text-to-speech-for-custom-voice-pdf",style:{position:"relative"}},Object(o.b)("a",{parentName:"h4",href:"#adaspeech-adaptive-text-to-speech-for-custom-voice-pdf","aria-label":"adaspeech adaptive text to speech for custom voice pdf permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"AdaSpeech: Adaptive Text to Speech for Custom Voice (",Object(o.b)("a",{parentName:"h4",href:"https://openreview.net/pdf?id=Drynvt7gg4L"},"PDF"),")"),Object(o.b)("p",null,"This paper presents a way to learn new speakers with a single backbone architecture while keeping the number\nof new parameters introduced to the system small."),Object(o.b)("p",null,"The overall architecture is based on the FastSpeech2 model with Pitch and Variance predictors. They also\nintroduce 3 more conditioning networks for better understanding the acoustic properties of a speaker."),Object(o.b)("p",null,"All the networks use Layer Normalisation at different stages. They proposed Conditional Layer Normalisation\nas a means to reduce the complexity required for fine-tuning with each new speaker."),Object(o.b)("p",null,"They propose to train separate Speaker Conditioning modules to predict the scale and bias values for each\nLayer Normalisation layer. This way, they are able to store only these Speaker Conditioning modules for\neach speaker and keep the rest of the model the same as they add more speakers to the system"),Object(o.b)("h3",{id:"-building-coqui-stt-for-responsiveness",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#-building-coqui-stt-for-responsiveness","aria-label":" building coqui stt for responsiveness permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üèÉüèø Building Coqui STT for responsiveness"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABQAG/8QAFwEAAwEAAAAAAAAAAAAAAAAAAQIEBf/aAAwDAQACEAMQAAABMJzziY5Njo2//8QAGxAAAgMAAwAAAAAAAAAAAAAAAwQCBQYBByX/2gAIAQEAAQUC6/qRWZec/wCkXOGaNlbNgTtZObgnyRgz/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAEhAjH/2gAIAQMBAT8B32CUP//EABYRAQEBAAAAAAAAAAAAAAAAAAERAP/aAAgBAgEBPwFJdHf/xAAjEAACAQMEAQUAAAAAAAAAAAABAgMAERIEISIxFDJBUWKx/9oACAEBAAY/AtRJqI0OmVSCCw4/a1L4jCSI7qrG2R9xUrI0YAa3OZQf2hDEyoswxe63yHxTrHI0GJJv6t8rUy4ddm/df//EABsQAQACAwEBAAAAAAAAAAAAAAERIQAxQWHw/9oACAEBAAE/Ib/j1VOlx7jhSNBYVb9eJ0gwEnjh8h42jtjvAmWyBO1L3B8pUdDrqs//2gAMAwEAAgADAAAAECA//8QAGREBAAMBAQAAAAAAAAAAAAAAAQARIbFh/9oACAEDAQE/EA1TW76JnOxwFz//xAAZEQADAAMAAAAAAAAAAAAAAAAAAREhoeH/2gAIAQIBAT8QZQUcPXT/xAAbEAEBAAMBAQEAAAAAAAAAAAABEQAhMVFBgf/aAAgBAQABPxAMTJpYdlAJHFmH2ghTIjbyD2JrDpTfiKNWqH5h14UEaQYxnxMloq0AKAAGirQpMd0YrlWBwfKz1z//2Q==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/2d77532e872dd695eb55f2495fa33512/dbdff/stt-responsiveness.jpg",srcSet:["/static/2d77532e872dd695eb55f2495fa33512/0988f/stt-responsiveness.jpg 250w","/static/2d77532e872dd695eb55f2495fa33512/d1f95/stt-responsiveness.jpg 500w","/static/2d77532e872dd695eb55f2495fa33512/dbdff/stt-responsiveness.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/reuben"},"Reuben Morais")),Object(o.b)("p",null,"One of the most exciting aspects of working on speech technology is the promise of a future where interactions\nwith devices can be as fluid and effortless as telling someone what you‚Äôre trying to achieve. To be understood\nby devices rather than needing to learn many languages."),Object(o.b)("p",null,"In everyday usage, we usually start off with a very clear task in mind: check the shipping status of the thing\nyou ordered last week, try to find a good day and time to fit a coffee and catch-up with a friend, make sure\nyou‚Äôre not forgetting any of the documents you‚Äôre supposed to bring for the visa application. But before you\ncan accomplish those tasks, you have to convert them into sequences of steps in a language that your phone\ncan understand."),Object(o.b)("p",null,"We‚Äôre all essentially programming our devices, and not in a fun, playful way: we still have to work to get\nthem to do very simple tasks. As the technical capabilities of consumer devices grow more and more, so does\nthe complexity of the languages we use to control them. As these devices become more ubiquitous and more\ninteractive, from wearables to virtual reality to brain-computer interfaces, we need better ways to interact\nwith and control our devices, if we want to explore all their potential."),Object(o.b)("p",null,"At Coqui we believe conversational interfaces are the key to solving many of these user experience problems\nand multiplying the potential of what devices can do for all people, even those who aren‚Äôt great fans of\nprogramming. Speech technology has made great strides in the past few years: we‚Äôve built systems that learn\nlanguages by themselves, and that often perform better than humans. But there‚Äôs also still a lot to do:\nspeech AI needs to speak many more languages, it needs to be more intelligent and adaptable, and it needs to\nbe more responsive."),Object(o.b)("p",null,"We‚Äôre hard at work making this a reality: a key focus of our speech-to-text engine Coqui STT has been\nresponsiveness, from the conception of the project. Latency in user interactions matters. A lot. Ask a user\nexperience designer. Ask someone who works on optimizing conversions. Or just remember what it feels like to\nhave a weird conversation filled with awkward silences. This is why we spent a ton of time optimizing the\ninference library that powers all üê∏STT packages: keeping the core library lean, making it end-to-end\nstreamable, using efficient data structures that can be loaded quickly from disk, and testing it extensively."),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"50%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABcRAAAXEQHKJvM/AAACBElEQVQoz32T63KbMBCFef/36Rv0Fk8y6aSp41twMBiDDQiExE3A1yXpTH+1Z0ajRVqdvZzFs8GROvCp6pqqLNFaY62FeaapNbqqqK5HVHahUgqtCt7hNPR/bMHc3nAmxnt7XvH69J0wOhMGAVEYvhMvGPoe50aG6sTYlrjB4frug6FLoTn/JdQ+fb7BS8uE8y3k185nE4SsdweO54SittyfFKvI8Hn9yuaScxfWfDl+nCWSsS5CcuG3C2N/Zshe8LqmlSwctlA02x2tlFv7PiY4oSWAOV04hy9UWUrph5RBhAoS8u2KPNvRXQv6IEavPpElj3j9UtY4UhVyIauutPQspZWyizjGZhnB6SDniuTtSFcqqlTI85j4Folfhb1lxMcNyuZ4szT/X+i6lqLIMSKUkmCFCGJMzf/g6abDdo7M9OSNo7DLPlC2jqsZKNqJa91T95PYI5n45HagcRO9CNaNE05y6oaBcRrxbrolqTru9lfufp54ChU/9gnrt4zHQ8rDNubrPmMTKe63Cd/WZx72KZerjJMELZShqQxOSh+nWUTpOqZ5YpDS5tcDY2MZwjf6S0QTBYyytLoxGy1i7ZjTC81yH4c0TUOfyLd/oHz+KYQT3iiCTGIYUXuQHljZrQjVivJGelgaiy9ipNcbR5nRpbxKJqGXN8sP0Ilf44b3Nwt+A6Qn+HYezBrAAAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/6b9f563c961d7e0a23c19ddf5d944531/da8b6/stt-latancy.png",srcSet:["/static/6b9f563c961d7e0a23c19ddf5d944531/43fa5/stt-latancy.png 250w","/static/6b9f563c961d7e0a23c19ddf5d944531/c6e3d/stt-latancy.png 500w","/static/6b9f563c961d7e0a23c19ddf5d944531/da8b6/stt-latancy.png 1000w","/static/6b9f563c961d7e0a23c19ddf5d944531/2e9ed/stt-latancy.png 1500w","/static/6b9f563c961d7e0a23c19ddf5d944531/9fabd/stt-latancy.png 2000w","/static/6b9f563c961d7e0a23c19ddf5d944531/6f79e/stt-latancy.png 2100w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"We‚Äôve previously talked about some of this work, such as the changes needed to make the entire inference\nprocess streamable. The diagram above shows the latency gains from this change alone. You can read more\nabout it in ",Object(o.b)("a",{parentName:"p",href:"https://coqui.ai/blog/stt/deepspeech-0-6-speech-to-text-engine"},"our blog post"),"."),Object(o.b)("p",null,"We think responsiveness is crucial for building engaging, enjoyable user experiences with speech. We will\ncontinue to push the boundaries of performance for speech AI, creating more accurate and more efficient\nmodels, moving processing as close as possible to end users. We‚Äôll make it as easy as calling a SaaS\nendpoint on a datacenter half a continent away, but with snappy responses for engaged conversations without\nawkward silences."),Object(o.b)("p",null,"If this sounds like something you‚Äôd like to work on, ",Object(o.b)("a",{parentName:"p",href:"https://coqui.ai/jobs"},"we‚Äôre hiring"),"! Coqui is looking\nfor talented engineers with experience in speech AI, MLOps and full stack web development. Come help us\nmake this future a reality!"),Object(o.b)("h3",{id:"-voice-lunch",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#-voice-lunch","aria-label":" voice lunch permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üç± Voice Lunch"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"66.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAANABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAMCBQYH/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwT/2gAMAwEAAhADEAAAAe1QtFEulFFAf//EABoQAAMBAAMAAAAAAAAAAAAAAAMEBQACEhP/2gAIAQEAAQUCqvlQWn1SEJvLiyNOKsmXtv/EABgRAQEAAwAAAAAAAAAAAAAAAAIBEBEx/9oACAEDAQE/AShBq9x//8QAGBEBAQADAAAAAAAAAAAAAAAAAQIDECH/2gAIAQIBAT8BuMjYjzX/xAAgEAACAgEDBQAAAAAAAAAAAAABAgMREgAQMRMhMmGR/9oACAEBAAY/AlMCZyuaHy9RM0yywyt06ruG9bYuLHOlkUFnUsy5HxJ52//EABwQAQACAwADAAAAAAAAAAAAAAEAESExUUFxgf/aAAgBAQABPyHEaXSwyV/CEH4CRYpY2Yd37gDE3OKdHpGj4F3kg7WL5Lk//9oADAMBAAIAAwAAABA7/wD/xAAZEQADAAMAAAAAAAAAAAAAAAAAAUERIcH/2gAIAQMBAT8QeEVOzZk//8QAGhEAAQUBAAAAAAAAAAAAAAAAQQABESFh8P/aAAgBAgEBPxATIdujLcVDr//EABsQAQEAAwADAAAAAAAAAAAAAAERACExQXGR/9oACAEBAAE/EAucHiUFLt1T2Zo6rDogOgQRLw4DTjkcYEGD4nhK/cjpuf0JwC6KiQguCUl3n//Z')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/26e280c36695e2d0c351ef812239d70f/dbdff/voice-lunch.jpg",srcSet:["/static/26e280c36695e2d0c351ef812239d70f/0988f/voice-lunch.jpg 250w","/static/26e280c36695e2d0c351ef812239d70f/d1f95/voice-lunch.jpg 500w","/static/26e280c36695e2d0c351ef812239d70f/dbdff/voice-lunch.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/JRMeyer"},"Josh Meyer")),Object(o.b)("p",null,"Coqui made its first appearance on ",Object(o.b)("a",{parentName:"p",href:"https://voicelunch.com/"},"Voice Lunch"),". Our co-founder, Josh Meyer, walked a\nvirtual room full of linguists (many of whom don‚Äôt usually write code) through one of our beginner-friendly\nPython notebooks. Each participant was able to train a speech-to-text model for the Serbian language, on their\nvery own!"),Object(o.b)("p",null,"Check out the ",Object(o.b)("a",{parentName:"p",href:"https://colab.research.google.com/github/coqui-ai/STT/blob/main/notebooks/train_with_common_voice.ipynb"},"notebook"),"\nfor yourself and train your own model‚Ä¶ you don‚Äôt have to know how to code, and it‚Äôs fun!"),Object(o.b)("h3",{id:"-university-of-groningen",style:{position:"relative"}},Object(o.b)("a",{parentName:"h3",href:"#-university-of-groningen","aria-label":" university of groningen permalink",className:"anchor before"},Object(o.b)("svg",{parentName:"a",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",focusable:"false",viewBox:"0 0 16 16"},"\n  ",Object(o.b)("path",{parentName:"svg",fill:"currentColor",d:"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z"}))),"üéì University of Groningen"),Object(o.b)("p",null,Object(o.b)("span",{parentName:"p",className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1000px"}},"\n      ",Object(o.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"18.8%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/jpeg;base64,/9j/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wgARCAAEABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAEDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAT/2gAMAwEAAhADEAAAAYuCs1QZ3//EABkQAQACAwAAAAAAAAAAAAAAAAMCBQABEf/aAAgBAQABBQKVYSFXnqKR4ef/xAAYEQACAwAAAAAAAAAAAAAAAAAAAgEiMf/aAAgBAwEBPwFJsLh//8QAFhEAAwAAAAAAAAAAAAAAAAAAAhAy/9oACAECAQE/ATlf/8QAGxAAAgMAAwAAAAAAAAAAAAAAAQIAAxExQoH/2gAIAQEABj8CpbXBXjGlFPUQqFHs/8QAGBABAAMBAAAAAAAAAAAAAAAAAQARITH/2gAIAQEAAT8hYFd4utwMaVW9cgNAC7pn/9oADAMBAAIAAwAAABD8P//EABgRAAMBAQAAAAAAAAAAAAAAAAABESHw/9oACAEDAQE/EGVvYM4P/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAEhMf/aAAgBAgEBPxCEHp//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhQaH/2gAIAQEAAT8Qp8tTGsSBusEhqmpDx55MkWwpvW5//9k=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(o.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"IMAGE",title:"IMAGE",src:"/static/586b2ac858f0026a7e8a17be070cd150/dbdff/groningen.jpg",srcSet:["/static/586b2ac858f0026a7e8a17be070cd150/0988f/groningen.jpg 250w","/static/586b2ac858f0026a7e8a17be070cd150/d1f95/groningen.jpg 500w","/static/586b2ac858f0026a7e8a17be070cd150/dbdff/groningen.jpg 1000w"],sizes:"(max-width: 1000px) 100vw, 1000px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    ")),Object(o.b)("p",null,"By ",Object(o.b)("a",{parentName:"p",href:"https://github.com/JRMeyer"},"Josh Meyer")),Object(o.b)("p",null,"Coqui has already made its way into the curriculum of the next generation of Speech Tech developers. Josh Meyer\ngave ",Object(o.b)("a",{parentName:"p",href:"https://docs.google.com/presentation/d/1nz0PAFfApny3rfZM3v_BUOIlJRy4XR1Jys3TOWNllPY/edit?usp=sharing"},"an invited lecture"),"\nto the inaugural class of Masters students at the ",Object(o.b)("a",{parentName:"p",href:"https://www.rug.nl/masters/voice-technology/"},"University of Groningen‚Äôs new MSc in Voice Technology"),".\nThese students will be building the future of voice, and they‚Äôre starting their journey with Coqui in their\ntoolbox."),Object(o.b)("p",null,"The presentation covered both STT and TTS, with a walk-through of one of our\n",Object(o.b)("a",{parentName:"p",href:"https://colab.research.google.com/github/coqui-ai/STT/blob/main/notebooks/train_with_common_voice.ipynb"},"Python notebooks"),".\nThe University of Groningen is located in a Frisian-speaking region of the Netherlands, and there‚Äôs a keen\ninterest in traditionally under-served languages among students and faculty. Coqui‚Äôs fine-tuning capabilities\nand multilingual lean make it a great fit for this next generation of language technologists."))}c.isMDXComponent=!0},yDk1:function(e,a,t){"use strict";var A=t("q1tI"),o=t.n(A),n=t("O9mE"),s=t("v+Ly"),r=t("mrST"),i=t("1Yd/"),l=t("ozyN"),c=t("7cfw"),p=t("t4Fg");a.a=function(e){var a=e.children,t=e.data,b=e.pageContext,h=t.mdx;return Object(A.useEffect)((function(){Object(p.a)()})),o.a.createElement(n.a,null,o.a.createElement(i.a,{title:b.frontmatter.title+" / Newsletter",description:b.frontmatter.description||h.excerpt}),o.a.createElement(s.a,null,o.a.createElement(r.a,{title:b.frontmatter.title,subtitle:b.frontmatter.description,name:b.frontmatter.name,picture:b.frontmatter.picture,date:b.frontmatter.date,toc:h.tableOfContents.items},o.a.createElement(l.a,null,a))),o.a.createElement(c.a,null))}}}]);
//# sourceMappingURL=component---src-pages-newsletter-04-10-2021-mdx-136f785d9fa73d934f62.js.map