{"componentChunkName":"component---src-pages-blog-stt-deepspeech-0-6-speech-to-text-engine-mdx","path":"/blog/stt/deepspeech-0-6-speech-to-text-engine","result":{"data":{"mdx":{"id":"20e7b302-4193-56e5-873b-6bd0aa8d5291","excerpt":"The Machine Learning team continues work on Coqui STT, an automatic speech recognition (ASR) engine which aims to make\nspeech recognitionâ€¦","body":"var _excluded = [\"components\"];\n\nvar _templateObject;\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\nfunction _taggedTemplateLiteral(strings, raw) { if (!raw) { raw = strings.slice(0); } return Object.freeze(Object.defineProperties(strings, { raw: { value: Object.freeze(raw) } })); }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar pageQuery = graphql(_templateObject || (_templateObject = _taggedTemplateLiteral([\"\\n  query($fileAbsolutePath: String) {\\n    ...SidebarPageFragment\\n  }\\n\"])));\nvar _frontmatter = {\n  \"title\": \"STT Fast, Lean, and Ubiquitous\",\n  \"name\": \"Reuben Morais\",\n  \"picture\": \"https://secure.gravatar.com/avatar/a0806241b0bfd0b4339c8d987d98b6db?s=128&d=mm&r=g\",\n  \"date\": \"December 5, 2019\"\n};\nvar layoutProps = {\n  pageQuery: pageQuery,\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"The Machine Learning team continues work on Coqui STT, an automatic speech recognition (ASR) engine which aims to make\\nspeech recognition technology and trained models openly available to developers. Coqui STT is a deep learning-based ASR\\nengine with a simple API. We also provide pre-trained English models.\"), mdx(\"p\", null, \"Our latest release, version v0.6, offers the highest quality, most feature-packed model so far. In this overview, we\\u2019ll\\nshow how Coqui STT can transform your applications by enabling client-side, low-latency, and privacy-preserving speech\\nrecognition capabilities.\"), mdx(\"h3\", {\n    \"id\": \"consistent-low-latency\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#consistent-low-latency\",\n    \"aria-label\": \"consistent low latency permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Consistent low latency\"), mdx(\"p\", null, \"Coqui STT v0.6 includes a host of performance optimizations, designed to make it easier for application developers to\\nuse the engine without having to fine tune their systems. Our new streaming decoder offers the largest improvement,\\nwhich means Coqui STT now offers consistent low latency and memory utilization, regardless of the length of the audio\\nbeing transcribed. Application developers can obtain partial transcripts without worrying about big latency spikes.\"), mdx(\"p\", null, \"Coqui STT is composed of two main subsystems: an acoustic model and a decoder. The acoustic model is a deep neural\\nnetwork that receives audio features as inputs, and outputs character probabilities. The decoder uses a beam search\\nalgorithm to transform the character probabilities into textual transcripts that are then returned by the system.\"), mdx(\"p\", null, \"In a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/blog/stt/speech-recognition-deepspeech\"\n  }, \"previous blog post\"), \", I discussed how we made the acoustic model\\nstreamable. With both systems now capable of streaming, there\\u2019s no longer any need for carefully tuned silence detection\\nalgorithms in applications. \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/dabinat\"\n  }, \"dabinat\"), \", a long-term volunteer contributor to the Coqui STT\\ncode base, contributed this feature. Thanks!\"), mdx(\"p\", null, \"In the following diagram, you can see the same audio file being processed in real time by Coqui STT, before and after\\nthe decoder optimizations. The program requests an intermediate transcription roughly every second while the audio is\\nbeing transcribed. The dotted black line marks when the program has received the final transcription. Then, the distance\\nfrom the end of the audio signal to the dotted line represents how long a user must wait after they\\u2019ve stopped speaking\\nuntil the final transcript is computed and the application is able to respond.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABcRAAAXEQHKJvM/AAACBElEQVQoz32T63KbMBCFef/36Rv0Fk8y6aSp41twMBiDDQiExE3A1yXpTH+1Z0ajRVqdvZzFs8GROvCp6pqqLNFaY62FeaapNbqqqK5HVHahUgqtCt7hNPR/bMHc3nAmxnt7XvH69J0wOhMGAVEYvhMvGPoe50aG6sTYlrjB4frug6FLoTn/JdQ+fb7BS8uE8y3k185nE4SsdweO54SittyfFKvI8Hn9yuaScxfWfDl+nCWSsS5CcuG3C2N/Zshe8LqmlSwctlA02x2tlFv7PiY4oSWAOV04hy9UWUrph5RBhAoS8u2KPNvRXQv6IEavPpElj3j9UtY4UhVyIauutPQspZWyizjGZhnB6SDniuTtSFcqqlTI85j4Folfhb1lxMcNyuZ4szT/X+i6lqLIMSKUkmCFCGJMzf/g6abDdo7M9OSNo7DLPlC2jqsZKNqJa91T95PYI5n45HagcRO9CNaNE05y6oaBcRrxbrolqTru9lfufp54ChU/9gnrt4zHQ8rDNubrPmMTKe63Cd/WZx72KZerjJMELZShqQxOSh+nWUTpOqZ5YpDS5tcDY2MZwjf6S0QTBYyytLoxGy1i7ZjTC81yH4c0TUOfyLd/oHz+KYQT3iiCTGIYUXuQHljZrQjVivJGelgaiy9ipNcbR5nRpbxKJqGXN8sP0Ilf44b3Nwt+A6Qn+HYezBrAAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/6b9f563c961d7e0a23c19ddf5d944531/da8b6/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png\",\n    \"srcSet\": [\"/static/6b9f563c961d7e0a23c19ddf5d944531/43fa5/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png 250w\", \"/static/6b9f563c961d7e0a23c19ddf5d944531/c6e3d/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png 500w\", \"/static/6b9f563c961d7e0a23c19ddf5d944531/da8b6/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png 1000w\", \"/static/6b9f563c961d7e0a23c19ddf5d944531/2e9ed/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png 1500w\", \"/static/6b9f563c961d7e0a23c19ddf5d944531/9fabd/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png 2000w\", \"/static/6b9f563c961d7e0a23c19ddf5d944531/6f79e/blog-stt-deepspeech-0-6-speech-to-text-engine-plots.png 2100w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"In this case, the latest version of Coqui STT provides the transcription 260ms after the end of the audio, which is 73%\\nfaster than before the streaming decoder was implemented. This difference would be even larger for a longer recording.\\nThe intermediate transcript requests at seconds 2 and 3 of the audio file are also returned in a fraction of the time.\"), mdx(\"p\", null, \"Maintaining low latency is crucial for keeping users engaged and satisfied with your application. Coqui STT enables\\nlow-latency speech recognition services regardless of network conditions, as it can run offline, on users\\u2019 devices.\"), mdx(\"h3\", {\n    \"id\": \"tensorflow-lite-smaller-models-faster-start-up-times\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#tensorflow-lite-smaller-models-faster-start-up-times\",\n    \"aria-label\": \"tensorflow lite smaller models faster start up times permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"TensorFlow Lite, smaller models, faster start-up times\"), mdx(\"p\", null, \"We have added support for \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.tensorflow.org/lite\"\n  }, \"TensorFlow Lite\"), \", a version of TensorFlow that\\u2019s optimized\\nfor mobile and embedded devices. This has reduced the Coqui STT package size from 98 MB to 3.7 MB. It has reduced our\\nEnglish model size from 188 MB to 47 MB. We did this via \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.tensorflow.org/lite/performance/post_training_quantization\"\n  }, \"post-training\\nquantization\"), \", a technique to compress model\\nweights after training is done. TensorFlow Lite is designed for mobile and embedded devices, but we found that for Coqui\\nSTT it is even faster on desktop platforms. And so, we\\u2019ve made it available on Windows, macOS, and Linux as well as\\nRaspberry Pi and Android. \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Coqui STT v0.6 with TensorFlow Lite runs faster than real time on a single core of a\\nRaspberry Pi 4.\")), mdx(\"p\", null, \"The following diagram compares the start-up time and peak memory utilization for Coqui STT versions v0.4.1, v0.5.1, and\\nour latest release, v0.6.0.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"43.2%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAC4jAAAuIwF4pT92AAAB3UlEQVQoz3WSXWsTQRSG99+11gs/4o2U1q9iI3qjguB/8K4aRbBemlaxeiWI4oUg0koSepGWljYNyWa/0/3KZjebzePsbGJBcWBmDgfmmfe85yi6bjAY+FQqHkvLNj+qLzj9ushA3SGIEnStg67rdDpdDMPh2yeL1rHKycmxyHXo9Xoib6CqKrZto1iWTZIElEopigIfHy/Cpgi6W3z/CWtrGr++fEbffoLd2qFb36bTOuLg8JB2uy2hpmlKsOM4KLnCOPaZm8sk8P39S1AVgfaGpRvI3Hp5Fd4pjJtPgYw0DhgOYyEkEfcQz/PwfZ8wDM+A5xcK4NbDK1PgBtduFcBX5TJsKIyaz5kIJJMxszWZTCTQdV2CIMhLtsRPPgvnpsAHpX+A6zlQ2JA2KxI4yUYSlO8sy2Spuc/9fh/FMMz/Kry+MgXe+Qs4TiRMRAI4/qMwiqKzkufnZx5eLoB6leWb05JXb0+Bz2aFyjPJYCy2I7qraZr0U3FdTwQeFy+M5OMPj64WXe5tslIugK/v3YW3wsO9l4QjMQDdLla7gdqo4mp7BOFQjF4oP1EKM33hQUqtZrO/WyM5PSAe9MV8pdQbfZHbJbKaxKEju2uaNkf7NTy1Tho5uJ4vVebrNwEyWth5Kr8iAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/acc57edc910edd3f5d44cdbd37143f05/da8b6/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png\",\n    \"srcSet\": [\"/static/acc57edc910edd3f5d44cdbd37143f05/43fa5/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png 250w\", \"/static/acc57edc910edd3f5d44cdbd37143f05/c6e3d/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png 500w\", \"/static/acc57edc910edd3f5d44cdbd37143f05/da8b6/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png 1000w\", \"/static/acc57edc910edd3f5d44cdbd37143f05/2e9ed/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png 1500w\", \"/static/acc57edc910edd3f5d44cdbd37143f05/9fabd/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png 2000w\", \"/static/acc57edc910edd3f5d44cdbd37143f05/df6b6/blog-stt-deepspeech-0-6-speech-to-text-engine-startup-memory.png 2471w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"We now use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"22 times less memory\"), \" and start up over \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"500 times faster\"), \". Together with the optimizations we\\u2019ve\\napplied to our language model, a complete Coqui STT package including the inference code and a trained English model is\\nnow more than \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"50% smaller\"), \".\"), mdx(\"h3\", {\n    \"id\": \"confidence-value-and-timing-metadata-in-the-api\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#confidence-value-and-timing-metadata-in-the-api\",\n    \"aria-label\": \"confidence value and timing metadata in the api permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Confidence value and timing metadata in the API\"), mdx(\"p\", null, \"In addition, the new decoder exposes timing and confidence metadata, providing new possibilities for applications. We\\nnow offer an extended set of functions in the API, not just the textual transcript. You also get metadata timing\\ninformation for each character in the transcript, and a per-sentence confidence value.\"), mdx(\"p\", null, \"The example below shows the timing metadata extracted from Coqui STT from a sample audio file. The per-character timing\\nreturned by the API is grouped into word timings. You can see the waveform below. Click the \\u201CPlay\\u201D button to listen to\\nthe sample.\"), mdx(\"iframe\", {\n    width: \"100%\",\n    height: \"300\",\n    src: \"//jsfiddle.net/fwd9peL5/5/embedded/result/\",\n    allowFullScreen: \"allowfullscreen\",\n    allowpaymentrequest: true,\n    frameBorder: \"0\"\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://tehiku.nz/\"\n  }, \"Te Hiku Media\"), \" are using Coqui STT to develop and deploy the first \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/M%C4%81ori_language\"\n  }, \"Te reo\\nM\\u0101ori\"), \" automatic speech recognizer. They have been exploring the use\\nof the confidence metadata in our new decoder to build a digital pronunciation helper for Te reo M\\u0101ori. Recently, they\\nreceived a $13 million NZD investment from New Zealand\\u2019s Strategic Science Investment Fund to build \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://tehiku.nz/te-hiku-tech/papa-reo/\"\n  }, \"Papa Reo, a\\nmultilingual language platform\"), \". They are starting with New Zealand English\\nand Te reo M\\u0101ori.\"), mdx(\"h3\", {\n    \"id\": \"windowsnet-support\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#windowsnet-support\",\n    \"aria-label\": \"windowsnet support permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Windows/.NET support\"), mdx(\"p\", null, \"Coqui STT v0.6 now offers packages for Windows, with .NET, Python, JavaScript, and C bindings. Windows support was a\\nmuch-requested feature that was contributed by \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/carlfm01\"\n  }, \"Carlos Fonseca\"), \", who also wrote the .NET\\nbindings and examples. Thanks Carlos!\"), mdx(\"p\", null, \"You can find more details about our Windows support by looking at \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/STT/tree/v0.6.0/examples/net_framework/DeepSpeechWPF\"\n  }, \"the WPF\\nexample\"), \" (pictured below). It uses the\\n.NET bindings to create a small UI around Coqui STT. Our .NET package is available in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.nuget.org/packages/STT\"\n  }, \"NuGet\\nGallery\"), \". You can install it directly from Visual Studio.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"73.2%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB6UlEQVQ4y62TW4/TMBCF8/9/EC9IIB7QIvFQpC2720tIEyfNtbm6ju1cDmOnRbDqsiuEpU9jK5PJzPGJ8/5HgSTLEWQlGFEJDT7MaPWE7hVaOaKRA/JWgEuNx7SDs85aQHUY+zNG0QGjAgYJYMLb15JbnhWcL6xEcyrAjjGCMLLdllWNYVySxmmG0gPUMP6VeZrgVz2cD08h9g9rbLY7bIn7+zWCIABjzJ7zPMdEycMwvIimD2IawSoB584vkEUMcZIiy1IwKpYkicX3fdR1/erA87zEuNNwntrpDS/MNzGdL3GpeGxJw1XKAd1D9BJCCJvwr8t2+PVY4+h72O1dYm/HjOmCoihCHMdomgZt2/6BkcE8c13XxrbrIHgHL2+XDnlZoKKkuqltsqGqKluMc36TsixtMRPPNJmWAv6JL7YxBaXSlxvTUJe9lNLGq05GMxOfy3LVMGoknI/bCMzdwfUO8DyPrLLFev0dm80Gq9U32+WtS1qKL4zTcrGsJtt82seIScODH1j9wjD85cPD4WDtk6UpiqIgbY/IyJfPO7yew4aMvcoF/Q4aPY1pxtVm7IthlRmfkEpZGXqSQNF+Mh3+xngZmdVU8KFS+F8r6UjDd36DoOLYFBy70wsUV7qbbHOyDOV99iv8BAJEg7lqBCnNAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/6ea80cc6ad61e741d4b979637b3193d5/da8b6/blog-stt-deepspeech-0-6-speech-to-text-engine-screenshot.png\",\n    \"srcSet\": [\"/static/6ea80cc6ad61e741d4b979637b3193d5/43fa5/blog-stt-deepspeech-0-6-speech-to-text-engine-screenshot.png 250w\", \"/static/6ea80cc6ad61e741d4b979637b3193d5/c6e3d/blog-stt-deepspeech-0-6-speech-to-text-engine-screenshot.png 500w\", \"/static/6ea80cc6ad61e741d4b979637b3193d5/da8b6/blog-stt-deepspeech-0-6-speech-to-text-engine-screenshot.png 1000w\", \"/static/6ea80cc6ad61e741d4b979637b3193d5/d8661/blog-stt-deepspeech-0-6-speech-to-text-engine-screenshot.png 1238w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"You can see the WPF example that\\u2019s available in our repository. It contains code demonstrating transcription from an\\naudio file, and also from a microphone or other audio input device.\"), mdx(\"h3\", {\n    \"id\": \"centralized-documentation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#centralized-documentation\",\n    \"aria-label\": \"centralized documentation permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Centralized documentation\"), mdx(\"p\", null, \"We have centralized the documentation for all our language bindings in a single website,\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stt.readthedocs.io/\"\n  }, \"stt.readthedocs.io\"), \". You can find the documentation for C, Python, .NET, Java and\\nNodeJS/Electron packages. Given the variety of language bindings available, we wanted to make it easier to locate the\\ncorrect documentation for your platform.\"), mdx(\"h3\", {\n    \"id\": \"improvements-for-training-models\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#improvements-for-training-models\",\n    \"aria-label\": \"improvements for training models permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Improvements for training models\"), mdx(\"p\", null, \"With the upgrade to TensorFlow 1.14, we now leverage the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM\"\n  }, \"CuDNN RNN\\nAPIs\"), \" for our training code.\\nThis change gives us around 2x faster training times, which means faster experimentation and better models.\"), mdx(\"p\", null, \"Along with faster training, we now also support online feature augmentation, as described in Google\\u2019s \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1904.08779\"\n  }, \"SpecAugment\\npaper\"), \". This feature was contributed by \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://iarahealth.com/\"\n  }, \"Iara Health\"), \", a\\nBrazilian startup providing transcription services for health professionals. Iara Health has used online augmentation to\\nimprove their production Coqui STT models.\"), mdx(\"div\", {\n    align: \"center\"\n  }, ' ', mdx(\"iframe\", {\n    width: \"560\",\n    height: \"315\",\n    src: \"https://www.youtube.com/embed/uNqARK7euXk\",\n    frameBorder: \"0\",\n    allow: \"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\",\n    allowFullScreen: true\n  }), ' '), mdx(\"p\", null, \"The video above shows a customer using the Iara Health system. By using voice commands and dictation, the user instructs\\nthe program to load a template. Then, while looking at results of an MRI scan, they dictate their findings. The user can\\ncomplete the report without typing. Iara Health has trained their own Brazilian Portuguese models for this specialized\\nuse case.\"), mdx(\"h3\", {\n    \"id\": \"try-out-coqui-stt-v06\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#try-out-coqui-stt-v06\",\n    \"aria-label\": \"try out coqui stt v06 permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Try out Coqui STT v0.6\"), mdx(\"p\", null, \"The \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/STT/releases/v0.6.0\"\n  }, \"Coqui STT v0.6 release\"), \" includes our speech recognition engine as\\nwell as a trained English model. We provide binaries for six platforms and, as mentioned above, have bindings to various\\nprogramming languages, including Python, JavaScript, Go, Java, and .NET.\"), mdx(\"p\", null, \"The included English model was trained on 3816 hours of transcribed audio coming from \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://voice.mozilla.org/en/datasets\"\n  }, \"Common Voice\\nEnglish\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.openslr.org/12\"\n  }, \"LibriSpeech\"), \",\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/lrec2004-fisher-corpus.pdf\"\n  }, \"Fisher\"), \",\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://catalog.ldc.upenn.edu/LDC97S62\"\n  }, \"Switchboard\"), \". The model also includes around 1700 hours of transcribed WAMU\\n(NPR) radio shows. It achieves a 7.5% word error rate on the LibriSpeech test clean benchmark, and is faster than real\\ntime on a single core of a Raspberry Pi 4.\"), mdx(\"p\", null, \"Coqui STT v0.6 includes our best English model yet. However, most of the data used to train it is American English. For\\nthis reason, it doesn\\u2019t perform as well as it could on other English dialects and accents. A lack of publicly available\\nvoice data in other languages and dialects is part of why \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://voice.mozilla.org/\"\n  }, \"Common Voice\"), \" was created. We\\nwant to build a future where a speaker of Welsh or Basque or Scottish English has access to speech technology with the\\nsame standard of quality as is currently available for speakers of languages with big markets like American English,\\nGerman, or Mandarin.\"), mdx(\"p\", null, \"Want to participate in Common Voice? You can donate your voice by reading small text fragments. Or validate existing\\nrecordings in 40 different languages, with more to come. Currently, Common Voice represents the world\\u2019s largest public\\ndomain transcribed \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://voice.mozilla.org/en/datasets\"\n  }, \"voice dataset\"), \". The dataset consists of nearly 2,400 hours of\\nvoice data with 29 languages represented, including English, French, German, Spanish and Mandarin Chinese, but also for\\nexample Welsh and Kabyle.\"), mdx(\"p\", null, \"The v0.6 release is now available on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/STT/releases/v0.6.0\"\n  }, \"GitHub\"), \" as well as on your\\nfavorite package manager. You can download our pre-trained model and start using Coqui STT in minutes. If you\\u2019d like to\\nknow more, you can find detailed release notes in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/STT/releases/v0.6.0\"\n  }, \"GitHub release\"), \";\\ninstallation and usage explanations in our \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/STT/blob/v0.6.0/README.rst\"\n  }, \"README\"), \". If that\\ndoesn\\u2019t cover what you\\u2019re looking for, you can also use our \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/STT/discussions\"\n  }, \"discussion\\nforum\"), \".\"), mdx(\"h3\", {\n    \"id\": \"license\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#license\",\n    \"aria-label\": \"license permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"License\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://creativecommons.org/licenses/by-sa/3.0/\"\n  }, \"Creative Commons Attribution Share-Alike License v3.0\"), \" or any later\\nversion\"));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#consistent-low-latency","title":"Consistent low latency"},{"url":"#tensorflow-lite-smaller-models-faster-start-up-times","title":"TensorFlow Lite, smaller models, faster start-up times"},{"url":"#confidence-value-and-timing-metadata-in-the-api","title":"Confidence value and timing metadata in the API"},{"url":"#windowsnet-support","title":"Windows/.NET support"},{"url":"#centralized-documentation","title":"Centralized documentation"},{"url":"#improvements-for-training-models","title":"Improvements for training models"},{"url":"#try-out-coqui-stt-v06","title":"Try out Coqui STT v0.6"},{"url":"#license","title":"License"}]}}},"pageContext":{"frontmatter":{"title":"STT Fast, Lean, and Ubiquitous","name":"Reuben Morais","picture":"https://secure.gravatar.com/avatar/a0806241b0bfd0b4339c8d987d98b6db?s=128&d=mm&r=g","date":"December 5, 2019"},"fileAbsolutePath":"/home/runner/work/coqui-ai.github.io/coqui-ai.github.io/src/pages/blog/stt/deepspeech-0-6-speech-to-text-engine.mdx"}},"staticQueryHashes":["1942088059","3709355695","932324783"]}