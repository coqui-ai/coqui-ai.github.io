{"componentChunkName":"component---src-pages-blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-mdx","path":"/blog/tts/solving-attention-problems-of-tts-models-with-double-decoder-consistency","result":{"data":{"mdx":{"id":"c2fe8d63-260b-5746-b31e-dc62396baa51","excerpt":"Despite the success of the latest attention based end2end text2speech (TTS)\nmodels, they suffer from attention alignment problems at…","body":"var _excluded = [\"components\"];\n\nvar _templateObject;\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\nfunction _taggedTemplateLiteral(strings, raw) { if (!raw) { raw = strings.slice(0); } return Object.freeze(Object.defineProperties(strings, { raw: { value: Object.freeze(raw) } })); }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar pageQuery = graphql(_templateObject || (_templateObject = _taggedTemplateLiteral([\"\\n  query($fileAbsolutePath: String) {\\n    ...SidebarPageFragment\\n  }\\n\"])));\nvar _frontmatter = {\n  \"title\": \"Double Decoder Consistency\",\n  \"name\": \"Eren Gölge\",\n  \"picture\": \"https://avatars.githubusercontent.com/u/1402048?s=460&v=4\",\n  \"date\": \"June 3, 2020\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar Link = makeShortcode(\"Link\");\nvar layoutProps = {\n  pageQuery: pageQuery,\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Despite the success of the latest attention based end2end text2speech (TTS)\\nmodels, they suffer from attention alignment problems at inference time. They\\noccur especially with long-text inputs or out-of-domain character sequences.\\nHere I\\u2019d like to propose a novel technique to fight against these alignment\\nproblems which I call Double Decoder Consistency (DDC) (with a limited\\ncreativity). DDC consists of two decoders that learn synchronously with\\ndifferent reduction factors. We use the level of consistency of these decoders\\nto attain better attention performance.\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"iframe\", {\n    width: \"560\",\n    height: \"315\",\n    src: \"https://www.youtube.com/embed/ADnBCz0Wd1U\",\n    frameBorder: \"0\",\n    allow: \"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\",\n    allowFullScreen: true\n  })), mdx(\"h3\", {\n    \"id\": \"end-to-end-tts-models-with-attention\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#end-to-end-tts-models-with-attention\",\n    \"aria-label\": \"end to end tts models with attention permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"End-to-End TTS Models with Attention\"), mdx(\"p\", null, \"Good examples of attention based TTS models are Tacotron and Tacotron2\\n[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.10135\"\n  }, \"1\"), \"][\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1712.05884\"\n  }, \"2\"), \"].\\nTacotron2 is also the main architecture used in this work. These models\\ncomprise a sequence-to-sequence architecture with an encoder, an\\nattention-module, a decoder and an additional stack of layers called Postnet.\\nThe encoder takes an input text and computes a hidden representation from which\\nthe decoder computes predictions of the target acoustic feature frames. A\\ncontext-based attention mechanism is used to align the input text with the\\npredictions. Finally, decoder predictions are passed over the Postnet which\\npredicts residual information to improve the reconstruction performance of the\\nmodel. In general, mel-spectrograms are used as acoustic features to represent\\naudio signals in a lower temporal resolution and perceptually meaningful way.\"), mdx(\"p\", null, \"Tacotron proposes to compute multiple non-overlapping output frames by the\\ndecoder. You are able to set the number of output frames per decoder step which\\nis called \\u201Creduction rate\\u201D (r). Larger the reduction rate, fewer the number of\\ndecoder steps required for the model to produce the same length output.\\nThereby, the model achieves faster training convergence and easier attention\\nalignment, as explained in [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.10135\"\n  }, \"1\"), \"]. However,\\nlarger r values also produce smoother output frames and therefore, reduce the\\nframe-level details.\"), mdx(\"p\", null, \"Although these models are used in TTS systems for more natural-sounding speech,\\nthey frequently suffer from attention alignment problems, especially at\\ninference time, because of out-of-domain words, long input texts, or\\nintricacies of the target language. One solution is to use larger r for a\\nbetter alignment however, as noted above, it reduces the quality of the\\npredicted frames. DDC tries to mitigate these attention problems by introducing\\na new architecture.\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"413px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"151.2%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAYAAAAsEj5rAAAACXBIWXMAAAsTAAALEwEAmpwYAAADIElEQVRIx6VWaU8qQRDc//97+GKMERBNQI6ATw4FEQQ5BVGOvftV9WPIiivic5LJLrs9NTXV1b1Y8sUIw1Anh+/7sl6vpVgsytnZmTw8PEgQBLu46LDkwDDBruvK4+OjDIdDabfb0uv1PsV8C2gCl8uljMdj6XQ6MhgMxHGcTyf4EeDLy4tcX19LKpWSZDKpTKPvf8yQV+p3d3enYK+vr7FARwOa8fz8LNVqVTzP+x0gGXW7Xc3sfD7XhER1PAowKniz2ZSrqyvJZDKSz+elUqnI+/v7/wOS1WQy0cQ8PT3JYrFQX/4IME5HepAa2rb9Ow3NdTqdqp5kF62S6FTA/YdxQa1WS05OTuT09FR1jAMz0zqmllkt5+fnkk6n5e3t7fCRA1gg2GzEh3n91Uo8TF5DPAv02VIgnCyQmCWqhvceNjCxvPeQKE5l6GFHf7OW/f1YHWwKIfRi4Gw2O6idB31DmN5SNttAZpALWQ3sKiw3JoLg/F2v179MmjJUQNy4oL0AU1YDQdhdVtiIg1dqSNuwYrjBPtMPgDZ0maOc2ij8RqOhi5hVtioupKlZKYlEQnK5nL6PJmbXM1EAAQEd7Djv92UCnzVwJC5iyzLlNRqN9LiFQkF7IgEpwf5wsSaA5hapBtjRg2GLWMSFLDEelaDUlZpSCtMUDDtuZKRxQEgBXWYPLKfYIQmvEZCJ4WIemWDMNpmZxeaYlGiOWAWENP8AAWTjB8cGRyErAm7gQyapUi5LHo3h8vJSMhcXks1mpYcTrAE+5CdhW9sOEklPK6ALuvp1A6MNAv6gCYwQMMbzKnStYVZubqRxfy/3SNgMCfDB0otUy05DZseBJfyt40MwdMBwgQ5NS9GwPmYDUqzGI60UYSyeOYjhUW1egUE/WyGSwbJj+fE+8Fz1EwRUG9h45wGkCb360NGhLKwKvF+j+a448c7f6hvfYM0xAEjb1LGgVqtJDlpmYasOrQMC0ViTLOtQ22KFMLu3t7fqT7auMpLE70o04x/64aHmSsuYT0CpVNINaGpj7NgGe+xnlLV86P23H6l9BmS7zybu78hf8VIRuiBoTcEAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/e4c7359592414d8f0b25f9c24e75284e/e1a93/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-model-overview.png\",\n    \"srcSet\": [\"/static/e4c7359592414d8f0b25f9c24e75284e/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-model-overview.png 250w\", \"/static/e4c7359592414d8f0b25f9c24e75284e/e1a93/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-model-overview.png 413w\"],\n    \"sizes\": \"(max-width: 413px) 100vw, 413px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"The bare-bone model used in this work is formalized as follows:\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"455px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"92.4%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB2ElEQVQ4y32U547DMAyD+/7Plh8FinTv3bRN9x46fMIx8OXSGjAcKxZNkUpK9jve77ev5/PZttutrddre71edjqdbLVa2X6/99hms8nW4/Foh8PhT34pD5imqXU6Het2uw6i/Ww2s16vZ8vl0vfMfr/v+0LAMMh4PB7OFFDAYJkkiTOjCp19Pp/fGWql1MVi4SVNJhMbjUbOiBig9/vdJVFOIcMQ8HK52HQ6dYYAUe54PPZnaSj9wtyPgJQMKEwol2QkYKIrhux2u0wCzn5lyCG0AwCD2u22z8FgYNVqNWPNO2nsgKF2oRasapv5fO7iw5Y1NE/VSM9SHkjjer26ATgbRZENh0NnKCZFef80JKA2CEde/E/DAXFRrVCr1axcLlur1XJ9EJ6hr4MJS9iTTFMzMQmDvGR0wQBYyFVWLhFbgG+3WxbnLHviEMI4dHRAbm02m1av101sYYdmJLKP49jdrFQq7iguNxoNZ4ammJaVLPe4Uf3EzRzU16CVBC7FeUqEDDnMQlNIQkdup89UIntYskpfnmHL16RyHTC0X30HELpQMjFYMXmGFXGM4lOEKec//hw0KBMHYYHG6ku0pkQq4VLA0DYzJf+nyTcsrOS29OZdWKZijB/1dGxxHQvMlAAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/62f5e5526dd116e8aa7c2ac1736457e6/d2e8e/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-1.png\",\n    \"srcSet\": [\"/static/62f5e5526dd116e8aa7c2ac1736457e6/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-1.png 250w\", \"/static/62f5e5526dd116e8aa7c2ac1736457e6/d2e8e/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-1.png 455w\"],\n    \"sizes\": \"(max-width: 455px) 100vw, 455px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"where y\", mdx(\"sub\", null, \"k\"), \" is a sequence of acoustic feature frames. x\", mdx(\"sub\", null, \"l\"), \" is\\na sequence of characters or phonemes, from which we compute sequence of encoder\\noutputs h\", mdx(\"sub\", null, \"l\"), \". r is the reduction factor which defines the number of\\noutput frames per decoder step. Attention alignments, query vector and encoder\\noutput at decoder step t are donated by a\", mdx(\"sub\", null, \"t\"), \", q\", mdx(\"sub\", null, \"t\"), \", and\\no\", mdx(\"sub\", null, \"t\"), \" respectively. Also, o\", mdx(\"sub\", null, \"t\"), \" defines a set of output frames\\nwhose size changed by r. Total number of decoder steps is donated by T.\"), mdx(\"p\", null, \"Note that teacher forcing is applied at training. Therefore, K=Tr at training\\ntime. However, the decoder is instructed to stop at inference by a separate\\nnetwork (Stopnet) which predicts a value in a range \", \"[0, 1]\", \". If its prediction\\nis larger than a defined threshold, the decoder stops inference.\"), mdx(\"h3\", {\n    \"id\": \"double-decoder-consistency\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#double-decoder-consistency\",\n    \"aria-label\": \"double decoder consistency permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Double Decoder Consistency\"), mdx(\"p\", null, \"DDC is based on two decoders working simultaneously with different reduction\\nfactors (r). One decoder (coarse) works with a large, and the other decoder\\n(fine) works with a small reduction factor.\"), mdx(\"p\", null, \"DDC is designed to settle the trade-off between the attention alignment and the\\npredicted frame quality tunned by the reduction factor. In general, standard\\nmodels have more robust attention performance with a larger r but due to the\\nsmoothing effect of multiple-frames prediction per iteration, final acoustic\\nfeatures are coarser compared to lower reduction factor models.\"), mdx(\"p\", null, \"DDC combines these two properties at training time as it uses the coarse\\ndecoder to guide the fine decoder to preserve the attention performance without\\na loss of precision in acoustic features. DDC achieves this by introducing an\\nadditional loss function comparing the attention vectors of these two decoders.\"), mdx(\"p\", null, \"For each training step, both decoders compute their relative attention vectors\\nand the outputs. Due to the differences in their respective r values, their\\nattention vectors are different lengths. The coarse decoder produces a shorter\\nvector compared to the fine decoder. In order to mitigate this, we interpolate\\nthe coarse attention vector to match the length of the fine attention vector.\\nAfter coercing them in the same length we use a loss function to penalize the\\ndifference in the alignments. This loss is able to synchronize two decoders\\nwith respect to their alignments.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"79.6%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACMUlEQVQ4y3VUiXaiQBD0/z9t80yyxvXEgyjigdwIDEdvVb81QeLOe/0YmJnqquoeBoLRtq30n8/m/XFf68agu3gfruvKarV6AMvzXMIwlDRNJY5jqev6aZJBn8XhcJDRaCRZlj0kIQgBgyCQ4/GogMYYqapKyrLU/Uz6AzCKIpnP53ogSRJltF6vZb/fi+d5Ot9utzpfLpdiWZaqmUwmcrlcvgHvT26eTqe6SDAyul6vGkxApvzGxI7jaAKukaV62DWUg0yGw6HK6w9KvN1uKpPATHo6nTTRg4ddliUOjcdj9aOC7KIoJEfQIwKRPdf+V/FBi0NNXQmf+CIGh8coSn7LJGYRICeFvBgyDdh9vL1JinmDfYZJslTqIv8CHTQG2ptaSrYFNtZgGEehJigAoAF5lMvBYpEpmdNjA+9q7PkCLPCR5m5QDBrNlngFiwgglMqDSeCrVN/3ZTabqb9N04ht21rEPAy6gInMsYkHWKnz+SxTtECOrCwATfcOjvx+f1cgAhJY/cS+5WIhVRJ/A7YA2W42skW2DABHAFjoqxBsU3iUQEENPymV7/bnp4aDC7BnoCsakGrvgIYv8CdAo3oEA4MRJLv7nRahovlxJBnCB/sDwNzdTjxYY3gFURRidIpiRAvDatNsgNho1gCyCkhMrIVWEqaxItoJGtgb/hlLBjse2kZf/vUSjV7Ak18vLyqbnja8Afde6wUJtEzU6eMfN4UNzFvQ69qnv6r+peDzL6oh0h8mn7MgAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/ef79d367f6ad0564ef7c89cf33a51067/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-ddc-overview.png\",\n    \"srcSet\": [\"/static/ef79d367f6ad0564ef7c89cf33a51067/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-ddc-overview.png 250w\", \"/static/ef79d367f6ad0564ef7c89cf33a51067/c6e3d/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-ddc-overview.png 500w\", \"/static/ef79d367f6ad0564ef7c89cf33a51067/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-ddc-overview.png 1000w\", \"/static/ef79d367f6ad0564ef7c89cf33a51067/f1720/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-ddc-overview.png 1024w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"The two decoders take the same input from the encoder. They also compute the\\noutputs in the same way except they use different reduction factors. The coarse\\ndecoder uses a larger reduction factor compared to the fine decoder. These two\\ndecoders are trained with separate loss functions comparing their respective\\noutputs with the real feature frames. The only interaction between these two\\ndecoders is the attention loss applied to compare their respective attention\\nalignments.\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"706px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"33.199999999999996%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAAA0UlEQVQoz32RiQqEMAxE+/9/WZB6X/U+s7xAiyxioMaE5s1ETVEU0ratdF0nzjl9H8dR0jSVLMukqirNTdNo33svxH3fev7DhAEul2WpwGVZpO97HSYDoz9Nk977BFprBZfrukqSJFLXtYIQeToEyBmGIQLfwvAAhpM8z9UNDgFRk1kfMYSv64ru3o4JStu26TAugAIBSEbsue5XmOe3CN+QtYHhlB4gtjjPU+Z51rWpMUGN2HEcei86ZBWcAcNR+CnUQBlCjLXp7fsehekDZv4HoP4cKys79uIAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/370c32ce06b13bf6e357b23241e24ab6/a1ee8/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-2.png\",\n    \"srcSet\": [\"/static/370c32ce06b13bf6e357b23241e24ab6/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-2.png 250w\", \"/static/370c32ce06b13bf6e357b23241e24ab6/c6e3d/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-2.png 500w\", \"/static/370c32ce06b13bf6e357b23241e24ab6/a1ee8/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-equation-2.png 706w\"],\n    \"sizes\": \"(max-width: 706px) 100vw, 706px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"h3\", {\n    \"id\": \"other-model-update\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#other-model-update\",\n    \"aria-label\": \"other model update permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Other Model Update\"), mdx(\"h4\", {\n    \"id\": \"batch-norm-prenet\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#batch-norm-prenet\",\n    \"aria-label\": \"batch norm prenet permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Batch Norm Prenet\"), mdx(\"p\", null, \"The Prenet is an important part of Tacotron-like auto-regressive models. It\\nprojects model output frames before passing to the decoder. Essentially, it\\ncomputes an embedding space of the feature (spectrogram) frames by which the\\nmodel de-factors the distribution of upcoming frames.\"), mdx(\"p\", null, \"I replaced the original Prenet (PrenetDropout) with the one using Batch\\nNormalization [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1502.03167\"\n  }, \"3\"), \"] (PrenetBN) after each\\ndense layer, and I removed the Dropout layers. Dropout is necessary for\\nlearning attention, especially when the data quality is low. However, it causes\\nproblems at inference due to distributional differences between training and\\ninference time. Using Batch Normalization is a good alternative. It avoids the\\nissues of Dropout and also provides a certain level of regularization due to\\nthe noise of batch-level statistics. It also normalizes computed embedding\\nvectors and generates a well-shaped embedding space.\"), mdx(\"h4\", {\n    \"id\": \"gradual-training\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#gradual-training\",\n    \"aria-label\": \"gradual training permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Gradual Training\"), mdx(\"p\", null, \"I used a gradual training scheme for the model training. I\\u2019ve introduced\\ngradual training in a previous \", mdx(Link, {\n    to: \"/blog/tts/gradual-training-with-tacotron-for-faster-convergence\",\n    mdxType: \"Link\"\n  }, \"blog\\npost\"), \". In short, we start the model training with a larger reduction\\nfactor and gradually reduce it as the model saturates.\"), mdx(\"p\", null, \"Gradual Training shortens the total training time significantly and yields\\nbetter attention performance due to its progression from coarse to fine\\ninformation levels.\"), mdx(\"h4\", {\n    \"id\": \"recurrent-postnet-at-inference\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#recurrent-postnet-at-inference\",\n    \"aria-label\": \"recurrent postnet at inference permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Recurrent PostNet at Inference\"), mdx(\"p\", null, \"The Postnet is the part of the network applied after the Decoder to improve the\\nDecoder predictions before the vocoder. Its output is summed with the Decoder\\u2019s\\nto form the final output of the model. Therefore, it predicts a residual which\\nimproves the Decoder output. So we can also apply Postnet more than one time,\\nassuming it computes useful residual information each time. I applied this\\ntrick only at inference and observe that, up to a certain number of iterations,\\nit improves the performance. For my experiments, I set the number of iterations\\nto 2.\"), mdx(\"h4\", {\n    \"id\": \"mb-melgan-vocoder-with-multiple-random-window-discriminator\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#mb-melgan-vocoder-with-multiple-random-window-discriminator\",\n    \"aria-label\": \"mb melgan vocoder with multiple random window discriminator permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"MB-Melgan Vocoder with Multiple Random Window Discriminator\"), mdx(\"p\", null, \"As a vocoder, I use Multi-Band Melgan [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/2005.05106\"\n  }, \"11\"), \"]\\ngenerator. It is trained with Multiple Random Window Discriminator\\n(RWD)[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1909.11646\"\n  }, \"13\"), \"] which is different than in the\\noriginal work [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/2005.05106\"\n  }, \"11\"), \"] where they used\\nMulti-Scale Melgan Discriminator (MSMD)[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1909.11646\"\n  }, \"12\"), \"].\"), mdx(\"p\", null, \"The main difference between these two is that RWD uses audio level information\\nand MSMD uses spectrogram level information. More specifically, RWD comprises\\nmultiple convolutional networks each takes different length audio segments with\\ndifferent sampling rates and performs classification whereas MSMD uses\\nconvolutional networks to perform the same classification on STFT output of the\\ntarget voice signal.\"), mdx(\"p\", null, \"In my experiments, I observed RWD yields better results with more natural and\\nless abberated voice.\"), mdx(\"h3\", {\n    \"id\": \"related-work\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#related-work\",\n    \"aria-label\": \"related work permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Related Work\"), mdx(\"p\", null, \"Guided attention [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1710.08969\"\n  }, \"4\"), \"] uses a soft diagonal\\nmask to force the attention alignment to be diagonal. As we do, it uses this\\nconstant mask at training time to penalize the model with an additional loss\\nterm. However, due to its constant nature, it dictates a constant prior to the\\nmodel which does not always to be true, especially long sentences with various\\npauses. It also causes skipping in my experiments which are tried to be solved\\nby using a windowing approach at inference time in their work.\"), mdx(\"p\", null, \"Using multiple decoders is initially introduced by\\n[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1907.09006\"\n  }, \"5\"), \"]. They use two decoders that run in\\nforward and backward directions through the encoder output. The main problem\\nwith this approach is that because of the use of two decoders with identical\\nreduction factors, it is almost 2 times slower to train compared to a vanilla\\nmodel. We solve the problem by using the second decoder with a higher reduction\\nrate. It accelerates the training significantly and also gives the user the\\nopportunity to choose between the two decoders depending on run-time\\nrequirements. DDC also does not use any complex scheduling or multiple loss\\nsignals that aggravates the model training.\"), mdx(\"p\", null, \"Lately, new TTS models introduced by\\n[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1905.0926\"\n  }, \"7\"), \"][\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/2005.11129\"\n  }, \"8\"), \"][\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/2006.04558\"\n  }, \"9\"), \"][\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://doi.org/10.1109/icassp40776.2020.9054484\"\n  }, \"10\"), \"]\\npredict output duration directly from the input characters. These models train\\na duration-predictor or use approximation algorithms to find the duration of\\neach input character. However, as you listen to their samples, one can observe\\nthat these models lead to degraded timbre and naturalness. This is because of\\nthe indirect hard alignment produced by these models. However, models with\\nsoft-attention modules can adaptively emphasize different parts of the speech\\nproducing a more natural speech.\"), mdx(\"h3\", {\n    \"id\": \"results-and-experiments\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#results-and-experiments\",\n    \"aria-label\": \"results and experiments permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Results and Experiments\"), mdx(\"h4\", {\n    \"id\": \"experiment-setup\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#experiment-setup\",\n    \"aria-label\": \"experiment setup permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Experiment Setup\"), mdx(\"p\", null, \"All the experiments are performed using LJspeech dataset\\n[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://keithito.com/LJ-Speech-Dataset/\"\n  }, \"6\"), \"] . I use a sampling-rate of 22050\\nHz and mel-scale spectrograms as the acoustic features. Mel-spectrograms are\\ncomputed with hop-length 256, window-length 1024. Mel-spectrograms are\\nnormalized into \", \"[-4, 4]\", \". You can see the used audio parameters below in our TTS\\nconfig format.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"// AUDIO PARAMETERS\\n    \\\"audio\\\":{\\n        // stft parameters\\n        \\\"num_freq\\\": 513,         // number of stft frequency levels. Size of the linear spectogram frame.\\n        \\\"win_length\\\": 1024,      // stft window length in ms.\\n        \\\"hop_length\\\": 256,       // stft window hop-lengh in ms.\\n        \\\"frame_length_ms\\\": null, // stft window length in ms.If null, 'win_length' is used.\\n        \\\"frame_shift_ms\\\": null,  // stft window hop-lengh in ms. If null, 'hop_length' is used.\\n\\n        // Audio processing parameters\\n        \\\"sample_rate\\\": 22050,   // DATASET-RELATED: wav sample-rate. If different than the original data,\\n                                //   it is resampled.\\n        \\\"preemphasis\\\": 0.0,     // pre-emphasis to reduce spec noise and make it more structured.\\n                                //  If 0.0, no -pre-emphasis.\\n        \\\"ref_level_db\\\": 20,     // reference level db, theoretically 20db is the sound of air.\\n\\n        // Silence trimming\\n        \\\"do_trim_silence\\\": true,// enable trimming of slience of audio as you load it. LJspeech (false),\\n                                //  TWEB (false), Nancy (true)\\n        \\\"trim_db\\\": 60,          // threshold for timming silence. Set this according to your dataset.\\n\\n        // MelSpectrogram parameters\\n        \\\"num_mels\\\": 80,         // size of the mel spec frame.\\n        \\\"mel_fmin\\\": 0.0,        // minimum freq level for mel-spec. ~50 for male and ~95 for female voices.\\n                                //   Tune for dataset!!\\n        \\\"mel_fmax\\\": 8000.0,     // maximum freq level for mel-spec. Tune for dataset!!\\n\\n        // Normalization parameters\\n        \\\"signal_norm\\\": true,    // normalize spec values. Mean-Var normalization if 'stats_path' is defined otherwise\\n                                //   range normalization defined by the other params.\\n        \\\"min_level_db\\\": -100,   // lower bound for normalization\\n        \\\"symmetric_norm\\\": true, // move normalization to range [-1, 1]\\n        \\\"max_norm\\\": 4.0,        // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\\n        \\\"clip_norm\\\": true,      // clip normalized values into the range.\\n    },\\n\")), mdx(\"p\", null, \"I used Tacotron2[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1712.05884\"\n  }, \"2\"), \"] as the base architecture\\nwith location-sensitive attention and applied all the model updates expressed\\nabove. The model is trained for 330k iterations and it took 5 days with a\\nsingle GPU although the model seems to produce satisfying quality after only 2\\ndays of training with DDC. I used a gradual training schedule shown below. The\\nmodel starts with r=7 and batch-size 64 and gradually reduces to r=1 and\\nbatch-size 32. The coarse decoder is set r=7 for the whole training.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"{\\n\\\"gradual_training\\\": [[0, 7, 64], [1, 5, 64], [50000, 3, 32], [130000, 2, 32], [290000, 1, 32]], // [first_step, r, batch_size]\\n}\\n\")), mdx(\"p\", null, \"I trained MB-Melgan vocoder using real spectrograms up to 1.5M steps, which\\ntook 10 days on a single GPU machine. For the first 600K iterations, it is\\npre-trained with only the supervised loss as in\\n[\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/2005.05106\"\n  }, \"11\"), \"] and than the discriminator is enabled\\nfor the rest of the training. I do not apply any learning rate schedule and I\\nused 1e-4 for the whole training.\"), mdx(\"h4\", {\n    \"id\": \"ddc-attention-performance\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#ddc-attention-performance\",\n    \"aria-label\": \"ddc attention performance permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"DDC Attention Performance\"), mdx(\"p\", null, \"The image below shows the validation alignments of the fine and the coarse\\ndecoders which have r=1 and r=7 respectively. We observe that two decoders show\\nalmost identical attention alignments with a slight roughness with the coarse\\ndecoder due to the interpolation.\"), mdx(\"p\", null, \"DDC significantly shortens the time required to learn the attention alignmet.\\nIn my experiments, the model is able to align just after 1k steps as opposed to\\n~8k steps with normal location-sensitive attention.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"30%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABmUlEQVQY002QT0hUURSHj5iUoq1a5R/CbYERVMZYOjrPN+orx9LnjEENNKWCRUWmFYajjrQR0k0rIdCFTLmQoTZBhDWWtBhdFDhkugwhce+MX+e9EfLCj7s4H9/9nSvzUwmaioKEKrqxSyPYxyN0lt+mrfQWg9YTtneWcc676Q9YR68TrLjznytTrjzC3dpH/N35plQGmRmNc0Z8+PLa8UpAcxXvERuPXOHGqT5Wt1Zd4duXCc5KIw1yTZm2HHfYpkZascsipP6kyLrCsTjnxY95yMZXoCkMYuhdp+DN0/dZ3PyVE04muCDNmPkO14GvqHOfCxCq7OHTRppdVcps7A3VKvTrOkZxCCO/A1NBp2246iHJn79dofM1HmnBXxLKcVrAKVGvbbsqe/n8Y53dvT3kdXSOk9LAJR14xHJTI5c5JybBE718XUq7wvjEAlVicFEfOsg52wWOhVlKrpHJasOP80nueYd4Zo0z0DTCY3+Up1aM/sZhJrpfkV7ZdIVf3n/ngfFcuZjL9ZvD+1yUF+Ep1lIbZDNZ/gG2qRGhZ08LsQAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/b2806805fe690ce3e06f39c3cb3e635e/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignment.png\",\n    \"srcSet\": [\"/static/b2806805fe690ce3e06f39c3cb3e635e/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignment.png 250w\", \"/static/b2806805fe690ce3e06f39c3cb3e635e/c6e3d/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignment.png 500w\", \"/static/b2806805fe690ce3e06f39c3cb3e635e/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignment.png 1000w\", \"/static/b2806805fe690ce3e06f39c3cb3e635e/99b7b/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignment.png 1360w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"At inference time, we ignore the coarse decoder and use only the fine decoder.\\nThe image below depicts the model outputs and attention alignments at inference\\ntime with 4 different sentences that are not seen at training time. This shows\\nus that the fine decoder is able to generalize successfully on novel sentences.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"39.6%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAACHklEQVQoz1WR60uTcRTHHwiiP6BQgiRp2gVasm6GYAyE6VpO5mWWkruY6HTpKtGFecm5WjqjbC1TNIrqXS/7h4Rmbu6ic27P8+z59Nt8Uy++HM73fOCcw1caNDzFKNkwHuvEddPHdj5Fx/QGdT0B6u4v0D4ZIavs4zVMcPt4O8ZKBz1XR4jnUtiWv3LRE+KSb4luR5h4Mo7kuODlhmSmXshe62FrP4N5YZXavll07jlM4+9JHxwwcMXH9RNWwdmwnnYTywju3TpnvXNUPQtievSG2M4u0sy9JXrPD/NA58FvCbKT3MPz8zOWyGvMHxbpX90gmzpkfixCr36EvnPDPDHNkdzdw/drk5a1EM2bYZxf1olvZ5CKahFVVlFVIUVF0zTkolJWQTuqJU8tcQXlP04V84J6xMiK4IoakiIguSBTOJRRxBCKKHkFpdRnZYp5WXga/3JqmSt5atmTS4sUwYklkt8cwF7ppqt6iIneF6TzCQbCP7A2+TE9XMQ7ExUXZJi6G6KzwoVdJ17umiIjpxgKf8PSNk2TfwXPTIS9gwRSnwjlmmQVwbRirxliK5em5dUn9LfGqB6cpXl0hXQuS/9lHwbJIkJppbWqn5gIzzq9ht7wmBpXgDvOZWKJJNJ4R5C2M25sJx2MNj7nTzqN83uURm+QxuV57G+j7Kdy+LtDWCscdJxyMlg/yW4yg2tznQbXSxrCQTqDH9n5neIvQKfD2Urvl0EAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/09e724b596058252333f24f77120914e/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png\",\n    \"srcSet\": [\"/static/09e724b596058252333f24f77120914e/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png 250w\", \"/static/09e724b596058252333f24f77120914e/c6e3d/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png 500w\", \"/static/09e724b596058252333f24f77120914e/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png 1000w\", \"/static/09e724b596058252333f24f77120914e/2e9ed/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png 1500w\", \"/static/09e724b596058252333f24f77120914e/9fabd/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png 2000w\", \"/static/09e724b596058252333f24f77120914e/7b24f/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-alignments.png 2048w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"I used 50 hard-sentences introduced by [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://arxiv.org/abs/1905.09263\"\n  }, \"7\"), \"]\\nto check the attention quality of the DDC model. As you see in the\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://colab.research.google.com/gist/erogol/32d22e21eaa1d0cc0cb52f0fd0c72c55/ddc_sentece_test_330k.ipynb\"\n  }, \"notebook\"), \"\\n(Open it on Colab to listen to Griffin-Lim based voice samples), the DDC model\\nperforms without any alignment problems. It is the first model, to my\\nknowledge, which performs flawlessly on these sentences.\"), mdx(\"h4\", {\n    \"id\": \"recurrent-postnet\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#recurrent-postnet\",\n    \"aria-label\": \"recurrent postnet permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Recurrent Postnet\"), mdx(\"p\", null, \"In the image below we see the average L1 difference between the real\\nmel-spectrogram and the model prediction for each Postnet iteration. The\\nresults improve until the 3rd iteration. We also observe that some of the\\nartifacts after the first iteration are removed by the second iteration that\\nyields a better L1 value. Therefore, we see here how effective the iterative\\napplication of the Posnet to improve the final model predictions.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"70%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADiUlEQVQ4yyVT209TBxzu25K5l6k4NAg4vA2VobTFwjmnt9PTlspkm8JkoAXaQlvOaemNUi7uYYBSkIK36LIs0QcTl/mibFnM4mXZYmJcZEOjJsseZvawPZBsf8C375w+fPmd3zm/3/e7fcfUsTsF9+ZBHNmRgL9Ghb9WRVu1ikCtBrkyioh7Bq/X72MiXIKzku/3pOHfOQI/83Qb2JWCXD2MobYF/PH3CkxKZQzNb5yCs2IIDhI7KohNg3BuDMO2oQ+9zVN4/s+PGOkuwrohBMd2FfZtcTiqhuHYSjD/8NthBO0zePLX9zC1s5KdZP6aBLwM9tZo8FZr8BHOrXGEnJ/j/us1pPovkEiFb08Gys4UFOYpu9LwEfYdSXZYxJ3fn8AU2J2GtHkI3toEPLVJIgGlSoVCa982jAHnNO6+fIFE3yVIVRqUvVnIOkjsIbGHz2LdCAYDRdxaXWWH/GCviMJflypXZjUv4aPvIGHINY17T18iFbwEOwt594/Co0Pv9L0svPW5coft87j9868wyQyyvtln7ELSsSUKiQXstIffGkCv7TQePHiG+LESrBsjkDiFyPWIXI+4XYNEa+WEQXkGKyscOXNsEVHvLEY65qG1z0H9YA5a4CyS9OO+WUzHvsRvj15hefImou1FJI+XoH10DqoOxiSYH+P7M+lrePzDGkyjA1egdi4h03MBia4S1K5F+otI91yE1lnCbO4G/v3vMa6WvkH8xEVkOLr26XkMdy9B7V5GOnjZsGfGv8b6+kOY3NybmS2LdRyFhxDeLY8k8cqWd6Lo5lGeUTZa3yKatsQhUIcCc1p5ZYF7Frk/c1Ucvb5iWTYBywREilNpLBjLlt8nGvJQCGlfFv1HF3DvzzUkY1cg8qIe8wTkxjHIB8fgJjwHCxCYE+lcxm1dNr7mSbSwqnyoAJcOPfBA3ghubcgh2LFgyEaNXUVLfRbu5gk4rYR53Ih1WcZhO5RHqGupLJs2fhSoRQ+JXXw2KjcVIDNBPJBD/4cLhmwSsS/QSt8tfQanMGXEumyTcNum0NI0hvCJ5bJs3I15WPjfSg2jEPbnINRniCwkwsq99hyZw0PKJjZwGU17uTdLgQR5tLIrnUigNe/L4CSP+d23v8AU+2QJp9rOYvDjc4iwGwNH5xGhHyTZpPoVnv70AsWpmzhJmUSohBBVEKYCwsdp9XzmnM5dx6O7q/gfk0JYhvkA5uEAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/e4a0e3fe77a7f947321db273f2d8b2ea/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png\",\n    \"srcSet\": [\"/static/e4a0e3fe77a7f947321db273f2d8b2ea/43fa5/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png 250w\", \"/static/e4a0e3fe77a7f947321db273f2d8b2ea/c6e3d/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png 500w\", \"/static/e4a0e3fe77a7f947321db273f2d8b2ea/da8b6/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png 1000w\", \"/static/e4a0e3fe77a7f947321db273f2d8b2ea/2e9ed/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png 1500w\", \"/static/e4a0e3fe77a7f947321db273f2d8b2ea/9fabd/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png 2000w\", \"/static/e4a0e3fe77a7f947321db273f2d8b2ea/0404f/blog-tts-solving-attention-problems-of-tts-models-with-double-decoder-consistency-recurrent-postnet.png 2044w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"h3\", {\n    \"id\": \"future-work\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#future-work\",\n    \"aria-label\": \"future work permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Future Work\"), mdx(\"p\", null, \"First of all I hope this section would not be \\u201Chere are the things we\\u2019ve not\\ntried and will not try\\u201D section.\"), mdx(\"p\", null, \"However, there are specifically three aspects of DDC which I like to\\ninvestigate more. The first is sharing the weights between the fine and the\\ncoarse decoders to reduce the total number of model parameters and observing\\nhow the shared weights benefit from different resolutions.\"), mdx(\"p\", null, \"The second is to measure the level of complexity required by the coarse\\ndecoder. That is, how much simpler the coarse architecture can get without\\nperformance loss.\"), mdx(\"p\", null, \"Finally, I like to try DDC with the different model architectures.\"), mdx(\"h3\", {\n    \"id\": \"conclusion\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#conclusion\",\n    \"aria-label\": \"conclusion permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Conclusion\"), mdx(\"p\", null, \"Here I tried to summarize a new method that significantly accelerates model\\ntraining, provides steadfast attention alignment and provides a choice in a\\nspectrum of quality and speed switching between the fine and the coarse\\ndecoders at inference. The user can choose depending on run-time requirements.\"), mdx(\"p\", null, \"You can replicate all this work using our\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/TTS\"\n  }, \"TTS\"), \". You can also see voice samples and\\nColab Notebooks from the links above. Let me know how it goes if you try DDC in\\nyour project.\"), mdx(\"p\", null, \"If you would like to cite this work, please use:\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"G\\xF6lge E. (2020) Solving Attention Problems of TTS models with Double Decoder Consistency.\\nerogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/\")));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#end-to-end-tts-models-with-attention","title":"End-to-End TTS Models with Attention"},{"url":"#double-decoder-consistency","title":"Double Decoder Consistency"},{"url":"#other-model-update","title":"Other Model Update"},{"url":"#related-work","title":"Related Work"},{"url":"#results-and-experiments","title":"Results and Experiments"},{"url":"#future-work","title":"Future Work"},{"url":"#conclusion","title":"Conclusion"}]}}},"pageContext":{"frontmatter":{"title":"Double Decoder Consistency","name":"Eren Gölge","picture":"https://avatars.githubusercontent.com/u/1402048?s=460&v=4","date":"June 3, 2020"},"fileAbsolutePath":"/home/runner/work/coqui-ai.github.io/coqui-ai.github.io/src/pages/blog/tts/solving-attention-problems-of-tts-models-with-double-decoder-consistency.mdx"}},"staticQueryHashes":["1942088059","3709355695","932324783"]}