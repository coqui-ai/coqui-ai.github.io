{"componentChunkName":"component---src-pages-blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-mdx","path":"/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages","result":{"data":{"mdx":{"id":"30a1ab04-34fa-5383-88d9-128c0f907755","excerpt":"ðŸ‘‰  Try the YourTTS demo ðŸ‘‰ Visit the YourTTS  project page ðŸ‘‰ Try YourTTS on  Colab ðŸ‘‰ Try voice conversion with YourTTS on  Colab Theâ€¦","body":"var _excluded = [\"components\"];\n\nvar _templateObject;\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\nfunction _taggedTemplateLiteral(strings, raw) { if (!raw) { raw = strings.slice(0); } return Object.freeze(Object.defineProperties(strings, { raw: { value: Object.freeze(raw) } })); }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar pageQuery = graphql(_templateObject || (_templateObject = _taggedTemplateLiteral([\"\\n  query($fileAbsolutePath: String) {\\n    ...SidebarPageFragment\\n  }\\n\"])));\nvar _frontmatter = {\n  \"title\": \"YourTTS: Zero-Shot Multi-Speaker Text Synthesis and Voice Conversion\",\n  \"name\": \"Edresson Casanova, Eren GÃ¶lge, and Julian Weber\",\n  \"picture\": \"https://avatars.githubusercontent.com/u/75583352?s=96&v=4\",\n  \"date\": \"Jan 3, 2022\"\n};\nvar layoutProps = {\n  pageQuery: pageQuery,\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"\\uD83D\\uDC49 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/\"\n  }, \"Try the YourTTS demo\")), mdx(\"p\", null, \"\\uD83D\\uDC49 Visit the YourTTS \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://edresson.github.io/YourTTS/\"\n  }, \"project page\")), mdx(\"p\", null, \"\\uD83D\\uDC49 Try YourTTS on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://colab.research.google.com/drive/1ftI0x16iqKgiQFgTjTDgRpOM1wC1U-yS?usp=sharing\"\n  }, \"Colab\")), mdx(\"p\", null, \"\\uD83D\\uDC49 Try voice conversion with YourTTS on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://colab.research.google.com/drive/1gjdwOKCZuavPn_5oy8QA01sKmXpEq5AZ?usp=sharing\"\n  }, \"Colab\")), mdx(\"p\", null, \"The recent surge of new end-to-end deep learning models has enabled new and\\nexciting Text-to-Speech (TTS) use-cases with impressive natural-sounding\\nresults. However, most of these models are trained on massive datasets\\n(20-40 hours) recorded with a single speaker in a professional environment. In\\nthis setting, expanding your solution to multiple languages and speakers is not\\nfeasible for everyone. Moreover, it is particularly tough for low-resource\\nlanguages not commonly targeted by mainstream research. To get rid of these\\nlimitations and bring zero-shot TTS to low resource languages, we built\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2112.02418\"\n  }, \"YourTTS\"), \", which can synthesize voices in\\nmultiple languages and reduce data requirements significantly by transferring\\nknowledge among languages in the training set. For instance, we can easily\\nintroduce Brazilian Portuguese to the model with a single speaker dataset by\\nco-training with a larger English dataset. It makes the model speak Brazilian\\nPortuguese with voices from the English dataset, or we can even introduce new\\nspeakers by zero-shot learning on the fly.\"), mdx(\"p\", null, \"In \\u201C\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YourTTS\"), \": Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice\\nConversion for everyone\\u201D we introduce a model with the following capabilities:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Multi-Lingual TTS\"), \": Synthesizing speech in multiple languages with a single\\nmodel.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Multi-Speaker TTS\"), \": Synthesizing speech with different voices with a single\\nmodel.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Zero-Shot learning\"), \": Adapting the model to synthesize the speech of a novel\\nspeaker without re-training the model.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Speaker/language adaptation\"), \": Fine-tuning a pre-trained model to learn a\\nnew speaker or language. (Learn Turkish from a relatively smaller dataset\\nby transferring knowledge from learned languages)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Cross-language voice transfer\"), \": Transferring a voice from its original\\nlanguage to a different language. (Using the voice of an English speaker in\\nFrench)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Zero-shot voice conversion\"), \": Changing the voice of a given speech clip.\")), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"91.19999999999999%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/b89c253536048e68d39ccf79d68bea74/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-modes.png\",\n    \"srcSet\": [\"/static/b89c253536048e68d39ccf79d68bea74/43fa5/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-modes.png 250w\", \"/static/b89c253536048e68d39ccf79d68bea74/c6e3d/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-modes.png 500w\", \"/static/b89c253536048e68d39ccf79d68bea74/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-modes.png 1000w\", \"/static/b89c253536048e68d39ccf79d68bea74/a2880/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-modes.png 1312w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"h3\", {\n    \"id\": \"model-architecture\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#model-architecture\",\n    \"aria-label\": \"model architecture permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Model Architecture\"), mdx(\"p\", null, \"YourTTS is an extension of our previous work \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2104.05557\"\n  }, \"SC-GlowTTS\"), \".\\nIt uses the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2106.06103\"\n  }, \"VITS\"), \" (Variational Inference with\\nadversarial learning for end-to-end Text-to-Speech) model as the backbone architecture and\\nbuilds on top of it. We use a larger text encoder than the original model.\\nAlso, YourTTS employs a separately trained speaker encoder model to compute the\\nspeaker embedding vectors (d-vectors) to pass speaker information to the rest\\nof the model. We use the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2009.14153\"\n  }, \"H/ASP\"), \" model as the\\nspeaker encoder architecture. See the figure below for the overall model\\narchitecture in training (right) and inference (left).\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"62.4%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/aeec1cf73c251540bf76af923d1737af/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-inference.png\",\n    \"srcSet\": [\"/static/aeec1cf73c251540bf76af923d1737af/43fa5/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-inference.png 250w\", \"/static/aeec1cf73c251540bf76af923d1737af/c6e3d/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-inference.png 500w\", \"/static/aeec1cf73c251540bf76af923d1737af/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-inference.png 1000w\", \"/static/aeec1cf73c251540bf76af923d1737af/2e9ed/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-inference.png 1500w\", \"/static/aeec1cf73c251540bf76af923d1737af/39a40/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-inference.png 1874w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"VITS is a peculiar TTS model as it employs different deep-learning techniques\\ntogether (adversarial learning, normalizing flows, variational auto-encoders,\\ntransformers) to achieve high-quality natural-sounding output. It is mainly\\nbuilt on the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2005.11129\"\n  }, \"GlowTTS\"), \" model. The GlowTTS\\nis light, robust to long sentences, converges rapidly, and is backed up by\\ntheory since it directly maximizes the log-likelihood of speech with the\\nalignment. However, its biggest weakness is the lack of naturalness and\\nexpressivity of the output.\"), mdx(\"p\", null, \"VITS improves on it by introducing specific updates. First, it replaces the\\nduration predictor with a stochastic duration predictor that better models the\\nvariability in speech. Then, it connects a HifiGAN vocoder to the decoder\\u2019s\\noutput and joins the two with a variational autoencoder (VAE). That allows the\\nmodel to train in an end2end fashion and find a better intermediate\\nrepresentation than traditionally used mel-spectrograms. This results in high\\nfidelity and more precise prosody, achieving better MOS values reported in the\\npaper.\"), mdx(\"p\", null, \"Note that both GlowTTS and VITS implementations are available on \\uD83D\\uDC38TTS.\"), mdx(\"h3\", {\n    \"id\": \"dataset\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#dataset\",\n    \"aria-label\": \"dataset permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Dataset\"), mdx(\"p\", null, \"We combined multiple datasets for different languages. We used \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://datashare.ed.ac.uk/handle/10283/2651\"\n  }, \"VCTK\"), \"\\nand \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1904.02882\"\n  }, \"LibriTTS\"), \" for English (multispeaker datasets),\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2005.05144\"\n  }, \"TTS-Portuguese Corpus\"), \" (TPC) for Brazilian\\nPortuguese, and the French subset of the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/\"\n  }, \"M-AILABS\"), \"\\ndataset (FMAI).\"), mdx(\"p\", null, \"We resample the audio clips to 16 kHz, apply voice activity detection to remove\\nsilences and apply \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/TTS/blob/aa2450e8f28b4367791a7988cf87e395e501ce67/TTS/utils/audio.py#L787\"\n  }, \"RMS volume normalization\"), \"\\nbefore passing them to the speaker encoder.\"), mdx(\"h3\", {\n    \"id\": \"training\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#training\",\n    \"aria-label\": \"training permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Training\"), mdx(\"p\", null, \"We train YourTTS incrementally, starting from a single speaker English dataset\\nand adding more speakers and languages along the way. We start from a\\npre-trained model on the LJSpeech dataset for 1M steps and continue with the\\nVCTK dataset for 200K steps. Next, we randomly initialize the new layers\\nintroduced by the YourTTS model on the VITS model. Then we add the other\\ndatasets one by one and train for \", \"~\", \"120K steps with each new dataset.\"), mdx(\"p\", null, \"Before we report results on each dataset, we also fine-tune the final model with\\nspeaker encoder loss (SCL) on that particular dataset. SCL compares output\\nspeech embeddings with the ground truth embeddings computed by the speaker\\nencoder with cosine similarity loss.\"), mdx(\"p\", null, \"We used a single V100 GPU and used a batch size of 64. We used the AdamW\\noptimizer with beta values 0.8 and 0.99 and a learning rate of 0.0002 decaying\\nexponentially with gamma 0.999875 per iteration. We also employed a weight\\ndecay of 0.01.\"), mdx(\"h3\", {\n    \"id\": \"results\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#results\",\n    \"aria-label\": \"results permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Results\"), mdx(\"p\", null, \"We compute \\u201Cmean opinion score\\u201D (MOS) tests and similarity MOS tests to evaluate the model\\nperformance. We also use \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2104.05557\"\n  }, \"speaker encoder cosine similarity (SECS)\"), \" to\\nmeasure the similarity between the predicted outputs and the actual audio clips\\nof a target speaker. We used a 3rd party library for SECS to be compatible with\\nthe previous work. We avoid details of our experiments for the sake of brevity.\\nPlease refer to the paper to see the details.\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"27.200000000000003%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA1UlEQVQY0z1QR6rFMBDL/c8V3iKN9N57gWSRA+gjQf7CyIzVPEZd11iWBcdxIMsyoeu6mgVBgKqqUJYlhmGAZVkoigK+7yNNUzRNgyiKEIahtOQbJP1+PxE9z5PYtm0wiG9JkuA8TwVt2yb8Cuz7jvu+8TyPONd1waCASeu6yoB30zTViMYMattWAWzOxmxIAxp9hsT3fWHkeS4xE1n5M5rnWcj3rusU9K2AyBDO+XVy4jhWqMG/s8U4jtpD3/cSTNP0vwLOSeauKHYcR8jDHZJPY3L/APdXZAuNFp69AAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/0bdf0c67d1d931092fb99c86800026c8/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png\",\n    \"srcSet\": [\"/static/0bdf0c67d1d931092fb99c86800026c8/43fa5/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png 250w\", \"/static/0bdf0c67d1d931092fb99c86800026c8/c6e3d/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png 500w\", \"/static/0bdf0c67d1d931092fb99c86800026c8/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png 1000w\", \"/static/0bdf0c67d1d931092fb99c86800026c8/2e9ed/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png 1500w\", \"/static/0bdf0c67d1d931092fb99c86800026c8/9fabd/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png 2000w\", \"/static/0bdf0c67d1d931092fb99c86800026c8/040e1/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-1.png 2458w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"Table (1) above shows our results on different datasets. Exp1 is trained with\\nonly the VCTK. Exp2. is with the VCTK and TPC. Then, we add the FMAI, LibriTTS\\nfor Exp3. and Exp4, respectively. The ground truth row reports the values for\\nthe real speaker clips in respective datasets. Finally, we compare our results\\nwith \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2005.08484\"\n  }, \"AttentronZS\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2104.05557\"\n  }, \"SC-GlowTTS\"), \".\\nNote that SC-GlowTTS is our previous work which leads the way to YourTTS. You\\ncan find its implementation under \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/TTS\"\n  }, \"\\uD83D\\uDC38TTS\"), \". We\\nachieve significantly better results than the works compared to in our\\nexperiments. MOS values are on-par or better than the ground truth in some\\ncases, which is surprising even for us to see.\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"11.6%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAWElEQVQI1z2O1w0AIQxD2X8/aiiCDOLTQ+I+IsclVsJaS3tvmZlaa8o565xzd7QxhsiAKaU/23tXjFHurlrr1fADBLOUojnnLYKDcIoIv6H8edzCeYIc+AEw0o7A7mPYtwAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/1a86528f25d208b19a99b3914ccc4ca0/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png\",\n    \"srcSet\": [\"/static/1a86528f25d208b19a99b3914ccc4ca0/43fa5/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png 250w\", \"/static/1a86528f25d208b19a99b3914ccc4ca0/c6e3d/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png 500w\", \"/static/1a86528f25d208b19a99b3914ccc4ca0/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png 1000w\", \"/static/1a86528f25d208b19a99b3914ccc4ca0/2e9ed/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png 1500w\", \"/static/1a86528f25d208b19a99b3914ccc4ca0/9fabd/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png 2000w\", \"/static/1a86528f25d208b19a99b3914ccc4ca0/62145/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-2.png 2456w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"Table (2) depicts the zero-shot voice conversion (ZSVC) results between\\nlanguages and genders by the speaker embeddings. For ZSVC, we pass the given\\nspeech clip from the posterior encoder to compute the hidden representation and\\nre-run the model in the inference mode again conditioned on the target\\nspeaker\\u2019s embedding. You see in the table the model\\u2019s performance between\\nlanguages and genders. For instance, \\u201Cen-pt\\u201D shows the results for converting\\nthe voice of a Portuguese speaker by conditioning on an English speaker.\\nAnd \\u201CM-F\\u201D offers the conversion of a Male speaker to a Female speaker.\"), mdx(\"div\", {\n    align: \"center\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"36%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABL0lEQVQoz1WRW6uCUBSE/f9/xycJIsKgJM3shqlZPlgJXbW8pMxhVigdYVjbkT17vq3yeDwQxzHm8zmOxyO22y2WyyVWq5XI8zw4joMgCGQuFgtMp1PsdjvYtg3TNOG6LlRVFV95vV4oigKceZ6jfd7vNy6XC+q67rz9fi/ixs1mg/V63QVqmvYNvF6veD6fuN1uuN/vyLJMRD9JEvnGdx7I9tR4PBYKhrXhvV4PhmFAITKD2smNFNudTicJbD3i+74vgcS1LAuj0Uia9vt96Lr+RSYqN3I2TSPimgcQmaJ3OBwQRZE0Y/hsNpPgMAwxHA7lPwgy23ES+xf5fD4jTdN/yAxiQ97bZDIRcU1kNlbKspQ2bdPP5yON2JiBVVV1Hv90i8w75GQgvcFgIM3/AKoi/q8AUjd8AAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/1cbc7cb409021c47eb620e97b3f6b898/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-3.png\",\n    \"srcSet\": [\"/static/1cbc7cb409021c47eb620e97b3f6b898/43fa5/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-3.png 250w\", \"/static/1cbc7cb409021c47eb620e97b3f6b898/c6e3d/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-3.png 500w\", \"/static/1cbc7cb409021c47eb620e97b3f6b898/da8b6/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-3.png 1000w\", \"/static/1cbc7cb409021c47eb620e97b3f6b898/2e9ed/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-3.png 1500w\", \"/static/1cbc7cb409021c47eb620e97b3f6b898/30dc5/blog-tts-yourtts-zero-shot-text-synthesis-low-resource-languages-table-3.png 1962w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"Table (3) yields the results for the speaker adaptation experiments where we\\nfine-tune the final YourTTS model by SCL on different length clips of a\\nparticular novel speaker. For instance, the top row shows the results for a\\nmodel trained on a male English speaker with 61 seconds of an audio clip. GT is\\nthe ground truth, ZS is zero-shot with only the speaker embeddings, and FT is\\nfine-tuning. These results show that our model can achieve high similarity\\nwhen fine-tuned with only 20 seconds of audio sample from a speaker in case\\nmere use of speaker embeddings is not enough to produce high-quality results.\"), mdx(\"p\", null, \"Due to the time and space constraints in the paper, we could not expand the\\nexperiments to all the possible use-cases of YourTTS. We plan to include those\\nin our future study and add new capabilities to YourTTS that would give more\\ncontrol over the model.\"), mdx(\"h3\", {\n    \"id\": \"try-out-yourtts\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#try-out-yourtts\",\n    \"aria-label\": \"try out yourtts permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Try out YourTTS\"), mdx(\"p\", null, \"Give YourTTS a try right on your browser using the demo on our \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/\"\n  }, \"homepage\"), \".\"), mdx(\"p\", null, \"YourTTS is also available in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/TTS\"\n  }, \"\\uD83D\\uDC38TTS\"), \" with a\\ntraining recipe and a pre-trained model. You can train your own model,\\nsynthesize voice with the pre-trained model or finetune it with your dataset.\"), mdx(\"h3\", {\n    \"id\": \"ethical-concerns\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#ethical-concerns\",\n    \"aria-label\": \"ethical concerns permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Ethical Concerns\"), mdx(\"p\", null, \"We are well aware that the expansion of the TTS technology enables various kinds\\nof malign uses of the technology. Therefore, we also actively study different\\napproaches to prevent or at the very least put more fences along the way of the\\nmisuse of the TTS technology.\"), mdx(\"p\", null, \"To exemplify this, on our demo, we add background music to avert the unintended\\nuse of the voice clips on different platforms.\"), mdx(\"p\", null, \"If you also want to contribute to our research and discussion in this field, join\\nus \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/coqui-ai/TTS/discussions/1036\"\n  }, \"here\"), \".\"), mdx(\"h3\", {\n    \"id\": \"conclusion\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#conclusion\",\n    \"aria-label\": \"conclusion permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }, \"\\n  \", mdx(\"path\", {\n    parentName: \"svg\",\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  }))), \"Conclusion\"), mdx(\"p\", null, \"YourTTS can achieve competitive results on multi-lingual, multi-speaker TTS, and\\nzero-shot learning. It also allows cross-language voice transfer, learning new\\nspeakers and languages from relatively more minor datasets than the traditional\\nTTS models.\"), mdx(\"p\", null, \"We are excited to present YourTTS and see all the different use-cases the \\uD83D\\uDC38\\nCommunity will apply it to. As always, feel free to reach out for any feedback.\"));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#model-architecture","title":"Model Architecture"},{"url":"#dataset","title":"Dataset"},{"url":"#training","title":"Training"},{"url":"#results","title":"Results"},{"url":"#try-out-yourtts","title":"Try out YourTTS"},{"url":"#ethical-concerns","title":"Ethical Concerns"},{"url":"#conclusion","title":"Conclusion"}]}}},"pageContext":{"frontmatter":{"title":"YourTTS: Zero-Shot Multi-Speaker Text Synthesis and Voice Conversion","name":"Edresson Casanova, Eren GÃ¶lge, and Julian Weber","picture":"https://avatars.githubusercontent.com/u/75583352?s=96&v=4","date":"Jan 3, 2022"},"fileAbsolutePath":"/Users/kdavis/Code/coqui-ai/coqui-ai.github.io/src/pages/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages.mdx"}},"staticQueryHashes":["1942088059","3709355695","932324783"]}