{"componentChunkName":"component---src-pages-newsletter-05-04-2021-mdx","path":"/newsletter/05-04-2021","result":{"data":{"mdx":{"id":"35eb3e6a-9aba-5c9d-a182-4f2c23270716","excerpt":"Welcome Years ago now, I remember starting work on the speech recognition engine that would become the core of\n Coqui üê∏ STT . Then it was‚Ä¶","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\nfunction _templateObject() {\n  var data = _taggedTemplateLiteral([\"\\n  query($fileAbsolutePath: String) {\\n    ...SidebarPageFragment\\n  }\\n\"]);\n\n  _templateObject = function _templateObject() {\n    return data;\n  };\n\n  return data;\n}\n\nfunction _taggedTemplateLiteral(strings, raw) { if (!raw) { raw = strings.slice(0); } return Object.freeze(Object.defineProperties(strings, { raw: { value: Object.freeze(raw) } })); }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar pageQuery = graphql(_templateObject());\nvar _frontmatter = {\n  \"title\": \"Coqui's First Mondays Newsletter\",\n  \"date\": \"April 5, 2021\"\n};\nvar layoutProps = {\n  pageQuery: pageQuery,\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"934px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"27.599999999999998%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA3XAAAN1wFCKJt4AAABCElEQVQY042RQUsCcRDF/VB9g+59gk6eOhTRpaBbh5CgYCuiTkYGah6CSIwiyiRbiiyVFg+5EoS6LO1/XS0oXffXriC1atCDOcyDeW/mTQAXjuMwhFHcH/g9H/CaPqG/W5T0V1ShUTbqjDIb7L/abR/vE8y8KEjyMfv5NHuFS87kO0LhCBvxBOuxBPHTc0K7EVJZGV2YhI+SZAvF3qzd7f4I9h287WpNgfho8WzWWdjaZnFzh+DSMtMra0zMzTO7KjE2GewJj0/NcOKKe+jY9vCGWsvkvlom55ZqamhvBqnrG26fFDIPeYTVRIoecHiR5ir3iKJWMBqW/+TBbDynz46by/9/4sv1G2shujMHTg19AAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png\",\n    \"srcSet\": [\"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/43fa5/logo-wordmark.png 250w\", \"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/c6e3d/logo-wordmark.png 500w\", \"/static/c9103bc33c8add5ab1fa4fa1c49c90ef/ca463/logo-wordmark.png 934w\"],\n    \"sizes\": \"(max-width: 934px) 100vw, 934px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \")), mdx(\"h3\", {\n    \"id\": \"welcome\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#welcome\",\n    \"aria-label\": \"welcome permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }), \"\\n  \", mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  })))), \"Welcome\"), mdx(\"p\", null, \"Years ago now, I remember starting work on the speech recognition engine that would become the core of\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/coqui-ai/STT\"\n  }), \"Coqui \\uD83D\\uDC38 STT\"), \". Then it was only a dream, creating open source speech\\ntechnology that brought research into the hands of the enterprise and regular developers. But here we\\nare, years later, and Coqui is a reality.\"), mdx(\"p\", null, \"We\\u2019ve grown a lot since then. \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/coqui-ai/STT\"\n  }), \"Coqui \\uD83D\\uDC38 STT\"), \" has gone from aspiration\\nto a actuality, powering the enterprise and providing speech technology to numerous low-resource languages.\\nAlso, \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/coqui-ai/TTS\"\n  }), \"Coqui \\uD83D\\uDC38 TTS\"), \", through that same dream, was born, bringing open,\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/coqui-ai/TTS#-tts-performance\"\n  }), \"human-quality\"), \" speech synthesis to regular developers\\nand the enterprise.\"), mdx(\"p\", null, \"I am grateful that you are joining us on this journey, and I want to personally thank everyone who has ever subscribed\\nto our newsletter, used our software, downloaded a model, filed a bug report, joined our\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://gitter.im/coqui-ai\"\n  }), \"discussion forms\"), \", or simply given us a \\u2B50 star on GitHub. Thank you!\"), mdx(\"p\", null, \"Stay tuned; there\\u2019s much more to come! \\uD83D\\uDC38\"), mdx(\"p\", null, \"On to our first, monthly newsletter where you\\u2019ll hear what we are up to and general reflections on speech tech!\"), mdx(\"h3\", {\n    \"id\": \"coqui-stt-playbook\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#coqui-stt-playbook\",\n    \"aria-label\": \"coqui stt playbook permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }), \"\\n  \", mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  })))), \"Coqui STT Playbook\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1000px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"61.6%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACt0lEQVQozw3SSVPaYACA4fygHnpsp1VLW6oioLIoWyAYSNgCX8AEvrCqkcgmYGQHCagglK2ijradTg/9YfX6Ht95EK3qk1Yt06tlWr0qmj6NpiAVZ0IpmBVLJ8KJcHFekq6EJJ0Ti8l8LhSmcQ9h2V2zqt9plR+RvW2Zzaj0WLcJ536hWr6sFqu9dutGKjcr9kPSEweNQVeajMtpNpsKFk5hLOwDhMGDKnGTEnGg2wHSGHDowyzVHQ2kQb/RFGvtWu4yH+SZEM9k6oXvj8t2NZMV4nySiUWDEQqjCUPIqUOcFjXP+Y9YFweB1BFnD4v58+NoMribDwez4fhhMb6fjn5MbxdTsXcl9RoXlWLYb48E7IA0Ib4DXcRvqwgQ0LgjiFV71UKrxOWSvCjEiqmq1Fw8L7uzu+7k7rLbvJmOa1I7cUgkGRdwWxDaZY4H8WLCTwcdvpTPHTvAoeOoJJCsg4B4RGDrt52LXmO4nBZq5c7wui51jhkiDjDGqUE4jyFMaE9DdobxNvotQcw0Rv2T8vG+d9cZsXN52Ok3pEF3/vJY79SG9/OG1C5AIh3CWHIPifnRgH2Ho6zRKB3OHqGUhggbUbCLApXWtQGOwOz5fvn75de/v9eT0fhp2epfHR/icfrg9TGCW9Q2gwIGsAjAYwnayWBmn0JDrO845ArbZxdHPP35eXM/ZcWzeCY6fny4aFUjfixGWXxWFfKKxKD5mmDdfkAGAFbp1/qLG3gCCIjtEAq9Rx8vJH0plyVkPKucT16ez8SSj7QAL3Zg0yMW3bpJK2fdJpNx643srVy/wvEwW8xDHprc+190coVZricUXs4TTfMwzVOHlNuqCnnRWMSPJAAWpdA8JBhyb3dzxapbd6Mqt1WN7m/Y9jZe8akUq+qtNaVCtvntw9a39xrl6mv0kmaX0/wfDtFUUQbKgn0AAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/13ebb059798b7e17243411a28f2fb044/da8b6/coqui-stt-playbook.png\",\n    \"srcSet\": [\"/static/13ebb059798b7e17243411a28f2fb044/43fa5/coqui-stt-playbook.png 250w\", \"/static/13ebb059798b7e17243411a28f2fb044/c6e3d/coqui-stt-playbook.png 500w\", \"/static/13ebb059798b7e17243411a28f2fb044/da8b6/coqui-stt-playbook.png 1000w\", \"/static/13ebb059798b7e17243411a28f2fb044/2e9ed/coqui-stt-playbook.png 1500w\", \"/static/13ebb059798b7e17243411a28f2fb044/9fabd/coqui-stt-playbook.png 2000w\"],\n    \"sizes\": \"(max-width: 1000px) 100vw, 1000px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \")), mdx(\"p\", null, \"Getting started with speech-to-text can be somewhat intimidating. There are new terms to learn, tools and\\npipelines to get acquainted with, and a whole lot of excitement about the speech experiences you can build.\\nAt Coqui, we want developers to be able to focus entirely on that last part: let their creativity run wild\\nso they can build applications, systems, and experiences using speech that will redefine how users interact\\nwith technology in the future.\"), mdx(\"p\", null, \"With that in mind, Coqui co-founder \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://mobile.twitter.com/_josh_meyer_\"\n  }), \"Josh Meyer\"), \" started a \\u201Cplaybook\\u201D:\\na complement to our technical documentation that told a cohesive story from start to end, covering the entire\\nprocess of getting familiar with Coqui \\uD83D\\uDC38 STT (and STT in general), defining all the pieces of the puzzle and\\nhow they fit together, and then walking you through the process of collecting and refining your data,\\ntraining models and getting confident in how they work, and finally deploying your model on your platform\\nand in your programming language of choice.\"), mdx(\"p\", null, \"This idea resonated with the community, and things really kicked into high gear when \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://twitter.com/KathyReid\"\n  }), \"Kathy Reid\"), \"\\njoined forces in the creation of the playbook. The result is an opinionated guide that is meant as a smooth\\nonboarding process for getting familiar with STT, getting developers from zero to a working speech-to-text\\nsystem, and giving them the knowledge and confidence to start tweaking with their processes and finding the\\nbest tailored way to make speech work for them.\"), mdx(\"p\", null, \"At Coqui, we want to take this idea to the next level: we want developers to be able to build speech experiences\\neffortlessly, as well as to easily share their work with the speech community - be it tools, models, new\\narchitectures, or datasets. As a first step, we have released the updated \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stt.readthedocs.io/en/latest/playbook/README.html\"\n  }), \"STT playbook\"), \",\\nreadable now on our main documentation site. We will continue to refine our documentation, tools, and models\\nworking together with our brilliant community to bring speech research into reality.\"), mdx(\"p\", null, \"Don\\u2019t forget to check out the updated \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stt.readthedocs.io/en/latest/playbook/README.html\"\n  }), \"STT playbook\"), \"\\nand \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://gitter.im/coqui-ai\"\n  }), \"join our growing speech community\"), \".\"), mdx(\"h3\", {\n    \"id\": \"few-shot-keyword-spotting-in-any-language\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#few-shot-keyword-spotting-in-any-language\",\n    \"aria-label\": \"few shot keyword spotting in any language permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }), \"\\n  \", mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  })))), \"Few-Shot Keyword Spotting in Any Language\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"980px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"34.800000000000004%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABkElEQVQozy2RW4/SQBiG+//vvTXGRBM1MXpldl3NaswKrBpZMbstZaGFaQeYAoWeT4/TwTeZZDLPfO93spqmIctKAvsPqx+f8b59Rf616dUz5/Ytdx/e4X2ZIG89EhEbVqQpL1+85+nlBR/dCT/VknV+xKqbmkjFLIZXhDeXzK/HyPvgHJQnjK+eY1/8Qo0jUpFSJ7VhR7XjybM3vLofMYlDRH4gbyusY5IRuC6bwSeCO0UkCqq8pWlByUec6xuSJf/VmdN2HUt3wevRdxSFIcJfkiYJVpzkhJPfrEY2h6ik1VlMu9pwI2Yshw+0uqiqKgiE0Lw1pjNtOJw9mL+1Hs3jYk6e51hZUeENBqj5Vps0TJ2pydRrL1cE44W5l0WBY9sUxbkiz1/hhefRtG2H4zgkpxNWmuXIqcvpcNTLyZBhyEmDXnEUsVspqrom0UlCzeL4vBSlFPvDgbIqydKMzXpt4q1af+6BCAS+7xPKULd3bruf1W6/x/M8XZHHdrul6zrD+uDeVOgxBEGAlNK8/QO3dQwIWWFougAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/2c3d0d168ad5f9315b408b2df295551f/2b72d/few-shot-keyword-spotting-in-any-language.png\",\n    \"srcSet\": [\"/static/2c3d0d168ad5f9315b408b2df295551f/43fa5/few-shot-keyword-spotting-in-any-language.png 250w\", \"/static/2c3d0d168ad5f9315b408b2df295551f/c6e3d/few-shot-keyword-spotting-in-any-language.png 500w\", \"/static/2c3d0d168ad5f9315b408b2df295551f/2b72d/few-shot-keyword-spotting-in-any-language.png 980w\"],\n    \"sizes\": \"(max-width: 980px) 100vw, 980px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \")), mdx(\"p\", null, \"We have traditionally focused on open vocabulary speech-to-text at Coqui, but now we are expanding to new\\nhorizons.\"), mdx(\"p\", null, \"The flexibility of open vocabulary STT is great: you can say literally anything to a\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/coqui-ai/STT\"\n  }), \"Coqui \\uD83D\\uDC38 STT\"), \" model, and it will transcribe it. However, there are many\\napplications where you don\\u2019t need to transcribe every word, you just need to spot a few keywords in the\\nstream of audio. One common application of this is wake word detection, e.g. \\u201D\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.youtube.com/watch?v=1ZXugicgn6U\"\n  }), \"Hey, Computer\"), \"\\u201D.\\nFor the task of wake word detection, deploying a large, open vocabulary STT model is impractical on edge devices.\\n(Edge devices have very small compute resources and assume limited power consumption.) As such, we\\u2019ve begun\\nresearch into robust, multilingual, practical model architectures for keyword spotting.\"), mdx(\"p\", null, \"Collaborating with researchers from \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://edge.seas.harvard.edu/\"\n  }), \"Harvard University\"), \" and\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.tensorflow.org/lite\"\n  }), \"Google\"), \", Coqui co-authored a publication on an\\nextremely efficient keyword spotting technique that scales to any language. The training technique is\\nintuitive and it works: we first train a base model to classify lots of keywords in lots of different\\nlanguages. This is a classification model, not a transcription model. This base model does really well\\nfor the keywords on which it was trained, but we really care about how well this model can perform on\\nnew keywords that it\\u2019s never heard in new languages. The answer: it does great! We can fine-tune this\\nbase model to any new keyword in any language with just five audio clips of the new word.\"), mdx(\"p\", null, \"We submitted this work to the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.interspeech2021.org/\"\n  }), \"INTERSPEECH 2021\"), \" conference. Wish us luck\\non acceptance! Until INTERSPEECH 2021, let us whet your appetite with this teaser, the abstract for the\\npaper:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Abstract:\"), \"\\nWe introduce a few-shot transfer learning method for key-word spotting in any language. Leveraging open\\nspeech corpora in nine languages, we automate the extraction of a large multilingual keyword bank and\\nuse it to train an embedding model. With just five training examples, we fine-tune the embedding model\\nfor keyword spotting and achieve an average F1 score of 0.75 on keyword classification for 180 new\\nkeywords unseen by the embedding model in these nine languages. This embedding model also generalizes\\nto new languages. We achieve an average F1 score of 0.65 on 5-shot models for 260 keywords sampled\\nacross 13 new languages unseen by the embedding model. We investigate streaming accuracy for our 5-shot\\nmodels in two contexts: keyword spotting and keyword search. Across 440 keywords in 22 languages,\\nwe achieve an average streaming keyword spotting accuracy of 85.2% with a false acceptance rate of\\n1.2%, and observe promising initial results on keyword search.\"), mdx(\"h3\", {\n    \"id\": \"sc-glowtts-an-efficient-zero-shot-multi-speaker-tts-model\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#sc-glowtts-an-efficient-zero-shot-multi-speaker-tts-model\",\n    \"aria-label\": \"sc glowtts an efficient zero shot multi speaker tts model permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }), \"\\n  \", mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  })))), \"SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker TTS Model\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"743px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"60.4%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABgUlEQVQoz5WSwW7TQBRF+xks+RE+gj2/wIIN/QV2LFBXVKoEG8QWCYRS1KobiFqqNlIINCq0ceoQO47t2jPjsWd8eIpYVakUz/rqzLvnvS3WvLZt2eSty23dF2qcJ44ixr9GBJMJVWXpfTvn9ft99t5+YhYGa6H3Ao0A8smAP997FLMxjfc8ebHLg8fbPHz0lIODk1XOyccbAa21RJenXA+OyIORjFzzrnfEs1c7PH+5w8XwapXz3m8+Yfz7jPHJPmnwkzpXZHnE5+OPHA6+oGvVrbKuHDc3Uy5GQ5I0l26ecHrFYb/P19MztDLdgFnZkOYJTZGQ3SpUqUmXAfN4RJZKXa1ouwBL06CNwVVGIAlFqWTqhnhZiA4nTswKyMZbto75ImUazim0pRZGlpfM5tFKQV10ctjKHbYyYU28WFKKL20sy6wgDP+iBOiU7rgUHYmzH1yHQz7033B+eSy15ZSiBaU4bRrXDehcja3El1UktxGlLrC1R5kKI/39/9xd4D+GIpv/qdfXDwAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"IMAGE\",\n    \"title\": \"IMAGE\",\n    \"src\": \"/static/3a26d3b6eb8dcc0c81662bc544ecc10f/b217e/sc-glowtts.png\",\n    \"srcSet\": [\"/static/3a26d3b6eb8dcc0c81662bc544ecc10f/43fa5/sc-glowtts.png 250w\", \"/static/3a26d3b6eb8dcc0c81662bc544ecc10f/c6e3d/sc-glowtts.png 500w\", \"/static/3a26d3b6eb8dcc0c81662bc544ecc10f/b217e/sc-glowtts.png 743w\"],\n    \"sizes\": \"(max-width: 743px) 100vw, 743px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \")), mdx(\"p\", null, \"At Coqui, we\\u2019re motivated to provide speech technology for all languages and people. One of the problems we\\u2019ve\\nencountered along way is data-hungry machine learning models. For some languages the data simply isn\\u2019t there!\\nFinding enough data is hard, and even if it\\u2019s available, training machine learning models is difficult too.\"), mdx(\"p\", null, \"To solve a part of this problem for text-to-speech tasks, we investigated different \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://en.wikipedia.org/wiki/Zero-shot_learning\"\n  }), \"zero-shot learning\"), \"\\napproaches using state-of-the-art text-to-speech models. (Zero-shot learning techniquies can greatly reduce\\ndata requirements for some algorithms.) This investigation bore fruit! We came up with a new algorithm we\\nchristened \\u201CSC-GlowTTS\\u201D, a catchy name I know. SC-GlowTTS can generalize to novel speakers after training\\nwith only 11 speakers for the target language. This means we need less data!\"), mdx(\"p\", null, \"Soon after this newsletter finds its way into your hands, we\\u2019ll release SC-GlowTTS\\u2019s code, models, and an\\nassociated article. Please stay tuned! But, if you can\\u2019t wait, check out the SC-GlowTTS\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://edresson.github.io/SC-GlowTTS/\"\n  }), \"project page\"), \".\"), mdx(\"p\", null, \"This is a work of all the Coqui \\uD83D\\uDC38 TTS community but special thanks to the main author\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/Edresson\"\n  }), \"Edresson Casanova\"), \" who organized and did the brunt of the work and\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/mueller91\"\n  }), \"Nicolas Michael M\\xFCller\"), \" for training the most expansive open\\nsource speaker encoder, which we used in this work.\"), mdx(\"p\", null, \"It is great to see how an open community of great developers and researchers can innovate without borders.\\nWe hope this is just a start for us and our great community to pave the way for open source TTS.\"), mdx(\"h3\", {\n    \"id\": \"new-release-v0011\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#new-release-v0011\",\n    \"aria-label\": \"new release v0011 permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"xmlns\": \"http://www.w3.org/2000/svg\",\n    \"width\": \"16\",\n    \"height\": \"16\",\n    \"focusable\": \"false\",\n    \"viewBox\": \"0 0 16 16\"\n  }), \"\\n  \", mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fill\": \"currentColor\",\n    \"d\": \"M4.441 7.38l.095.083.939.939-.708.707-.939-.939-2 2-.132.142a2.829 2.829 0 003.99 3.99l.142-.132 2-2-.939-.939.707-.708.94.94a1 1 0 01.083 1.32l-.083.094-2 2A3.828 3.828 0 01.972 9.621l.15-.158 2-2A1 1 0 014.34 7.31l.101.07zm7.413-3.234a.5.5 0 01.057.638l-.057.07-7 7a.5.5 0 01-.765-.638l.057-.07 7-7a.5.5 0 01.708 0zm3.023-3.025a3.829 3.829 0 01.15 5.257l-.15.158-2 2a1 1 0 01-1.32.083l-.094-.083-.94-.94.708-.707.939.94 2-2 .132-.142a2.829 2.829 0 00-3.99-3.99l-.142.131-2 2 .939.939-.707.708-.94-.94a1 1 0 01-.082-1.32l.083-.094 2-2a3.828 3.828 0 015.414 0z\"\n  })))), \"New Release v0.0.11\"), mdx(\"p\", null, \"Oh, one more thing!\"), mdx(\"p\", null, \"We are happy to release two new German \\uD83D\\uDC38 TTS models trained and shared by the great \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://twitter.com/ThorstenVoice\"\n  }), \"ThorstenVoice\"), \".\"), mdx(\"p\", null, \"You can see a list of all the released models and you can start using them with the simple command line calls:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-terminal\"\n  }), \"> pip install -U tts\\n> tts --list_models\\n> tts --text \\\"Coqui TTS is great!\\\" --out_path path/to/save/output.wav\\n> tts --model_name tts_models/de/thorsten/tacotron2-DCA --text \\\"Coqui TTS ist bereit, Deutsch zu sprechen.\\\" --out_path output.wav\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#welcome","title":"Welcome"},{"url":"#coqui-stt-playbook","title":"Coqui STT Playbook"},{"url":"#few-shot-keyword-spotting-in-any-language","title":"Few-Shot Keyword Spotting in Any Language"},{"url":"#sc-glowtts-an-efficient-zero-shot-multi-speaker-tts-model","title":"SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker TTS Model"},{"url":"#new-release-v0011","title":"New Release v0.0.11"}]}}},"pageContext":{"frontmatter":{"title":"Coqui's First Mondays Newsletter","date":"April 5, 2021"},"fileAbsolutePath":"/Users/kdavis/Code/coqui-ai/coqui-ai.github.io/src/pages/newsletter/05-04-2021.mdx"}},"staticQueryHashes":["1942088059","3709355695"]}